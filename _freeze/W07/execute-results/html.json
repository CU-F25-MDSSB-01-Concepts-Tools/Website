{
  "hash": "4671c5b916e5506358f695e14ab67029",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#07: Models in Science, Linear Model, Interaction Effects, Nonlinear Models\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    toc: true\n    toc-depth: 1\n    slide-number: true\n    smaller: true\n    chalkboard: \n      buttons: true\n    preview-links: true\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n## Preliminaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.1     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.3.0\n✔ modeldata    1.5.1     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'palmerpenguins'\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n:::\n\n\n\n# Models in Science\n\n## What is a model? {.smaller background-color=\"khaki\"}\n\n- A model is an informative representation of an object, person or system. \n\n![](https://upload.wikimedia.org/wikipedia/commons/4/46/Astrid_Andersen_01.jpg){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/b/b8/Beta-D-Gulopyranose_Molek%C3%BClbaukasten_9288.JPG){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/7/73/AtmosphericModelSchematic.png){height=200} \n\n- A model is a [**simplified**]{style='color:blue;'} representation of the real world focusing on the essential parts for its [**purpose**]{style='color:blue;'}.\n\n::: aside\nSource: <https://en.wikipedia.org/wiki/Model>\n:::\n\n## Scientific Modeling {.smaller background-color=\"khaki\"}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n**Scientific modeling** is an activity that produces models representing empirical objects, phenomena, and physical processes, to make a particular part or feature of the world easier to \n\n- understand, \n- define, \n- quantify, \n- visualize, or \n- simulate.\n:::\n\n::: {.column width='50%'}\n\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/MathModel.svg/2560px-MathModel.svg.png)\n\n#### Famous wisdom \n\n> [All models are wrong, but some are useful.](https://en.wikipedia.org/wiki/All_models_are_wrong)\n:::\n\n::::\n\n::: aside\nSource: <https://en.wikipedia.org/wiki/Scientific_modelling>\n:::\n\n\n## Model Purposes and Data Analysis {.smaller background-color=\"khaki\"}\n\n:::: {.columns}\n\n::: {.column width='60%'}\n- **Agent-based models** and **differential equations**\n    - are used to **explain** the **dynamics** of one or more variables (typically) over time. They are used to answer [*mechanistic questions*]{style='color:blue;'}.\n- [**Variable-based models**]{style='color:blue;'} \n    - are used to specify relations between variables\n    - to **explain** relations and make **predictions**.\n    - Used to answer [*inferential*]{style='color:blue;'} and [*predictive questions*]{style='color:blue;'}.   \n    - (With experimental and theoretical effort also for [*causal questions*]{style='color:blue;'}.)\n\n:::\n\n::: {.column width='40%'}\n![](img/DataAnalysisFlowChart_LeekPeng.jpeg)\n:::\n\n::::\n\n. . . \n\nThe most basic variable-based model is the [**linear model**]{style='color:blue;'}.\n\n\n## Models to Learn from Data {.smaller background-color=\"khaki\"}\n\nIn much of the following we use a *narrower* definition of **model**:\n\nA **mathematical model** that consists of **variables** and **functions**. \n\n:::: {.columns}\n\n::: {.column width='50%'}\nDifferent names and roles of [**variables**]{style='color:blue;'}: \n\n- random variables\n- covariates\n- predictors\n- latent variables\n- features\n- targets\n- outcomes\n:::\n\n::: {.column width='50%'}\n[**Functions**]{style='color:blue;'} relate variables to each other\n\n- In a linear model we model one variable as a weighted sum of the other variables\n\nOnce a model (setup of variables and functions) is specified, modelers use [**Data**]{style='color:blue;'} to find the *best* function, called\n\n- estimation\n- training\n- fitting\n- learning\n\n\n:::\n\n\n\n::::\n\n\n## Modeling Mindsets {.smaller background-color=\"khaki\"}\n\nEven within the setup of *Models to Learn from Data* there are many different [**Modeling Mindsets**]{style='color:blue;'}\n\n![](img/ModelingMindsets.png)\n\n::: aside\nMolnar, C. (2022). [Modeling mindsets: The many cultures of learning from data.](https://books.google.de/books?id=qV-DzwEACAAJ) Christoph Molnar c/o Mucbook Clubhouse. \n:::\n\n\n\n\n\n\n# Linear Model\nThe first work-horse model in data science\n\n\n## Hello again penguins! {.scrollable .smaller}\n\nWe use the dataset [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/)\n\nChinstrap, Gentoo, and Adélie Penguins\n\n![](https://upload.wikimedia.org/wikipedia/commons/0/08/South_Shetland-2016-Deception_Island%E2%80%93Chinstrap_penguin_%28Pygoscelis_antarctica%29_04.jpg){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Hope_Bay-2016-Trinity_Peninsula%E2%80%93Ad%C3%A9lie_penguin_%28Pygoscelis_adeliae%29_04.jpg){height=200}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n## Body mass in grams\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(body_mass_g)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Flipper length in millimeters\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(flipper_length_mm)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Relate variables as a line {.smaller background-color=\"aquamarine\"}\n\nA *line* is a shift-scale transformation of the identity function usually written in the form \n\n$$f(x) = a\\cdot x + b$$\n\nwhere [$a$ is the *slope*]{style=\"color:red;\"}, [$b$ is the *intercept*]{style=\"color:blue;\"}.^[This a scale and a shift in the $y$ direction. Note: For lines there is always an analog transformations on the $x$ direction.]\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\na <- 0.5\nb <- 1\nfunc <- function(x) a*x + b\nggplot() + geom_function(fun = func, size = 2) +\n # Set axis limits and make axis equal\n\txlim(c(-0.5,2)) + ylim(c(0,2)) + coord_fixed() + \n\tgeom_line( # intercept line:\n\t data=tibble(x=c(0,0),y=c(0,1)), \n\t mapping = aes(x,y), \n\t color = \"blue\", size = 2) +\n\tgeom_line( # slope:\n\t data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), \n\t mapping = aes(x,y), \n\t color = \"red\", size = 2) +\n\tgeom_line( # x-interval of length one:\n\t data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), \n\t mapping = aes(x,y), color = \"gray\") +\n\ttheme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Penguins: Linear model {.smaller}\n\n**Flipper length** as a function of **body mass**.\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Penguins: A smooth line {.smaller}\n\n**Flipper length** as a function of **body mass** with `loess`^[loess = locally estimated scatterplot smoothing] smoothing. \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"loess\") + \n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis is a less theory-driven and more data-driven model. Why?    \n[We don't have a simple mathematical form of the function.]{.fragment}\n\n## Terminology variable-based models {.smaller background-color=\"khaki\"}\n\n- **Response variable:**^[Also **dependent variable** in statistics or empirical social sciences.] Variable whose behavior or variation you are trying to understand, on the y-axis\n- **Explanatory variable(s):**^[Also **independent variable(s)** in statistics or empirical social sciences.] Other variable(s) that you want to use to explain the variation in the response, on the x-axis\n- **Predicted value:** Output of the model function. \n  - The model function gives the **(expected) average value** of the response variable conditioning on the explanatory variables\n  - **Residual(s):** A measure of how far away a case is from its predicted value (based on the particular model)   \n    Residual = Observed value minus Predicted value  \n    The residual tells how far above/below the expected value each case is\n\n## More explanatory variables {.smaller}\n\nHow does the relation between flipper length and body mass change with different species?\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm, \n            color = species)) +\n geom_point() +\n geom_smooth(method = \"lm\",\n             se = FALSE) + \n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## ggplot-hint: How to color penguins but keep one model? {.smaller}\n\nPut the mapping of the color aesthetic into the `geom_point` command. \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(method = \"lm\",\tse = FALSE) + \n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Beware of Simpson's paradox {.smaller background-color=\"aquamarine\"} \n\nSlopes for all groups can be in the opposite direction of the main effect's slope!\n\n![](https://upload.wikimedia.org/wikipedia/commons/f/fb/Simpsons_paradox_-_animation.gif){height=400}\n\n::: aside\nSource: <https://en.wikipedia.org/wiki/Simpson%27s_paradox>\n:::\n\n\n## The paradox is real! {.smaller}\n\nHow does the relation between bill length and body mass change with different species?\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = bill_length_mm, \n            y = bill_depth_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(mapping = aes(color = species),\n             method = \"lm\",\n             se = FALSE) + \n geom_smooth(method = \"lm\",\n\t\t\t\t\t\t\t\t\t\t\t\t\tse = FALSE) + \n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Models - upsides and downsides {.smaller background-color=\"khaki\"}\n\n- Models can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.\n  - How would you visualize dependencies of more than two variables?\n- The risk is that a model is imposing structure that is not really there in the real world data. \n  - People imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.\n  - Every model is a simplification of the real world, but there are good and bad models (for particular purposes). \n  - A skeptical (but constructive) approach to a model is always advisable. \n  \n  \n## Variation around a model {.smaller background-color=\"khaki\"}\n\n... is as interesting and important as the model!\n\n*[**Statistics**]{style='color:blue;'} is the explanation of uncertainty of variation in the context of what remains unexplained.*\n\n- The scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability. \n- Or is it randomness?\n- Adding more explanatory variables can help (but need not)\n\n## *All models are wrong ...* {.smaller background-color=\"khaki\"}\n\n*... but some are useful.* (George Box)\n\n\nExtending the range of the model: \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE, \n \t\t\t\t\t\t\t\t\t\t\t\tfullrange = TRUE) +\n\txlim(c(0,7000)) + ylim(c(0,230)) +\n theme_classic(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- The model predicts that penguins with zero weight still have flippers of about 140 mm on average.\n- Is the model useless? [Yes, around zero body mass. No, it works OK in the range of the body mass data.]{.fragment}\n\n## Two model purposes {.smaller background-color=\"khaki\"}\n\nLinear models can be used for:\n\n- **Explanation:** Understand the relationship of variables in a quantitative way.   \n*For the linear model, interpret slope and intercept.*\n    - In other words: We make **inference** about relations in any sample of penguins.\n- **Prediction:** Plug in new values for the explanatory variable(s) and receive the expected response value.   \n*For the linear model, predict the flipper length of new penguins by their body mass.*\n\n# Fitting Models\n\nToday: The linear model. \n\n## In R: `tidymodels`\n\n![](https://datasciencebox.org/course-materials/_slides/u4-d02-fitting-interpreting-models/img/tidymodels.png)\n\n:::{.aside}\nFrom <https://datasciencebox.org>\n:::\n\n## Our goal\n\nPredict flipper length from body mass\n\naverage `flipper_length_mm` $= \\beta_0 + \\beta_1\\cdot$ `body_mass_g`\n\n\n## Step 1: Specify model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n## Step 2: Set the model fitting *engine*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n## Step 3: Fit model and estimate parameters {.smaller}\n\nOnly now, the data and the variable selection comes in. \n\nUse of **formula syntax**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n```\n\n\n:::\n:::\n\n\n[`parsnip`](http://parsnip.tidymodels.org) is package in `tidymodels` which is to provide a tidy, unified interface to models that can be used to try a range of models. \n\n:::{.aside}\nNote: The fit command does not follow the tidyverse principle the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in base R. \n:::\n\n## What does the output say? {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n```\n\n\n:::\n:::\n\n\n. . .\n\naverage `flipper_length_mm` $= 136.72956 + 0.01528\\cdot$ `body_mass_g`\n\n. . .\n\n**Interpretation:**   \nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg).\nPenguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted.\n\n## Show output in *tidy* form\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n\ttidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n```\n\n\n:::\n:::\n\n\n## Parameter estimation {.smaller background-color=\"aquamarine\"}\n\nNotation from statistics: $\\beta$'s for the population parameters and $\\hat\\beta$'s for the parameters estimated from the sample statistics. \n\n$$\\hat y = \\beta_0 + \\beta_1 x$$\n\nThe population parameters $\\beta_0$ and $\\beta_1$ we cannot have. ($\\hat y$ stands for *predicted value of $y$*. )\n \n. . .\n\nInstead, we estimate $\\hat\\beta_0$ and $\\hat\\beta_1$ in the model fitting process.\n\n$$\\hat y = \\hat\\beta_0 + \\hat\\beta_1 x$$\n\n:::{style=\"background-color:khaki;padding:10px;\"}\nA typical follow-up data analysis question is what the fitted values $\\hat\\beta_0$ and $\\hat\\beta_1$ tell us about the (unknown) population-wide values $\\beta_0$ and $\\beta_1$? \n\nThis is the typical [**inferential question**]{style='color:blue}.\n::: \n\n\n\n## `parsnip` model objects {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npengmod <- linear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins)\nclass(pengmod) # attributes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"_lm\"       \"model_fit\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntypeof(pengmod) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(pengmod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"lvl\"          \"ordered\"      \"spec\"         \"fit\"          \"preproc\"     \n[6] \"elapsed\"      \"censor_probs\"\n```\n\n\n:::\n:::\n\n\n. . . \n\nMost interesting for us for now: `$fit`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npengmod$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n```\n\n\n:::\n:::\n\n\n. . . \n\nNotice: `parsnip model object` is now missing in the output.\n\n## `$fit` is the object created by `lm` (base R) {.smaller .scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(pengmod$fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n```\n\n\n:::\n\n```{.r .cell-code}\npengmod$fit$call\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n```\n\n\n:::\n\n```{.r .cell-code}\npengmod$fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)  body_mass_g \n136.72955927   0.01527592 \n```\n\n\n:::\n\n```{.r .cell-code}\npengmod$fit$fitted.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        5        6        7        8        9 \n194.0142 194.7780 186.3763 189.4315 192.4867 192.1048 208.1445 189.8134 \n      10       11       12       13       14       15       16       17 \n201.6522 187.1401 193.2504 185.6125 194.7780 203.9436 193.2504 189.4315 \n      18       19       20       21       22       23       24       25 \n205.4712 187.5220 200.8884 188.6677 191.7229 194.7780 197.0694 194.7780 \n      26       27       28       29       30       31       32       33 \n194.7780 190.9591 185.6125 184.8487 197.0694 186.3763 196.3056 187.1401 \n      34       35       36       37       38       39       40       41 \n196.3056 187.5220 200.1246 197.0694 190.9591 187.1401 207.7626 184.8487 \n      42       43       44       45       46       47       48       49 \n196.3056 184.0849 203.9436 182.5573 206.9988 189.0496 182.1754 189.4315 \n      50       51       52       53       54       55       56       57 \n200.1246 190.1953 202.4160 189.4315 198.5970 181.0297 193.2504 190.9591 \n      58       59       60       61       62       63       64       65 \n194.7780 180.2659 194.0142 184.8487 203.9436 191.7229 198.5970 180.2659 \n      66       67       68       69       70       71       72       73 \n197.0694 187.9039 199.3608 183.3211 204.7074 191.7229 196.3056 190.9591 \n      74       75       76       77       78       79       80       81 \n200.1246 193.2504 201.6522 193.2504 196.3056 190.9591 197.8332 185.6125 \n      82       83       84       85       86       87       88       89 \n208.5264 194.7780 200.8884 187.9039 190.9591 194.7780 190.1953 197.0694 \n      90       91       92       93       94       95       96       97 \n191.7229 190.9591 202.4160 188.6677 204.7074 187.1401 202.4160 193.2504 \n      98       99      100      101      102      103      104      105 \n203.1798 181.0297 199.3608 193.6323 208.9083 183.7030 201.6522 181.4116 \n     106      107      108      109      110      111      112      113 \n190.9591 194.0142 196.3056 185.2306 209.6721 195.1599 206.9988 185.6125 \n     114      115      116      117      118      119      120      121 \n202.0341 196.3056 198.9789 181.0297 194.3961 187.9039 187.5220 184.8487 \n     122      123      124      125      126      127      128      129 \n190.1953 189.4315 195.9237 183.3211 197.8332 186.7582 202.4160 183.3211 \n     130      131      132      133      134      135      136      137 \n197.8332 187.5220 190.1953 190.1953 205.0893 189.0496 196.3056 185.2306 \n     138      139      140      141      142      143      144      145 \n197.4513 188.6677 201.6522 188.6677 189.8134 183.3211 193.6323 182.5573 \n     146      147      148      149      150      151      152      153 \n192.4867 201.6522 189.8134 189.4315 194.0142 193.2504 197.8332 205.4712 \n     154      155      156      157      158      159      160      161 \n223.8023 204.7074 223.8023 219.2195 206.2350 210.0540 216.1643 203.9436 \n     162      163      164      165      166      167      168      169 \n215.4005 207.7626 221.5109 207.7626 226.0937 200.8884 226.0937 200.1246 \n     170      171      172      173      174      175      176      177 \n232.9678 210.0540 218.4557 223.8023 213.1091 203.9436 213.8729 213.1091 \n     178      179      180      181      182      183      184      185 \n214.6367 199.3608 223.0385 206.9988 221.5109 216.9281 208.5264 213.8729 \n     186      187      188      189      190      191      192      193 \n229.1488 215.4005 219.2195 212.3453 216.9281 203.1798 218.4557 197.0694 \n     194      195      196      197      198      199      200      201 \n223.8023 202.4160 209.2902 221.5109 211.5815 200.8884 219.2195 214.6367 \n     202      203      204      205      206      207      208      209 \n217.6919 210.8177 217.6919 203.9436 213.1091 211.5815 213.8729 202.4160 \n     210      211      212      213      214      215      216      217 \n213.1091 204.7074 221.5109 200.8884 217.6919 203.9436 223.0385 208.5264 \n     218      219      220      221      222      223      224      225 \n223.8023 207.7626 225.3299 208.5264 221.5109 209.2902 213.1091 214.6367 \n     226      227      228      229      230      231      232      233 \n216.1643 208.5264 225.3299 206.9988 228.3851 209.2902 227.6213 207.3807 \n     234      235      236      237      238      239      240      241 \n219.9833 208.9083 218.4557 209.2902 222.2747 206.9988 217.6919 211.1996 \n     242      243      244      245      246      247      248      249 \n221.5109 212.3453 219.2195 209.2902 223.0385 210.8177 216.1643 211.9634 \n     250      251      252      253      254      255      256      257 \n211.1996 207.3807 216.9281 210.8177 222.2747 212.7272 220.7471 208.9083 \n     258      259      260      261      262      263      264      265 \n220.7471 208.5264 220.7471 206.6169 220.7471 213.1091 227.6213 207.7626 \n     266      267      268      269      270      271      273      274 \n220.7471 203.5617 226.0937 211.1996 228.3851 211.9634 210.8177 224.5661 \n     275      276      277      278      279      280      281      282 \n216.1643 219.2195 190.1953 196.3056 192.4867 190.5772 193.6323 197.0694 \n     283      284      285      286      287      288      289      290 \n186.3763 194.0142 200.1246 193.2504 194.7780 194.3961 193.2504 198.5970 \n     291      292      293      294      295      296      297      298 \n191.3410 198.5970 187.1401 193.2504 189.4315 203.9436 191.7229 188.6677 \n     299      300      301      302      303      304      305      306 \n181.0297 194.7780 187.1401 200.1246 188.6677 194.7780 193.2504 206.2350 \n     307      308      309      310      311      312      313      314 \n185.6125 202.4160 187.9039 199.3608 191.7229 196.3056 195.5418 210.0540 \n     315      316      317      318      319      320      321      322 \n177.9745 205.4712 197.0694 192.4867 190.9591 190.1953 192.8685 204.7074 \n     323      324      325      326      327      328      329      330 \n188.6677 202.4160 186.3763 192.8685 187.5220 197.0694 191.7229 198.5970 \n     331      332      333      334      335      336      337      338 \n187.9039 189.4315 186.3763 198.5970 194.7780 190.5772 197.0694 192.4867 \n     339      340      341      342      343      344 \n192.4867 197.8332 188.6677 194.3961 199.3608 194.3961 \n```\n\n\n:::\n\n```{.r .cell-code}\npengmod$fit$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           1            2            3            5            6            7 \n-13.01424280  -8.77803858   8.62371500   3.56853188  -2.48665124 -11.10475335 \n           8            9           10           11           12           13 \n-13.14446474   3.18663399 -11.65220061  -1.14008078 -13.25044702  -3.61248922 \n          14           15           16           17           18           19 \n -3.77803858  -5.94358795  -8.25044702   5.56853188  -8.47117951  -3.52197867 \n          20           21           22           23           24           25 \n -6.88840483 -14.66767234 -11.72285546  -5.77803858 -12.06942592 -14.77803858 \n          26           27           28           29           30           31 \n -7.77803858  -7.95905968   1.38751078 -12.84869344 -17.06942592  -8.37628500 \n          32           33           34           35           36           37 \n-18.30563014   0.85991922 -12.30563014   7.47802133  -4.12460905  -7.06942592 \n          38           39           40           41           42           43 \n-10.95905968  -6.14008078 -23.76256685  -2.84869344  -1.30563014   1.91510234 \n          44           45           46           47           48           49 \n -7.94358795   2.44269390 -16.99877107  -7.04957023  -3.17540821   0.56853188 \n          50           51           52           53           54           55 \n -9.12460905  -4.19526390 -14.41599639   0.56853188   1.40298251   5.97028546 \n          56           57           58           59           60           61 \n -2.25044702  -4.95905968  -1.77803858   0.73408124  -0.01424280   0.15130656 \n          62           63           64           65           66           67 \n -8.94358795  -6.72285546  -6.59701749   3.73408124  -5.06942592   7.09612344 \n          68           69           70           71           72           73 \n-11.36081327   6.67889812  -6.70738373  -1.72285546  -6.30563014   5.04094032 \n          74           75           76           77           78           79 \n -3.12460905  -3.25044702  -6.65220061  -2.25044702 -12.30563014  -3.95905968 \n          80           81           82           83           84           85 \n -2.83322170   3.38751078 -12.52636263  -7.77803858  -7.88840483   3.09612344 \n          86           87           88           89           90           91 \n  3.04094032  -4.77803858  -1.19526390  -8.06942592  -1.72285546  11.04094032 \n          92           93           94           95           96           97 \n  2.58400361  -3.66767234 -18.70738373  -0.14008078   5.58400361  -3.25044702 \n          98           99          100          101          102          103 \n -7.17979217  -3.02971454  -7.36081327  -1.63234491  -5.90826052  -0.70299977 \n         104          105          106          107          108          109 \n-11.65220061  11.58838757  -6.95905968   4.98575720  -6.30563014  -4.23059133 \n         110          111          112          113          114          115 \n-12.67205630   2.84006353 -15.99877107   7.38751078  -5.03409850  -5.30563014 \n         116          117          118          119          120          121 \n -2.97891538   6.97028546   4.60385931   1.09612344   1.47802133   2.15130656 \n         122          123          124          125          126          127 \n  7.80473610 -13.43146812   6.07626775   2.67889812   1.16677830   4.24181711 \n         128          129          130          131          132          133 \n -7.41599639   7.67889812  12.16677830   2.47802133   6.80473610   2.80473610 \n         134          135          136          137          138          139 \n -6.08928162  -2.04957023  -6.30563014   5.76940867   2.54867619  -3.66767234 \n         140          141          142          143          144          145 \n -8.65220061   4.33232766  -2.81336601   4.67889812  -3.63234491   9.44269390 \n         146          147          148          149          150          151 \n -7.48665124 -11.65220061  -5.81336601   5.56853188  -1.01424280  -6.25044702 \n         152          153          154          155          156          157 \n  3.16677830   5.52882049   6.19772176   5.29261627  -5.80227824  -4.21950356 \n         158          159          160          161          162          163 \n  3.76502471   0.94604581   2.83567957   5.05641205  -0.40052465   6.23743315 \n         164          165          166          167          168          169 \n -5.51089090   6.23743315 -13.09366558   9.11159517  -9.09366558   9.87539095 \n         170          171          172          173          174          175 \n-11.96782760  -1.05395419   3.54429222  -5.80227824   1.89086269   9.05641205 \n         176          177          178          179          180          181 \n  1.12706691   1.89086269   0.36327113  16.63918673  -8.03848246   3.00122893 \n         182          183          184          185          186          187 \n -1.51089090   5.07188379   0.47363737  -6.87293309   0.85115130   4.59947535 \n         188          189          190          191          192          193 \n  0.78049644   0.65465847   2.07188379   4.82020783 -10.45570778  10.93057408 \n         194          195          196          197          198          199 \n  1.19772176   7.58400361   6.70984159   0.48910910   5.41845425   9.11159517 \n         200          201          202          203          204          205 \n  5.78049644  -1.63672887  -2.69191200  -0.81774997   2.30808800   6.05641205 \n         206          207          208          209          210          211 \n 11.89086269   5.41845425   6.12706691   5.58400361   6.89086269   3.29261627 \n         212          213          214          215          216          217 \n  2.48910910   7.11159517   3.30808800  10.05641205   7.96151754  10.47363737 \n         218          219          220          221          222          223 \n  6.19772176   6.23743315   3.67013020  11.47363737   1.48910910   6.70984159 \n         224          225          226          227          228          229 \n  7.89086269   6.36327113   0.83567957   7.47363737   4.67013020   2.00122893 \n         230          231          232          233          234          235 \n -8.38505292   5.70984159  -4.62125714   4.61933104   1.01670066   3.09173948 \n         236          237          238          239          240          241 \n  5.54429222   2.70984159   5.72531332  11.00122893   0.30808800   0.80035214 \n         242          243          244          245          246          247 \n  8.48910910   5.65465847   8.78049644   2.70984159   0.96151754   3.18225003 \n         248          249          250          251          252          253 \n  9.83567957   4.03655636  10.80035214  -4.38066896   8.07188379   8.18225003 \n         254          255          256          257          258          259 \n  5.72531332   2.27276058   7.25290488   7.09173948  -5.74709512   1.47363737 \n         260          261          262          263          264          265 \n -1.74709512   1.38312682 -11.74709512   2.89086269   1.37874286   5.23743315 \n         266          267          268          269          270          271 \n  9.25290488  13.43830994   3.90633442   5.80035214  -6.38505292   2.03655636 \n         273          274          275          276          277          278 \n  4.18225003  -2.56607402  -4.16432043  -6.21950356   1.80473610  -0.30563014 \n         279          280          281          282          283          284 \n  0.51334876  -2.57716179   3.36765509   0.93057408  -8.37628500   2.98575720 \n         285          286          287          288          289          290 \n -5.12460905   4.74955298  -1.77803858  -0.39614069  -8.25044702   2.40298251 \n         291          292          293          294          295          296 \n -1.34095757   2.40298251   9.85991922 -12.25044702   0.56853188  -8.94358795 \n         297          298          299          300          301          302 \n-10.72285546   2.33232766   5.97028546  -1.77803858   7.85991922  -3.12460905 \n         303          304          305          306          307          308 \n 11.33232766   5.22196142  -2.25044702  -1.23497529   1.38751078  -1.41599639 \n         309          310          311          312          313          314 \n -0.90387656   3.63918673   3.27714454   2.69436986  -0.54183436  -0.05395419 \n         315          316          317          318          319          320 \n 14.02546859  -0.47117951  12.93057408  -5.48665124   5.04094032   5.80473610 \n         321          322          323          324          325          326 \n  3.13145087  -3.70738373   1.33232766   9.58400361   0.62371500   5.13145087 \n         327          328          329          330          331          332 \n 11.47802133   3.93057408   1.27714454   4.40298251  -0.90387656   7.56853188 \n         333          334          335          336          337          338 \n  4.62371500   4.40298251   7.22196142   3.42283821   8.93057408  -3.48665124 \n         339          340          341          342          343          344 \n  2.51334876   9.16677830  13.33232766  -1.39614069  10.63918673   3.60385931 \n```\n\n\n:::\n:::\n\n\n## Fitting method: Least squares regression {.smaller background-color=\"aquamarine\"}\n\n- The regression line shall minimize the sum of the squared residuals     \n  (or, identically, their mean). \n- Mathematically: The residual for case $i$ is $e_i = \\hat y_i - y_i$. \n- Now we want to minimize $\\sum_{i=1}^n e_i^2$   \n(or equivalently $\\frac{1}{n}\\sum_{i=1}^n e_i^2$ the *mean of squared errors*, which we will look at later). \n\n## Visualization of residuals {.smaller background-color=\"aquamarine\" .scrollable}\n\nThe residuals are the gray lines between predicted values on the regression line and the actual values. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npengmod <- \n  linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(flipper_length_mm ~ body_mass_g, data = penguins)\npenguins |> bind_cols(predict(pengmod,penguins)) |> \n  ggplot(aes(body_mass_g, flipper_length_mm)) +\n  geom_segment(aes(x = body_mass_g, y = flipper_length_mm, xend = body_mass_g, yend = .pred), color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method=\"lm\") + \n  geom_point(aes(y=.pred), color = \"red\", alpha =  0.3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n## Check: Fitted values and Residuals {.smaller background-color=\"aquamarine\" .scrollable}\n\nRecall: **Residual = Observed value - Predicted value**\n\nThe *Predicted values* are also called *Fitted values*. Hence: \n\nResiduals + Fitted values = Observed values\n\n\n::: {.cell}\n\n```{.r .cell-code}\npengmod$fit$residuals + pengmod$fit$fitted.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21 \n181 186 195 193 190 181 195 193 190 186 180 182 191 198 185 195 197 184 194 174 \n 22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41 \n180 189 185 180 187 183 187 172 180 178 178 188 184 195 196 190 180 181 184 182 \n 42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61 \n195 186 196 185 190 182 179 190 191 186 188 190 200 187 191 186 193 181 194 185 \n 62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81 \n195 185 192 184 192 195 188 190 198 190 190 196 197 190 195 191 184 187 195 189 \n 82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 \n196 187 193 191 194 190 189 189 190 202 205 185 186 187 208 190 196 178 192 192 \n102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n203 183 190 193 184 199 190 181 197 198 191 193 197 191 196 188 199 189 189 187 \n122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 \n198 176 202 186 199 191 195 191 210 190 197 193 199 187 190 191 200 185 193 193 \n142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 \n187 188 190 192 185 190 184 195 193 187 201 211 230 210 218 215 210 211 219 209 \n162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 \n215 214 216 214 213 210 217 210 221 209 222 218 215 213 215 215 215 216 215 210 \n182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 \n220 222 209 207 230 220 220 213 219 208 208 208 225 210 216 222 217 210 225 213 \n202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 \n215 210 220 210 225 217 220 208 220 208 224 208 221 214 231 219 230 214 229 220 \n222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 \n223 216 221 221 217 216 230 209 220 215 223 212 221 212 224 212 228 218 218 212 \n242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 \n230 218 228 212 224 214 226 216 222 203 225 219 228 215 228 216 215 210 219 208 \n262 263 264 265 266 267 268 269 270 271 273 274 275 276 277 278 279 280 281 282 \n209 216 229 213 230 217 230 217 222 214 215 222 212 213 192 196 193 188 197 198 \n283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 \n178 197 195 198 193 194 185 201 190 201 197 181 190 195 181 191 187 193 195 197 \n303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 \n200 200 191 205 187 201 187 203 195 199 195 210 192 205 210 187 196 196 196 201 \n323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 \n190 212 187 198 199 201 193 203 187 197 191 203 202 194 206 189 195 207 202 193 \n343 344 \n210 198 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins$flipper_length_mm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 181 186 195  NA 193 190 181 195 193 190 186 180 182 191 198 185 195 197\n [19] 184 194 174 180 189 185 180 187 183 187 172 180 178 178 188 184 195 196\n [37] 190 180 181 184 182 195 186 196 185 190 182 179 190 191 186 188 190 200\n [55] 187 191 186 193 181 194 185 195 185 192 184 192 195 188 190 198 190 190\n [73] 196 197 190 195 191 184 187 195 189 196 187 193 191 194 190 189 189 190\n [91] 202 205 185 186 187 208 190 196 178 192 192 203 183 190 193 184 199 190\n[109] 181 197 198 191 193 197 191 196 188 199 189 189 187 198 176 202 186 199\n[127] 191 195 191 210 190 197 193 199 187 190 191 200 185 193 193 187 188 190\n[145] 192 185 190 184 195 193 187 201 211 230 210 218 215 210 211 219 209 215\n[163] 214 216 214 213 210 217 210 221 209 222 218 215 213 215 215 215 216 215\n[181] 210 220 222 209 207 230 220 220 213 219 208 208 208 225 210 216 222 217\n[199] 210 225 213 215 210 220 210 225 217 220 208 220 208 224 208 221 214 231\n[217] 219 230 214 229 220 223 216 221 221 217 216 230 209 220 215 223 212 221\n[235] 212 224 212 228 218 218 212 230 218 228 212 224 214 226 216 222 203 225\n[253] 219 228 215 228 216 215 210 219 208 209 216 229 213 230 217 230 217 222\n[271] 214  NA 215 222 212 213 192 196 193 188 197 198 178 197 195 198 193 194\n[289] 185 201 190 201 197 181 190 195 181 191 187 193 195 197 200 200 191 205\n[307] 187 201 187 203 195 199 195 210 192 205 210 187 196 196 196 201 190 212\n[325] 187 198 199 201 193 203 187 197 191 203 202 194 206 189 195 207 202 193\n[343] 210 198\n```\n\n\n:::\n:::\n\n\n\n\n## Proporties of least squares regression  {.smaller background-color=\"aquamarine\"}\n\nThe regression lines goes through the point (`mean(x)`, `mean(y)`). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(penguins$body_mass_g, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4201.754\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(penguins$flipper_length_mm, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 200.9152\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> bind_cols(predict(pengmod,penguins)) |> \n  ggplot(aes(body_mass_g, flipper_length_mm)) +\n  geom_segment(aes(x = body_mass_g, y = flipper_length_mm, xend = body_mass_g, yend = .pred), color = \"gray\") +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method=\"lm\") + \n  geom_point(aes(y=.pred), color = \"red\", alpha =  0.3) + \n  geom_point(data = tibble(x = mean(penguins$body_mass_g, na.rm = T), y = mean(penguins$flipper_length_mm, na.rm = T)), \n  \t\t\t\t\t\t\t\t\t\t\tmapping = aes(x,y), color = \"green\", size = 5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n## Proporties of least squares regression  {.smaller background-color=\"aquamarine\"}\n\nResiduals sum up to zero \n\n\n::: {.cell}\n\n```{.r .cell-code}\npengmod <- linear_reg() |>  set_engine(\"lm\") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)\npengmod$fit$residuals |> sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.135758e-13\n```\n\n\n:::\n:::\n\n\n. . .\n\nThere is no correlation between residuals and the explanatory variable \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.083343e-16\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe correlation of $x$ and $y$ is the slope $\\beta_1$ \"corrected\" by their standard deviations. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrelation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\nsd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)\nsd_mass <- sd(penguins$body_mass_g, na.rm = T)\nc(correlation, sd_flipper, sd_mass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   0.8712018  14.0617137 801.9545357\n```\n\n\n:::\n\n```{.r .cell-code}\ncorrelation * sd_flipper / sd_mass\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01527592\n```\n\n\n:::\n\n```{.r .cell-code}\npengmod$fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)  body_mass_g \n136.72955927   0.01527592 \n```\n\n\n:::\n:::\n\n\n\n## Correlation and linear regression {.smaller background-color=\"aquamarine\"}\n\nWhen the two variables in the linear regression are standardized (standard scores)\n\n- the intercept is zero \n- the coefficient coincides with the correlation\n\n\n\n# Linear Models and Dummy Variables\n\n\n## Explanatory variables are categorical {.smaller}\n\nLet's try what happens with `species` as explanatory variable. \nRemember, we have three species: Adelie, Chinstrap, Gentoo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ species, data = penguins) |> \n\ttidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n```\n\n\n:::\n:::\n\n\nWhat happened? \n\n. . . \n\nTwo of the three species categories appear as variables now.\n\n- Categorical variables are automatically encoded to **dummy variables**\n- Each coefficient describes the expected difference between flipper length of that particular species compared to the baseline level\n- What is the baseline level? [[The first category!]{style='color:red;'} (Here alphabetically `\"Adelie\"`)]{.fragment}\n\n\n## How do dummy variables look? {.smaller}\n\nspecies    | speciesChinstrap | speciesGentoo \n-----------|------------------|--------------\nAdelie     |      0           | 0\nAdelie     |      0           | 0\nChinstrap  |      1           | 0\nGentoo     |      0           | 1\nGentoo     |      0           | 1\nGentoo     |      0           | 1\nAdelie     |      0           | 0\nChinstrap  |      1           | 0\n\nThen the fitting of the linear model is as before using the zero-one variables. \n\n## Interpretation {.smaller background-color=\"khaki\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ species, data = penguins) |> \n\ttidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n```\n\n\n:::\n:::\n\n\n- Flipper length of the baseline species is the intercept.    \n    - Average flipper length of Adelie is 190 mm\n- Flipper length of the two other species add their coefficient\n    - Average flipper length of Chinstrap is 190 + 5.87 mm\n    - Average flipper length of Gentoo is 190 + 27.2 mm\n\n\n## Compare to a visualization {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n\tset_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ species, data = penguins) |> \n\ttidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(species, flipper_length_mm)) + geom_boxplot() +\n \tstat_summary(fun.y=mean, geom=\"point\", size=5, color=\"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_summary()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-31-1.png){width=288}\n:::\n:::\n\n\nThe red dots are the average values for species. \n\n# Linear models and R-squared\n\n## Quality of different linear models? {.smaller} \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n```\n\n\n:::\n:::\n\naverage `flipper_length_mm` $= 137 + 0.0153\\cdot$ `body_mass_g`\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n\tfit(bill_depth_mm ~ bill_length_mm, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     20.9       0.844      24.7  4.72e-78\n2 bill_length_mm  -0.0850    0.0191     -4.46 1.12e- 5\n```\n\n\n:::\n:::\n\n\naverage `bill_depth_mm` $= 20.9 -0.085\\cdot$ `bill_length_mm`\n\n. . .\n\n**Technical:** The idea of the `tidy()` function is to turn an object into a tidy tibble. Here, it extracts the coefficients of the linear model (and more statistical information).\n\n\n\n\n## R-squared of a fitted model {.smaller background-color=\"aquamarine\"}\n\n$R^2$ is the percentage of variability in the response explained by the regression model. \n\nR-squared is also called **coefficient of determination**. \n\n**Definition**: \n\n$R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}$\n\nwhere $SS_\\text{res} = \\sum_i(y_i - f_i)^2 = \\sum_i e_i^2$ is the *sum of the squared residuals*, and   \n$SS_\\text{tot} = \\sum_i(y_i - \\bar y)^2$ the *total sum of squares* which is proportional to the variance of $y$. ($\\bar y$ is the mean of $y$.)\n\n![](https://upload.wikimedia.org/wikipedia/commons/8/86/Coefficient_of_Determination.svg)\n\n\n\n## Linear model R-squared {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |>\n glance()  # glance shows summary statistics of model fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n**Interpretation R-squared?** [75.9% of the variance of flipper length can be explained by a linear relation with body mass. ]{.fragment}\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n\tfit(bill_depth_mm ~ bill_length_mm, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0552        0.0525  1.92      19.9 0.0000112     1  -708. 1422. 1433.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n5.52% of the variance of bill depth can be explained by a linear relation with bill length. \n\n. . . \n\n**Technical:** The idea of the `glance()` function is to construct a single row summary \"glance\" of a model, fit, or other object. \n\n## R-squared and correlation {.smaller}\n\nFor a linear model with one predictor, the square of the correlation coefficient is the same as the R-squared of the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n\tfit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8712018\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7589925\n```\n\n\n:::\n:::\n\n\n. . . \n\nHence, the name $R^2$!\n\n# Linear models with more predictors\n\n## More predictors: Coefficients {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    122.      2.86         42.7  1.70e-138\n2 body_mass_g      0.0131  0.000545     23.9  7.56e- 75\n3 bill_length_mm   0.549   0.0801        6.86 3.31e- 11\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    158.      4.63         34.1  1.80e-111\n2 body_mass_g      0.0109  0.000538     20.3  1.93e- 60\n3 bill_length_mm   0.592   0.0717        8.26 3.42e- 15\n4 bill_depth_mm   -1.68    0.181        -9.29 1.99e- 18\n```\n\n\n:::\n:::\n\n\n## More predictors: `glance` R-squared {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.759         0.758  6.91     1071. 4.37e-107     1 -1146. 2297. 2309.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.788         0.787  6.49      631. 4.88e-115     2 -1123. 2255. 2270.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.831         0.830  5.80      556. 2.99e-130     3 -1084. 2179. 2198.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n##  More predictors: Equations {.smaller}\n\naverage `flipper_length_mm` $= 137 + 0.0153\\cdot$ `body_mass_g`    \n75.9% explained variance\n\naverage `flipper_length_mm` $= 122 + 0.0131\\cdot$ `body_mass_g` $+ 0.549\\cdot$ `bill_length_mm`    \n78.8% explained variance\n\naverage `flipper_length_mm` $= 158 + 0.0109\\cdot$ `body_mass_g` $+ 0.592\\cdot$ `bill_length_mm` $- 1.68\\cdot$ `bill_length_mm`   \n83.1% explained variance\n\n\n# Interaction Effects\n\nAdding products of variables as new variable.\n\n## Adding a categorical variable as main effect {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      159.       2.39         66.6  2.45e-196\n2 body_mass_g        0.00840  0.000634     13.3  1.40e- 32\n3 speciesChinstrap   5.60     0.788         7.10 7.33e- 12\n4 speciesGentoo     15.7      1.09         14.4  6.80e- 37\n```\n\n\n:::\n:::\n\n\nA **main effect** by categorical dummy variables allows for different intercepts per species. (However, the slopes are the same.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.854         0.853  5.40      659. 7.42e-141     3 -1060. 2129. 2149.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nAdding species increases R-squared better than adding bill length and bill depth together!\n\n\n## Adding as interaction effect {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term                          estimate std.error statistic   p.value\n  <chr>                            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                  165.       3.55         46.5  1.56e-148\n2 body_mass_g                    0.00668  0.000952      7.01 1.30e- 11\n3 speciesChinstrap             -13.9      7.30         -1.90 5.84e-  2\n4 speciesGentoo                  6.06     6.05          1.00 3.17e-  1\n5 body_mass_g:speciesChinstrap   0.00523  0.00195       2.68 7.66e-  3\n6 body_mass_g:speciesGentoo      0.00236  0.00135       1.75 8.16e-  2\n```\n\n\n:::\n:::\n\n\n- Note the `*` for interaction effect!\n- Also main effects for both variables are in as coefficients.\n- Adelie is the baseline species (because it is first in the alphabet).\n- An **interaction effect** allows for different slopes for each species!\n\n\n## Regression lines by species {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\nCompare the slopes to the regression output on the slides before!\n\n\n## Different equations for each species! {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term                          estimate std.error statistic   p.value\n  <chr>                            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                  165.       3.55         46.5  1.56e-148\n2 body_mass_g                    0.00668  0.000952      7.01 1.30e- 11\n3 speciesChinstrap             -13.9      7.30         -1.90 5.84e-  2\n4 speciesGentoo                  6.06     6.05          1.00 3.17e-  1\n5 body_mass_g:speciesChinstrap   0.00523  0.00195       2.68 7.66e-  3\n6 body_mass_g:speciesGentoo      0.00236  0.00135       1.75 8.16e-  2\n```\n\n\n:::\n:::\n\n\nAdelie:   \naverage `flipper_length_mm` $= 165 + 0.00668\\cdot$ `body_mass_g`    \n\nChinstrap:   \naverage `flipper_length_mm` $= 165 - 13.6 + (0.00668 + 0.00523)\\cdot$ `body_mass_g`    \n\nGentoo:   \naverage `flipper_length_mm` $= 165 + 6.06 + (0.00668 + 0.00236)\\cdot$ `body_mass_g`    \n\n\n## Improvement of interaction effects is small here {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.854         0.853  5.40      659. 7.42e-141     3 -1060. 2129. 2149.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> \n glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.857         0.855  5.35      404. 9.60e-140     5 -1056. 2125. 2152.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n## Interaction effects of two categoricals {.smaller}\n\nAssume $x_1$ and $x_2$ are categorical variables. We add their product in the linear model as a *new variable*: \n$y_i = \\beta_0 + \\beta_1x_1  + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots$. \n\n**Example:** $x_1$ is *gender female* and $x_2$ *having kids* \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(gender_female = c(0,1,1,0), has_kids = c(1,0,1,0)) |> \n mutate(gender_female_x_has_kids = gender_female * has_kids) |> \n knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| gender_female| has_kids| gender_female_x_has_kids|\n|-------------:|--------:|------------------------:|\n|             0|        1|                        0|\n|             1|        0|                        0|\n|             1|        1|                        1|\n|             0|        0|                        0|\n\n\n:::\n:::\n\n\n- What is the baseline? [*Being male without kids.*]{.fragment}\n- Thought experiment: When we estimate a model explaining life satisfaction with these. How would we see if being a mother increases life satisfaction more than being a father? [*positive coefficient for `gender_female_x_has_kids`*]{.fragment}\n\n\n# Nonlinear Models\n\n## When a linear model is bad\n\nExample: Total corona cases in Germany in the first wave 2020. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhofull <- read_csv(\"data/WHO-COVID-19-global-data.csv\", show_col_types = FALSE) |> \n\tfilter(Country == \"Germany\") \nwho <- whofull |> \n\tfilter(Date_reported < \"2020-03-20\", Date_reported > \"2020-02-25\") \nwho |> \n\tggplot(aes(Date_reported, Cumulative_cases)) + \n\tgeom_line() + geom_point() + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\n## $\\log$ transformation {.smaller}\n\nInstead of `Cumulative_cases` we look at $\\log($`Cumulative_cases`$)$\n\n::: {.cell}\n\n```{.r .cell-code}\n\twho |> \n\tggplot(aes(Date_reported, log(Cumulative_cases))) + geom_line() + geom_point() +\n\tgeom_smooth(method = \"lm\") + theme_minimal(base_size = 20)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\nAlmost perfect fit of the linear model: $\\log(y)=\\log(\\beta_0) + \\beta_1\\cdot x$   \n($y=$ `Cumulative cases`, $x=$ Days)\n\n. . . \n\nExponentiation gives the model: $y=\\beta_0 e^{\\beta_1\\cdot x}$    \n(Check $e^{\\log(\\beta_0) + \\beta_1\\cdot x} = e^{\\log(\\beta_0)} e^{\\beta_1\\cdot x} = \\beta_0 e^{\\beta_1\\cdot x}$)\n\n## Exponential growth! {.smaller}\n\n[$y=\\beta_0 e^{\\beta_1\\cdot x}$]{style='color:red;'}  \nFor comparison: Logistic function [$y = \\frac{N \\beta_0 e^{\\beta_1\\cdot x}}{N + \\beta_0 e^{\\beta_1\\cdot x}}$]{style='color:blue;'}   for $N=200000$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwholm <- \twho |> \n lm(log(Cumulative_cases) ~ Date_reported, data = _) \nwho |> \n\tggplot(aes(Date_reported, Cumulative_cases)) + geom_line() + geom_point() +\n geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = \"red\") + \n geom_function(\n  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = \"blue\") +\n theme_minimal(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\nLogistic growth (as in the SI model) mimicks exponential growth initially. \n \n\n## $\\log$ transformation {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\twho |> \n\tggplot(aes(Date_reported, log10(Cumulative_cases))) + geom_line() + geom_point() +\n\tgeom_smooth(method = \"lm\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\nAn important difference of `log10(Cumulative_cases) ~ Date_Reported` to the penguin model `flipper_length_mm ~ body_mass_g`?\n?\n\n. . . \n\n- $x$ has an ordered structure and no duplicates\n\nThe fit looks so good. Why?\n\n. . . \n\nBecause there is a *mechanistic explanation* behind: The SI model. \n\n## However, it works only in a certain range {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwho |> \n\tggplot(aes(Date_reported, Cumulative_cases)) + geom_point() +\n geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = \"red\") + \n geom_function(\n  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = \"blue\") +\n theme_minimal(base_size = 20) +\n geom_line(data = whofull |> filter(Date_reported < \"2020-06-30\")) + ylim(c(0,300000))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 52 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\n## However, it works only in a certain range (log scale on y) {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwho |> \n\tggplot(aes(Date_reported, Cumulative_cases)) + geom_point() +\n geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = \"red\") + \n geom_function(\n  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = \"blue\") +\n theme_minimal(base_size = 20) +\n geom_line(data = whofull |> filter(Date_reported < \"2020-06-30\")) + ylim(c(0,300000)) +\n scale_y_log10()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W07_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n\n",
    "supporting": [
      "W07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}