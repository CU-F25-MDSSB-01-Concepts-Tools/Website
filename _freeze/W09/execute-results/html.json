{
  "hash": "7d2745f6dc61bc696bbe9ea80fc0506a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#09: Hypothesis Testing, Classification and Regression with Decision Trees, Overfitting\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    smaller: true\n    toc: true\n    toc-depth: 1\n    slide-number: true\n    chalkboard: \n      buttons: true\n    preview-links: false\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n## Packages used here\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.1     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.3.0\n✔ modeldata    1.5.1     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(openintro)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:modeldata':\n\n    ames\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'palmerpenguins'\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n:::\n\n\n\n# Hypothesis testing\nLarge part of the content adapted from <http://datasciencebox.org>.\n\n\n## Organ donors {.smaller}\n\n**Background story:** People providing an organ for donation sometimes seek the help of a special \"medical consultant\". These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant's clients. \n\n. . . \n\n**Example track record:** One consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).\n\n. . . \n\n[Is this strong evidence for a good track record?]{style='color:red;'}\n\n## Data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\norgan_donor <- tibble(\n  outcome = c(rep(\"complication\", 3), rep(\"no complication\", 59))\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\norgan_donor |>\n  count(outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  outcome             n\n  <chr>           <int>\n1 complication        3\n2 no complication    59\n```\n\n\n:::\n:::\n\n\n\n## Parameter vs. statistic {.smaller}\n\nA **parameter** for a hypothesis test is the \"true\" value of interest. We typically estimate the parameter using a **sample statistic** as a **point estimate**.\n\n$q$: true rate of complication, here 0.1 (10% complication rate in US)\n\n$\\hat{q}$: rate of complication in the sample = $\\frac{3}{62}$ = \n0.048 (This is the point estimate.)\n\n\n## Correlation vs. causation  {.smaller}\n\n**Is it possible to infer the consultant's claim using the data?**\n\nNo. The claim is: *There is a causal connection*. However, the data are observational.\nFor example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate (for example).\n\n. . .\n\n**What is possible?**\n\nWhile it is not possible to assess the causal claim, it is still possible to test for an association using these data. For this question we ask, **could the low complication rate of $\\hat{q}$ = 0.048 be due to chance?**\n\n\n\n## Two claims\n\n- [**Null hypothesis $H_0$:**]{style='color:blue;'} \"There is nothing going on\"\n\nComplication rate for this consultant is no different than the US average of 10%\n\n- [**Alternative hypothesis $H_1$:**]{style='color:blue;'} \"There is something going on\"\n\nComplication rate for this consultant is **lower** than the US average of 10%\n\n\n## Understand hypothesis testing as a court trial {.smaller}\n\n- **Null hypothesis**, $H_0$: Defendant is innocent\n- **Alternative hypothesis**, $H_A$: Defendant is guilty\n\n\n- **Present the evidence:** Collect data\n\n- **Judge the evidence:** \"Could these data plausibly have happened by chance if the null hypothesis were true?\"\n    * Yes: Fail to reject $H_0$\n    * No: Reject $H_0$\n    \n\n## Hypothesis testing framework {.smaller}\n\n- Start with a null hypothesis, $H_0$, that represents the status quo\n\n- Set an alternative hypothesis, $H_A$, that represents the research question, i.e. what we are testing for\n\n- Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a **p-value**.    \n**Definition p-value:** *Probability of observed or more extreme outcome given that the null hypothesis is true.*\n    - if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\n    - if they do, then reject the null hypothesis in favor of the alternative\n\n\n## Setting the hypotheses {.smaller}\n\nWhich of the following is the correct set of hypotheses for the claim that the consultant has lower complication rates?\n\n(a) $H_0: q = 0.10$; $H_A: q \\ne 0.10$ \n\n(b) $H_0: q = 0.10$; $H_A: q > 0.10$ \n\n(c) $H_0: q = 0.10$; $H_A: q < 0.10$ \n\n(d) $H_0: \\hat{q} = 0.10$; $H_A: \\hat{q} \\ne 0.10$ \n\n(e) $H_0: \\hat{q} = 0.10$; $H_A: \\hat{q} > 0.10$ \n\n(f) $H_0: \\hat{q} = 0.10$; $H_A: \\hat{q} < 0.10$ \n\n. . . \n\n(c) is correct.   \nHypotheses are about the true rate of complication $q$ not the observed ones $\\hat{q}$\n\n\n# Simulation for Hypothesis Testing\n\n## Simulating the null distribution {.smaller}\n\nSince $H_0: q = 0.10$, we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10.\n\n**How should we simulate the null distribution for this study using a bag of chips?**\n\n- How many chips? [For example 10 which makes 10% choices possible]{.fragment style='color:red;'}\n- How many colors? [2]{.fragment style='color:red;'}\n- What should colors represent? [\"complication\", \"no complication\"]{.fragment style='color:red;'}\n- How many draws?  [62 as the data]{.fragment style='color:red;'}\n- With replacement or without replacement? [With replacement]{.fragment style='color:red;'}\n\nWhen sampling from the null distribution, what would be the expected proportion of \"complications\"?  [0.1]{.fragment  style='color:red;'}\n\n\n## Simulation! {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234) # The seed is set for the sake of stable presentation outcomes.\noutcomes <- c(\"complication\", \"no complication\")\nsim1 <- sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\nsim1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [5] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [9] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[13] \"no complication\" \"complication\"    \"no complication\" \"no complication\"\n[17] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[21] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[25] \"no complication\" \"no complication\" \"no complication\" \"complication\"   \n[29] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[33] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[37] \"no complication\" \"no complication\" \"complication\"    \"no complication\"\n[41] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[45] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[49] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[53] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[57] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[61] \"no complication\" \"no complication\"\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(sim1 == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0483871\n```\n\n\n:::\n:::\n\n\nOh OK, this was exactly the consultant's rate. But maybe it was a rare event? Let us repeat it. \n\n## More simulation! {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_sim <- function() sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\n\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1290323\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1290323\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09677419\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09677419\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1774194\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1129032\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(one_sim() == \"complication\")/62\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1129032\n```\n\n\n:::\n:::\n\n\n\n## Automating with `tidymodels`^[Of course, you can also do it in your own way without packages.] {.smaller}\n\n\n:::: {.columns}\n\n::: {.column width='40%'}\n\n::: {.cell}\n\n```{.r .cell-code}\norgan_donor\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 62 × 1\n   outcome        \n   <chr>          \n 1 complication   \n 2 complication   \n 3 complication   \n 4 no complication\n 5 no complication\n 6 no complication\n 7 no complication\n 8 no complication\n 9 no complication\n10 no complication\n# ℹ 52 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='60%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 100, type = \"draw\") |> \n  calculate(stat = \"prop\")\nnull_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 100 × 2\n   replicate   stat\n       <int>  <dbl>\n 1         1 0.0323\n 2         2 0.0645\n 3         3 0.0968\n 4         4 0.0161\n 5         5 0.161 \n 6         6 0.0968\n 7         7 0.0645\n 8         8 0.129 \n 9         9 0.161 \n10        10 0.0968\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n\n\n## Visualizing the null distribution {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n# p-value\n\n## Calculating the p-value, visually {.smaller}\n\n**What is the p-value:** *How often was the simulated sample proportion at least as extreme as the observed sample proportion?*^[This a one-sided hypothesis test because we are looking for extremely lower rates not also extremely higher rates. We do one-sided test if we know the direction of interest, here \"low\" and \"lower\" rates.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")  +\n geom_vline(xintercept = 3/62, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Calculating the p-value, directly\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n summarise(p_value = sum(stat <= 3/62)/n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1    0.13\n```\n\n\n:::\n:::\n\n\nThis is the fraction of simulations where complications was equal or below 0.0483871. \n\n\n## Significance level  {.smaller}\n\n- A **significance level** $\\alpha$ is a threshold we make up to make our judgment about the plausibility of the null hypothesis being true given the observed data. \n\n- We often use $\\alpha = 0.05 = 5\\%$ as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model. \n\n- If p-value < $\\alpha$, reject $H_0$ in favor of $H_A$: The data provide convincing evidence for the alternative hypothesis.\n\n- If p-value > $\\alpha$, fail to reject $H_0$ in favor of $H_A$: The data do not provide convincing evidence for the alternative hypothesis.\n\n**What is the conclusion of the hypothesis test?**\n\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. \nThese data do not provide convincing evidence that this consultant incurs a lower complication rate than the 10% overall US complication rate.\n\n## 100 simulations is not sufficient {.smaller}\n\n- We simulate 15,000 times to get an accurate distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 15000, type = \"simulate\") |> \n  calculate(stat = \"prop\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe `\"simulate\"` generation type has been renamed to `\"draw\"`. Use `type =\n\"draw\"` instead to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_vline(xintercept = 3/62, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Our more robust p-value \n\nFor the null distribution with 15,000 simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n  filter(stat <= 3/62) |>\n  summarise(p_value = n()/nrow(null_dist))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.125\n```\n\n\n:::\n:::\n\n\nOh OK, our first p-value was much more borderline in favor of the alternative hypothesis.\n\n## p-value in model outputs {.smaller}\n\nModel output for a linear model with palmer penguins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> set_engine(\"lm\") |> \n\tfit(bill_length_mm ~ bill_depth_mm, data = penguins) |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     55.1       2.52      21.9  6.91e-67\n2 bill_depth_mm   -0.650     0.146     -4.46 1.12e- 5\n```\n\n\n:::\n:::\n\n\nModel output for a logistic regression model with email from [openintro](https://www.openintro.org/data/index.php?data=email)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>  set_engine(\"glm\") |>\n  fit(spam ~ from + cc, data = email, family = \"binomial\") |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  13.6      309.        0.0439   0.965\n2 from1       -15.8      309.       -0.0513   0.959\n3 cc            0.00423    0.0193    0.220    0.826\n```\n\n\n:::\n:::\n\n\nWhat do the p-values mean? What is the null hypothesis?\n\n. . .\n\n**Null-Hypothesis:** There is no relationship between the predictor variable and the response variable, that means that the coefficient is equal to zero. \n\n**Smaller p-value** $\\to$ more evidence for rejecting the hypothesis that there is no effect. \n\n## xkcd on p-values {.smaller}\n\n:::: {.columns}\n\n::: {.column width='60%'}\n[![](https://imgs.xkcd.com/comics/p_values_2x.png){height=\"500px\"}](https://xkcd.com/1478/)\n[![](https://imgs.xkcd.com/comics/significant.png){height=\"500px\"}](https://xkcd.com/882/)\n:::\n\n::: {.column width='40%'}\n:::{.fragment}\n- Significance levels are fairly arbitrary. Sometimes they are used (wrongly) as definitive judgments\n- They can even be used to do *p-hacking*: Searching for \"significant\" effects in observational data\n- In parts of science it has become a \"gamed\" performance metric.\n- The p-value says nothing about effect size!\n:::\n:::\n\n::::\n\n## p-value misinterpretation {.smaller}\n\np-values do not measure^[From the American Statistical Association (ASA) 2016]\n\n- the probability that the studied hypothesis is true\n- the probability that the data were produced by random chance alone\n- the size of an effect\n- the \"importance of a result\" or \"evidence regarding a model or hypothesis\" (it is only against the null hypothesis).\n\n. . .\n\nCorrect:   \nThe p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\np-values and significance tests, when properly applied and interpreted, increase the rigor of the conclusions drawn from data.^[From the American Statistical Association (ASA) 2019]\n\n\n\n# Classification: Compare logistic regression and decision tree\n\n## Purpose of the section {.smaller}\n\n- Go again through the modeling workflow (with `tidymodels`) and see that large parts are identical\n- Look again at the coefficients of a logistic regression model\n- Learn the basic idea of a decision tree (you will not learn the details here)\n- Do the classification with both models and compare the confusion matrices\n\n## Specify recipe and models {.smaller}\n\nFor both logistic regression and decision tree:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_recipe <- recipe(sex ~ ., data = penguins) |> \n step_rm(year)\n```\n:::\n\n\n. . .\n\n- We specify a recipe to predict `sex` with all available variables in `penguins`\n    - In typical model development, more pre-processing steps are done. \n\n. . .\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n**Logistic Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_logreg <- logistic_reg() |>\n set_engine(\"glm\")\n```\n:::\n\n\n`peng_logreg` specifies to fit with `glm` (generalized linear model from `base` R) \n:::\n\n::: {.column width='50%'}\n\n**Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_tree <- decision_tree() |>\n set_engine(\"rpart\") |>\n set_mode(\"classification\")\n```\n:::\n\n\n`peng_tree` specifies to fit  for `classification` with the `rpart`^[`rpart` is a package for *recursive partitioning and regression trees*. Different decision tree procedures are subsumed as **classification and regression trees (CART)**] as engine. \n\n:::\n::::\n\n\n## Split and fit {.smaller}\n\nFor logistic regression and decision tree:\n\nSplit into test and training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\npenguins_split <- initial_split(penguins, prop = 0.7, strata = sex)\npeng_train <- training(penguins_split)\npeng_test <- testing(penguins_split)\npeng_workflow <- workflow() |> add_recipe(peng_recipe)\n```\n:::\n\n\n. . .\n\n:::: {.columns}\n\n::: {.column width='50%'}\n**Logistic Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_logreg_fit <- peng_workflow |> \n add_model(peng_logreg) |> \n fit(data = peng_train)\n```\n:::\n\n\n:::\n\n::: {.column width='50%'}\n**Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_tree_fit <- peng_workflow |> \n add_model(peng_tree) |> \n fit(data = peng_train)\n```\n:::\n\n:::\n\n::::\n\n\n## Look at fitted logistic regression {.smaller}\n\n:::: {.columns}\n\n::: {.column width='60%'}\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_logreg_fit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  term               estimate std.error statistic      p.value\n  <chr>                 <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)       -90.4      16.7        -5.41  0.0000000632\n2 speciesChinstrap   -6.06      1.90       -3.19  0.00142     \n3 speciesGentoo      -8.65      3.49       -2.48  0.0132      \n4 islandDream         0.752     1.15        0.656 0.512       \n5 islandTorgersen     0.380     1.13        0.335 0.737       \n6 bill_length_mm      0.532     0.150       3.56  0.000378    \n7 bill_depth_mm       1.79      0.456       3.93  0.0000866   \n8 flipper_length_mm   0.0552    0.0620      0.891 0.373       \n9 body_mass_g         0.00693   0.00162     4.27  0.0000193   \n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='40%'}\n- What do the categorical predictors tell us? Which are signigficant?\n- What do the numerical predictors tell us? Which are signigficant?\n- Why is the coefficient for `body_mass_g` so small, but highly significant?\n:::\n\n::::\n \n. . . \n\n\n**Categorical predictors:** We have 3 `species`, 3 `island`. So, we see\n4 new variables, 2 for `species` and 2 for `island` (the third is the reference category). Species are significant (p < 0.05), but islands not.\n\n**Numerical predictors:** `flipper_length_mm` is insignificant, though its coefficient is larger than for `body_mass_g`. Reason: values of `body_mass_g` are larger than those of `flipper_length_mm`. Body mass differs by much more grams than flipper length differs by millimeters. \n\n\n## What is a decision tree? {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_tree_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_rm()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn=234 (6 observations deleted due to missingness)\n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 234 117 female (0.50000000 0.50000000)  \n   2) bill_length_mm< 48.15 166  57 female (0.65662651 0.34337349)  \n     4) bill_depth_mm< 18.05 103  14 female (0.86407767 0.13592233)  \n       8) body_mass_g< 4987.5 88   4 female (0.95454545 0.04545455) *\n       9) body_mass_g>=4987.5 15   5 male (0.33333333 0.66666667) *\n     5) bill_depth_mm>=18.05 63  20 male (0.31746032 0.68253968)  \n      10) body_mass_g< 3875 29  10 female (0.65517241 0.34482759)  \n        20) flipper_length_mm< 190.5 18   3 female (0.83333333 0.16666667) *\n        21) flipper_length_mm>=190.5 11   4 male (0.36363636 0.63636364) *\n      11) body_mass_g>=3875 34   1 male (0.02941176 0.97058824) *\n   3) bill_length_mm>=48.15 68   8 male (0.11764706 0.88235294)  \n     6) body_mass_g< 5175 35   8 male (0.22857143 0.77142857)  \n      12) bill_depth_mm< 18 9   3 female (0.66666667 0.33333333) *\n      13) bill_depth_mm>=18 26   2 male (0.07692308 0.92307692) *\n     7) body_mass_g>=5175 33   0 male (0.00000000 1.00000000) *\n```\n\n\n:::\n:::\n\n\n- A sequence of rules for yes/no decisions\n- Selects variables and thresholds which separate the data to predict (here `sex`) best \n- Further details are not the scope of this course\n\n\n## Show rules {.smaller}\n\nWe \"dig out\" the original fitted `rpart`-object from the `workflow`-object with `peng_tree_fit$fit$fit$fit` and plot it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.rules(peng_tree_fit$fit$fit$fit, roundint=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ..y                                                                                                 \n 0.05 when bill_length_mm <  48 & body_mass_g <  4988 & bill_depth_mm <  18                           \n 0.17 when bill_length_mm <  48 & body_mass_g <  3875 & bill_depth_mm >= 18 & flipper_length_mm <  191\n 0.33 when bill_length_mm >= 48 & body_mass_g <  5175 & bill_depth_mm <  18                           \n 0.64 when bill_length_mm <  48 & body_mass_g <  3875 & bill_depth_mm >= 18 & flipper_length_mm >= 191\n 0.67 when bill_length_mm <  48 & body_mass_g >= 4988 & bill_depth_mm <  18                           \n 0.92 when bill_length_mm >= 48 & body_mass_g <  5175 & bill_depth_mm >= 18                           \n 0.97 when bill_length_mm <  48 & body_mass_g >= 3875 & bill_depth_mm >= 18                           \n 1.00 when bill_length_mm >= 48 & body_mass_g >= 5175                                                 \n```\n\n\n:::\n:::\n\n\n- The first three rules would predict `female` for all observations\n- The last five rules would predict `male` for all observations\n\nThe order of `male` and `female` is because `sex` is a factor with the first level `female` and the second level `male`. The probabilities in front of the rule-text are for the second level: `male`.\n\n## Visualize tree {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(peng_tree_fit$fit$fit$fit, \n           roundint=FALSE, tweak=1.5)\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nHow to read?\n\n- The percentage is the fraction of the total cases in this group\n- The probability-number is the fraction of observations which are male in the group\n- `male` of `female` and color would match the predicted outcome at this decision node\n\n\n## Make predictions for test data {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_logreg_pred <- \n predict(peng_logreg_fit, peng_test) |> \n bind_cols(peng_test) \npeng_logreg_pred |> \n conf_mat(truth = sex, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction female male\n    female     46   10\n    male        2   41\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_tree_pred <- \n predict(peng_tree_fit, peng_test) |> \n bind_cols(peng_test)\npeng_tree_pred |> \n conf_mat(truth = sex, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction female male\n    female     43   12\n    male        5   39\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n- The logistic regression has more correct predictions. \n- [Warning:]{style='color:red;'} The function `conf_mat` (from `yardstick` of `tidymodels`) shows the transposed confusion matrix compared with [Wikipedia:Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix). In `conf_mat`, the *true conditions* are in columns. The wikipedia convention is that columns are the *predicted conditions*. \n\n\n\n\n# Regression: Compare linear regression and decision tree\n\n## Purpose of the section {.smaller}\n\n- Go again through the modeling workflow (with `tidymodels`) and see that large parts are identical\n- Look again at the coefficients of a linear model\n- See how the decision tree looks like for a regression problem\n- Compare the two most common performance measures for regression models: Root Mean Squared Error (RMSE) and R-squared\n\n## Specify recipe and models {.smaller}\n\n- We specify a recipe to predict `body_mass_g` with all available variables in `penguins` and put it in a workflow\n    - Typically, more pre-processing steps are specified here, but we are mostly fine\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_recipe2 <- recipe(body_mass_g ~ ., data = penguins) |> \n step_rm(year)\npeng_workflow2 <- workflow() |> \n add_recipe(peng_recipe2)\n```\n:::\n\n\nWe can re-use the split and the training and test set. \n\n\n. . .\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n**Linear Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_linreg <- linear_reg() |>\n set_engine(\"lm\")\n```\n:::\n\n:::\n\n::: {.column width='50%'}\n\n**Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree <- decision_tree() |>\n set_engine(\"rpart\") |>\n set_mode(\"regression\")\n```\n:::\n\n:::\n::::\n\n\n## Fit Regression Models {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n**Linear Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_linreg_fit <- peng_workflow2 |> \n add_model(peng_linreg) |> \n fit(data = peng_train)\n```\n:::\n\n\n:::\n\n::: {.column width='50%'}\n**Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_fit <- peng_workflow2 |> \n add_model(peng_regtree) |> \n fit(data = peng_train)\n```\n:::\n\n:::\n\n::::\n\n\n## Look at fitted linear regression {.smaller}\n\n:::: {.columns}\n\n::: {.column width='60%'}\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_linreg_fit |> tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)         -829.     701.      -1.18  2.38e- 1\n2 speciesChinstrap    -268.     109.      -2.47  1.44e- 2\n3 speciesGentoo       1010.     164.       6.15  3.52e- 9\n4 islandDream          -14.5     70.0     -0.208 8.36e- 1\n5 islandTorgersen      -21.4     75.7     -0.283 7.77e- 1\n6 bill_length_mm        19.8      8.55     2.32  2.13e- 2\n7 bill_depth_mm         53.7     23.9      2.25  2.56e- 2\n8 flipper_length_mm     13.6      3.54     3.84  1.58e- 4\n9 sexmale              393.      58.5      6.71  1.57e-10\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='40%'}\n- What do the categorical predictors tell us? Which are signigficant?\n- What do the numerical predictors tell us? Which are signigficant?\n:::\n\n::::\n \n. . . \n\n\n**Categorical predictors:** We have 3 `species`, 3 `island` and 2 `sex`. So, we see\n5 new variables. Species and sex are significant, but islands not.\n\n**Numerical predictors:** All 3 are significant.\n\n\n## Show rules {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.rules(peng_regtree_fit$fit$fit$fit, roundint=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ..y                                                    \n 3418 when species is Adelie or Chinstrap & sex is female\n 3961 when species is Adelie or Chinstrap & sex is   male\n 4693 when species is              Gentoo & sex is female\n 5474 when species is              Gentoo & sex is   male\n```\n\n\n:::\n:::\n\n\n- For each terminal node a certain value is predicted (the mean of the remaining penguins)\n\n\n## Visualize tree {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(peng_regtree_fit$fit$fit$fit, \n           roundint=FALSE, tweak=1.5)\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nHow to read?\n\n- The percentage is the fraction of the total cases in this group\n- The number is the predicted outcome (mean body mass) at this decision node\n\n\n## Make predictions for test data {.smaller}\n\n:::: {.columns}\n\n::: {.column width='65%'}\n**Linear Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_linreg_pred <- \n predict(peng_linreg_fit, peng_test) |> \n bind_cols(peng_test) \npeng_linreg_pred |> \n select(.pred, body_mass_g, everything()) |> \n slice(10*(1:10)) # show selected rows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 9\n   .pred body_mass_g species   island    bill_length_mm bill_depth_mm\n   <dbl>       <int> <fct>     <fct>              <dbl>         <dbl>\n 1 3349.        3700 Adelie    Torgersen           36.6          17.8\n 2 3979.        3950 Adelie    Dream               38.8          20  \n 3 3902.        3800 Adelie    Dream               36.3          19.5\n 4 4106.        3875 Adelie    Torgersen           41.4          18.5\n 5 3499.        3400 Adelie    Dream               40.2          17.1\n 6 4597.        4150 Gentoo    Biscoe              42            13.5\n 7 4688.        4200 Gentoo    Biscoe              45.5          13.9\n 8 4929.        4850 Gentoo    Biscoe              48.5          15  \n 9 4014.        4150 Chinstrap Dream               52            19  \n10 4108.        4050 Chinstrap Dream               50.7          19.7\n# ℹ 3 more variables: flipper_length_mm <int>, sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='35%'}\n**Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_pred <- \n predict(peng_regtree_fit, peng_test) |> \n bind_cols(peng_test)\npeng_regtree_pred |> \n select(.pred, body_mass_g, species, sex) |> \n slice(10*(1:10)) # show same selected rows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   .pred body_mass_g species   sex   \n   <dbl>       <int> <fct>     <fct> \n 1 3418         3700 Adelie    female\n 2 3961.        3950 Adelie    male  \n 3 3961.        3800 Adelie    male  \n 4 3961.        3875 Adelie    male  \n 5 3418         3400 Adelie    female\n 6 4693.        4150 Gentoo    female\n 7 4693.        4200 Gentoo    female\n 8 4693.        4850 Gentoo    female\n 9 3961.        4150 Chinstrap male  \n10 3961.        4050 Chinstrap male  \n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n## Regression Performance Evaluation {.smaller}\n\n[**R-squared**]{style='color:blue;'}: Percentage of variability in `body_mass_g` explained by the model\n\n:::: {.columns}\n\n::: {.column width='50%'}\n**Linear Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq(peng_linreg_pred, \n    truth = body_mass_g, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.861\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n**Decision Tree**\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq(peng_regtree_pred, \n    truth = body_mass_g, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.806\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n[**Root Mean Squared Error (RMSE)**:]{style='color:blue;'} $\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}$  \nwhere $\\hat{y}_i$ is the predicted value and $y_i$ the true value. (The name RMSE is descriptive.)\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse(peng_linreg_pred, \n     truth = body_mass_g, \n     estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        290.\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse(peng_regtree_pred, \n     truth = body_mass_g, \n     estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        337.\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\nWhich model is better in prediction? [**Linear regression.** The R-squared is higher.]{.fragment}\n\n\n## What RMSE is better? {.smaller}\n\n. . .\n\n**Lower. The lower the error, the better the model's prediction.**\n\nIt is the other way round than R-squared! Do not confuse them.\n\n. . .\n\nNotes: \n\n- The common method to fit a linear model is the *ordinary least squares* (OLS) method\n- That means the fitted parameters should deliver the lowest possible sum of squared errors (SSE) between predicted and observed values. \n- Minimizing the sum of squared errors (SSE) is identical to minimizing the mean of squared errors (MSE) because it only adds the factor $1/n$.\n- Minimizing the mean of squared errors (MSE) is identical to minimizing the root mean of squared errors (RMSE) because the square root is strictly monotone function.\n\nConclusion: RMSE can be seen as a definition of the OLS optimization goal. \n\n## Interpreting RMSE {.smaller}\n\nIn contrast to R-squared, RMSE can only be interpreted with knowledge about the range and of the response variable! It also has the same unit (grams for `body_mass_g`).\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npeng_test |> \n ggplot(aes(x=body_mass_g, fill = species)) + \n geom_histogram(binwidth = 50) + \n theme_minimal(base_size = 20)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npeng_linreg_pred |> \n ggplot(aes(x=.pred, fill = species)) + \n geom_histogram(binwidth = 50) + \n theme_minimal(base_size = 20)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\npeng_regtree_pred |> \n ggplot(aes(x=.pred, fill = species)) + \n geom_histogram(binwidth = 50) + \n theme_minimal(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n\n\nThe RMSE shows many grams predicted values deviate from the true value on average. (Taking the squaring of differences and root of the average into account.)\n\n\n# Overfitting with decision trees\n\n## Compare training and test data {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n**Linear Regression** Predict with test data:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(peng_linreg_fit, peng_test) |> bind_cols(peng_test) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     290.   \n2 rsq     standard       0.861\n```\n\n\n:::\n:::\n\n\nPredict with training data:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(peng_linreg_fit, peng_train) |> bind_cols(peng_train) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     283.   \n2 rsq     standard       0.880\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n\n**Decision Tree** Predict with test data:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(peng_regtree_fit, peng_test) |> bind_cols(peng_test) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     337.   \n2 rsq     standard       0.806\n```\n\n\n:::\n:::\n\n\nPredict with training data:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(peng_regtree_fit, peng_train) |> bind_cols(peng_train) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     309.   \n2 rsq     standard       0.857\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\nWhere is the prediction better? (Lower RMSE, higher R-squared)\n\n. . . \n\nPerformance is better for training data (compare values to testing data of the same model). Why? It was used to fit. The model is optimized to predict the training data.\n\n\n## Make a deeper tree {.smaller}\n\n- In `decision_tree()` we can set the maximal depth of the tree to 30.   \n- The trees we had before were also automatically *pruned* by sensible defaults.  \n- By setting the cost complexity parameter to -1 we avoid pruning.^[The details of this go beyond the scope of this course.]  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_deep <- decision_tree(\n cost_complexity = -1,\n tree_depth = 30\n) |>\n set_engine(\"rpart\") |>\n set_mode(\"regression\")\npeng_regtree_deep_fit <- peng_workflow2 |> add_model(peng_regtree_deep) |> \n fit(data = peng_train)\n```\n:::\n\n\n## The deep tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(peng_regtree_deep_fit$fit$fit$fit, \n           roundint=FALSE, tweak=1)\n```\n\n::: {.cell-output-display}\n![](W09_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Compare pruned and deep tree {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n**Pruned decision tree**\n\nPredict with **training data**:\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_fit |> \n predict(new_data = peng_train) |> bind_cols(peng_train) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     309.   \n2 rsq     standard       0.857\n```\n\n\n:::\n:::\n\n\nPredict with **testing data**:\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_fit |> \n predict(peng_test) |> bind_cols(peng_test) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     337.   \n2 rsq     standard       0.806\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width='50%'}\n\n**Deep decision tree**\n\nPredict with **training data**:\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_deep_fit |> \n predict(new_data = peng_train) |> bind_cols(peng_train) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     247.   \n2 rsq     standard       0.908\n```\n\n\n:::\n:::\n\n\nPredict with **testing data**:\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_regtree_deep_fit |> \n predict(peng_test) |> bind_cols(peng_test) |> \n metrics(truth = body_mass_g, estimate = .pred) |> slice(1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     333.   \n2 rsq     standard       0.808\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n\n## More in an R-script\n\n- Overfitting in spam predicton models.\n\n- From decision trees to random forrests. \n\nMove to the [script](https://raw.githubusercontent.com/CU-F24-MDSSB-01-Concepts-Tools/Website/refs/heads/main/emails_decision_tree_overfitting_randomforrest.r).\n\n\n\n\n# What do we mean by \"model\"? How to be specific\n\n## \"Model\" in Statistical Learning {.smaller}\n\nWe already had the difference between *variable-based* and *agent-based* models in earlier lectures.\n\nBut even in the variable-based model setting of **statistical learning**, the term *model* can be more or less abstract: \n\n1. Very general: $Y = f(X_1, \\dots, X_m) + \\varepsilon$ where $Y$ is the response variable and $X_i$ are features which we put in our **model**: the abstract and unknown function $f$.   \n$\\varepsilon$ is the *error* which we can never explain usually do not know. \n2. More specific: The **model** $f$ could already be of a specific type, like *logistic regression*, a *decision tree* or other functional forms. As this need not be the *real* function we may call it *assumed model* $\\hat f$   For example a linear model $\\hat f(X_1, \\dots, X_m) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_m X_m + \\varepsilon$. Now, the model has specified parameters which values are unknown. \n\n\n## More specific: Fitted model {.smaller}\n\n3. Fitted model: When we have a data set with values for $Y, X_1, \\dots, X_m$ we can fit values for the parameters $\\hat\\beta_0, \\dots, \\hat\\beta_m$ to the data. This is the *fitted model* $\\hat f$. This is called parameter estimation: We estimate $\\hat\\beta_0, \\dots, \\hat\\beta_m$ with the hope that they match the *real* values $\\beta_0, \\dots, \\beta_m$ and that the linear model $\\hat f$ matches the *real* function $f$. \n4. Now we could specify further to fit a specific parameterized model with a specific algorithm, and a specific set of hyperparameters, and maybe more ...\n    \nSometimes model means only a certain aspect of all these, for example the *formula* like `sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + species + island`\n\n. . .\n\n[Take away:]{style='color:blue;'} \"Model\" can mean things with very different granularity. That is OK because they are all related and all fit the definition of being a **simplified representation of reality**. \n\n[**Be prepared to specify what you mean when you are talking about a model.**]{style='color:blue;'}\n\n\n",
    "supporting": [
      "W09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}