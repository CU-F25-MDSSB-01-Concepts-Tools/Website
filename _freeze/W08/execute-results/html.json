{
  "hash": "29d22a0ab67354d8ddbd42ea7ea0855c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#08: Predicting Categorical Variables, Logistic Regression, Classification Problems\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    toc: true\n    toc-depth: 1\n    smaller: true\n    slide-number: true\n    chalkboard: \n      buttons: true\n    preview-links: true\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n## Preliminaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'palmerpenguins'\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.1     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.3.0\n✔ modeldata    1.5.1     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(openintro)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:modeldata':\n\n    ames\n```\n\n\n:::\n:::\n\n\n# Predicting Categorical Data\n\nLarge part of the content adapted from <http://datasciencebox.org>.\n \n## What if response is binary?  {.smaller}\n\n- Example: **Spam filter** for emails\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(email) # Part of the openintro data package \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 07:16:41, 2012-01-01 08:03:59, 2012-01-01 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …\n```\n\n\n:::\n:::\n\n\n## Multinomial response variable?\n\n- We will not cover other categorical variables than binary ones here.\n- However, many of the probabilistic concepts transfer. \n\n\n\n## Variables {.smaller}\n\n`?email` shows all variable descriptions. For example: \n\n- `spam` Indicator for whether the email was spam.\n- `from` Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\n- `cc` Number of people cc'ed.\n- `time` Time at which email was sent.\n- `attach` The number of attached files.\n- `dollar` The number of times a dollar sign or the word “dollar” appeared in the email.\n- `num_char` The number of characters in the email, in thousands.\n- `re_subj` Whether the subject started with “Re:”, “RE:”, “re:”, or “rE:”\n\n:::{.aside}\nThe development, extraction, or discovery of such variables is called **feature engineering**, **feature extraction** or **feature discovery**. Usually, a combination of *domain knowledge* and *data science* skill is needed to do this. \n:::\n\n## Data exploration {.smaller}\n\nWould you expect spam to be longer or shorter?\n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n. . .\n\nWould you expect spam subject to start with \"Re:\" or the like?\n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |> ggplot(aes(y = re_subj, fill = spam)) + geom_bar()  \n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Linear models? {.smaller}\n\nBoth seem to give some signal. **How can we model the relationship?**\n\nWe focus first on just `num_char`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe would like to have a better concept!\n\n::: aside\nWhy do we need `y = as.numeric(spam)-1` instead of just `y = spam`?? Answer: `spam` is a factor although it looks like a numeric variable. The underlying integers for the two categories `\"0\"` and `\"1\"` are `1` and `2`. The strange form transforms the factor into a numerical variables with the values `0` and `1` as we need it for a target variable in a linear model.\n:::\n\n\n## A penguins example - predict sex{.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> na.omit() |> \n ggplot(aes(x = body_mass_g, y = as.numeric(sex)-1)) + \n geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n[It does not make much sense to predict 0-1-values with a linear model.]{style='color:red;'}\n\n\n\n## A probabilistic concept {.smaller background-color=\"aquamarine\"}\n\n- We treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n  - **Bernoulli trial:** a random experiment with exactly two possible outcomes, *success* and *failure*, in which the *probability of success* is the same every time the experiment is conducted\n\n. . .\n\n- Each email is treated as Bernoulli trial with separate probability of success\n\n$$ y_i ∼ \\text{Bernoulli}(p_i) $$\n\n. . .\n\n- We use the predictor variables to model the Bernoulli parameter $p_i$\n\n. . .\n\n- Now we conceptualized a continuous response, but still a linear model does not fit perfectly for $p_i$ (since a probability is between 0 and 1). \n- However, we can transform the linear model to have the appropriate range.\n\n\n\n# Generalized linear models\n\n## Characterising GLMs {.smaller background-color=\"aquamarine\"}\n\n- **Generalized linear models (GLMs)** are a way of addressing many problems in regression\n- Logistic regression is one example\n\nAll GLMs have the following three characteristics:\n\n1. A **probability distribution** as a generative model for the outcome variable $y_i \\sim \\text{Distribution}(\\text{parameter})$\n\n. . .\n\n2. A **linear model** $\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k$  \nwhere $\\eta$ is related to a mean parameter of the distribution by the ...\n\n. . .\n\n3. **Link function** that relates the linear model to the parameter of the outcome distribution. \n  \n\n\n# Logistic regression\n\n## Logistic regression {.smaller background-color=\"aquamarine\"}\n\n- **Logistic regression** is a GLM used to model a binary categorical outcome using numerical and categorical predictors.\n- The *distribution* is the Bernoulli distribution with parameter $p$ (success probability). \n- As *link function* connecting $\\eta_i$ to $p_i$ we use the **logit function**.\n\n. . .\n\n- **Logit function:** $\\text{logit}: [0,1] \\to \\mathbb{R}$\n\n$$\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\n\n- $\\frac{p}{1-p}$ is called the **odds** of a success which happens with probability $p$.  \nExample: Roll a six with a die has $p=1/6$. Thus, the odds are $\\frac{1/6}{5/6} = 1/5$. Sometimes written as 1:5. *\"The odds of success are one to five.\"*\n\n\n## Properties of the logit {.smaller background-color=\"aquamarine\"}\n\n- Logit takes values between 0 and 1 and returns values between $-\\infty$ and $\\infty$\n- The inverse of the logit function if the **logistic function** (mapping values from $-\\infty$ and $\\infty$ to values between 0 and 1):\n$$\\text{logit}^{-1}(x) = \\text{logistic}(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}$$\n- Logit can be interpreted as the log odds of a success -- more on this later.\n\n:::{.aside}\n- The logistic function is the solution of the differential equation $\\frac{d}{dx}f(x) = f(x)(1-f(x))$ which also appears in the SI-model of epidemics (and other models of exponential growth with saturation).\n- Good exercises to check your math skills:\n  1. Show that $\\text{logit}^{-1}(x) = \\text{logistic}(x)$ or the other way round. \n  2. Transform $\\frac{e^x}{1+e^x}$ into $\\frac{1}{1+e^{-x}}$.\n  3. Check that $\\text{logistic}(x)$ is a solution to $\\frac{d}{dx}f(x) = f(x)(1-f(x))$.\n- You can request hints from me when you get stuck.\n:::\n\n\n## Logit and [logistic]{style='color:red;'} function {.smaller background-color=\"aquamarine\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## The logistic regression model {background-color=\"aquamarine\"}\n\n- Based on the three GLM criteria we have\n  - $y_i \\sim \\text{Bernoulli}(p_i)$\n  - $\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}$\n  - $\\text{logit}(p_i) = \\eta_i$\n\n. . .\n\n- From which we get\n\n$$p_i = \\frac{e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}{1 + e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}$$\n\n\n## Modeling spam {.smaller}\n\nWith `tidymodels` we fit a GLM in the same way as a linear model except we\n\n- specify the model with `logistic_reg()`\n- use `\"glm\"` instead of `\"lm\"` as the engine \n- define `family = \"binomial\"` for the link function to be used in the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\ntidy(spam_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n```\n\n\n:::\n:::\n\n\n:::{.aside}\n- The family is *binomial* because the Bernoulli distribution is a special case of the binomial distribution $\\text{Binomial}(n,p)$ with $n=1$. \n- The binomial distribution specifies the probability to have $k$ successes in $n$ Bernoulli trials with the same success probability $p$. \n:::\n\n## Spam model  {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(spam_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n```\n\n\n:::\n:::\n\n\nModel:\n$$\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}$$\n\n\n\n## Predicted probability: Examples {.smaller}\n\nWe can compute the **predicted probability** that an email with 2000 character is spam as follows:\n\n$$\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot 2 = -1.9242$$\n\n(Note: `num_char` is in thousands.)\n\n$$\\frac{p}{1-p} = e^{-1.9242} = 0.15 \\Rightarrow p = 0.15 \\cdot (1 - p)$$\n\n$$p = 0.15 - 0.15\\cdot p \\Rightarrow 1.15\\cdot p = 0.15$$\n\n$$p = 0.15 / 1.15 = 0.13$$\n\n\n## Predicted probability {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic <- function(t) 1/(1+exp(-t))\npreds <- tibble(x=c(2,15,40), y = logistic(-1.80-0.0621*x))\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-1.80-0.0621*x),color=\"red\") +\n geom_point(data = preds, mapping = aes(x,y), color = \"blue\", size = 3)\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nSpam probability 2,000 characters: 0.1273939  \nSpam probability 15,000 characters: 0.06114  \nSpam probability 40,000 characters: 0.0135999\n\n## Interpretation of coefficients {.smaller}\n\n$$\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}$$\n\nWhat does an increase by thousand characters (`num_char + 1`) imply?\n\n. . .\n\nLet us assume the predicted probability of an email is $p_0$. Then an increase of `num_char` by one implied that the log-odds become \n\n$$\\log\\left(\\frac{p_0}{1-p_0}\\right) - 0.0621 = \\log\\left(\\frac{p_0}{1-p_0}\\right) - \\log(e^{0.0621})$$\n\n$$ = \\log\\left(\\frac{p_0}{1-p_0}\\right) + \\log(\\frac{1}{e^{0.0621}}) = \\log\\left(\\frac{p_0}{1-p_0} \\frac{1}{e^{0.0621}}\\right) = \\log\\left(\\frac{p_0}{1-p_0} 0.94\\right)$$ \n\nThat means the odds of being spam decrease by 6%. \n\n\n## Penguins example {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nsex_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(sex ~ body_mass_g, data = na.omit(penguins), family = \"binomial\")\ntidy(sex_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) -5.16     0.724        -7.13 1.03e-12\n2 body_mass_g  0.00124  0.000173      7.18 7.10e-13\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nna.omit(penguins) |> ggplot(aes(x = body_mass_g, y = sex)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-5.16+0.00124*x) + 1,color=\"red\") +\n xlim(c(2000,7000))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Multiple drawing groups in `geom_function()`\nℹ Did you use the correct group, colour, or fill aesthetics?\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n## Summarizing the Idea of Logistic Regression {.smaller}\n\n- We want to explain/predict a binary response variable. \n- We want to use the concept of linear models.\n- To that end we first interprete each outcome as the outcome of a probabilisitic event happening with probability $p_i$. That way we transform the problem from modeling a binary outcome to a numerical outcome.\n- However, probabilities are bounded between 0 and 1.\n- We use the logit function to map probabilities to the real line. That means we predict the log-odds of the probability.\n- We model the log-odds with a linear model.\n\n\n\n\n\n\n\n\n\n# Sensitivity and specificity\n\n## False positive and negative {.smaller}\n\n|                         | Email labelled spam           | Email labelled not spam       |\n|-------------------------|-------------------------------|-------------------------------|\n| **Email is spam**       | True positive                 | False negative (Type 2 error) |\n| **Email is not spam**   | False positive (Type 1 error) | True negative                 |\n\n\n## Confusion matrix\n\nMore general: [**Confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix)\nof statistical classification: \n\n![](img/confusion_small.png)\n\n## Sensitivity and specificity {.smaller}\n\n:::: {.columns}\n\n::: {.column width='40%'}\n![](img/confusion_small.png)\n:::\n\n::: {.column width='60%'}\n**Sensitivity** is the *true positive rate*: TP / (TP + FN)  \n**Specificity** is the *true negative rate*: TN / (TN + FP)\n:::\n\n::::\n\n\nFor spam a *positive* case is an email labelled as *spam*: \n\n**Sensitivity:** Fraction of emails *labelled as* spam among all emails which *are* spam.  \nLow sensitivity $\\to$ More false negatives $\\to$ More spam in you inbox!\n\n**Specificity:** Fraction of emails *labelled as not* spam among all emails which *are not* spam.  \nLow specificity $\\to$ More false positives $\\to$ More relevant emails in spam folder!\n\n. . .\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? \nWhat are the trade-offs associated with each decision? \n\n\n## COVID-19 tests {.smaller}\n\nA COVID-19 test is called *positive* when it indicates the presence of the virus.\n\n*What is the sensitivity of a test?*\n\n. . . \n\nProbability the test is positive when the tested person has COVID-19. (The true positive rate.)\n\n. . .\n\n*What is the specificity of a test?*\n\n. . . \n\nProbability the test is negative when the tested person does not have COVID-19. (The true negative rate.)\n\nOften the sensitivity is around 90% and the specificity is around 99%. What does that mean?  \n\n. . .\n\n- When you do not have COVID the test is more likely to be correct, than when you do have COVID. However, specificity and sensitivity do not tell something about the probability to have COVID when the test is positive, or to not have COVID when the test is negative.\n\n\n## Another view\n\n![](https://upload.wikimedia.org/wikipedia/commons/5/5a/Sensitivity_and_specificity_1.01.svg)\n\n# Prediction\n\n## Prediction vs. Explanation {.smaller}\n\n**Explanation** and **Prediction** are different **Modelling Mindsets**^[From Molnar, C. (2022). Modeling mindsets: The many cultures of learning from data. Christoph Molnar c/o Mucbook Clubhouse.].\n\n- With an *Explanatory Mindset* we want to understand the relationship between the predictors and the outcome.\n    - We interpret coefficients\n    - We ask what variance can be explained by a linear model or a PCA\n- With a *Predictive Mindset* we want to predict the outcome for new observations.\n    - We use the model to predict the outcome for new observations\n    - We ask how well we can predict the outcome for new observations\n    - We may ignore why the model works\n    \nImportant: The mindsets are very different but they may use the same models!\n\n\n\n\n\n## Prediction Example: Building a spam filter {.smaller}\n\n- Data: Set of emails and we know\n    - if each email is spam or not \n    -  several other features \n- Use logistic regression to predict the probability that an incoming email is spam\n- Use model selection to pick the model with the best predictive performance\n\n. . .\n\n- Building a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)\n\n. . .\n\n- A simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam\n\n## Emails: Use all predictors {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = email, family = \"binomial\") |>\n  tidy() |> print(n = 22)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 × 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  -9.09e+1   9.80e+3  -0.00928 9.93e- 1\n 2 to_multiple1 -2.68e+0   3.27e-1  -8.21    2.25e-16\n 3 from1        -2.19e+1   9.80e+3  -0.00224 9.98e- 1\n 4 cc            1.88e-2   2.20e-2   0.855   3.93e- 1\n 5 sent_email1  -2.07e+1   3.87e+2  -0.0536  9.57e- 1\n 6 time          8.48e-8   2.85e-8   2.98    2.92e- 3\n 7 image        -1.78e+0   5.95e-1  -3.00    2.73e- 3\n 8 attach        7.35e-1   1.44e-1   5.09    3.61e- 7\n 9 dollar       -6.85e-2   2.64e-2  -2.59    9.64e- 3\n10 winneryes     2.07e+0   3.65e-1   5.67    1.41e- 8\n11 inherit       3.15e-1   1.56e-1   2.02    4.32e- 2\n12 viagra        2.84e+0   2.22e+3   0.00128 9.99e- 1\n13 password     -8.54e-1   2.97e-1  -2.88    4.03e- 3\n14 num_char      5.06e-2   2.38e-2   2.13    3.35e- 2\n15 line_breaks  -5.49e-3   1.35e-3  -4.06    4.91e- 5\n16 format1      -6.14e-1   1.49e-1  -4.14    3.53e- 5\n17 re_subj1     -1.64e+0   3.86e-1  -4.25    2.16e- 5\n18 exclaim_subj  1.42e-1   2.43e-1   0.585   5.58e- 1\n19 urgent_subj1  3.88e+0   1.32e+0   2.95    3.18e- 3\n20 exclaim_mess  1.08e-2   1.81e-3   5.98    2.23e- 9\n21 numbersmall  -1.19e+0   1.54e-1  -7.74    9.62e-15\n22 numberbig    -2.95e-1   2.20e-1  -1.34    1.79e- 1\n```\n\n\n:::\n:::\n\n\n:::{.aside}\nWe treat the warning later.\n:::\n\n## The prediction task\n\n- The mechanics of prediction is **easy**:\n  - Plug in values of predictors to the model equation\n  - Calculate the predicted value of the response variable, $\\hat{y}$\n\n. . . \n\n- Getting it right is **harder**\n  - There is no guarantee the model estimates you have are correct\n  - Or that your model will perform as well with **new data** as it did with your sample data\n\n\n\n\n## Balance Over- and Underfitting\n\nIn a one predictor model we can show both visually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(y4 ~ x2, data = association)\n\nloess_fit <- loess(y4 ~ x2, data = association)\n\nloess_overfit <- loess(y4 ~ x2, span = 0.05, data = association)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: k-d tree limited by memory. ncmax= 200\n```\n\n\n:::\n\n```{.r .cell-code}\nassociation |>\n  select(x2, y4) |>\n  mutate(\n    Underfit = augment(lm_fit$fit) |> select(.fitted) |> pull(),\n    OK       = augment(loess_fit) |> select(.fitted) |> pull(),\n    Overfit  = augment(loess_overfit) |> select(.fitted) |> pull(),\n  ) |>\n  pivot_longer(\n    cols      = Underfit:Overfit,\n    names_to  = \"fit\",\n    values_to = \"y_hat\"\n  ) |>\n  mutate(fit = fct_relevel(fit, \"Underfit\", \"OK\", \"Overfit\")) |>\n  ggplot(aes(x = x2)) +\n  geom_point(aes(y = y4), color = \"darkgray\") +\n  geom_line(aes(y = y_hat, group = fit, color = fit), size = 1) +\n  labs(x = NULL, y = NULL, color = NULL) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.7)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n:::{.aside}\nThis is simulated data. \n:::\n\n## Spending our data\n\n**Problem:** \n\n- There are several steps to create a useful prediction model: parameter estimation, model selection, performance assessment, etc.\n- Doing all of this on the entire data we have available can lead to **overfitting**.\n\n\n**Solution:** We subsets our data for different tasks, as opposed to allocating all data to parameter estimation (as we have done so far). \n\n\n# Splitting Data\n\n## Splitting data {.smaller}\n\n- **Training set:**\n  - Sandbox for model building \n  - Spend most of your time using the training set to develop the model\n  - Majority of the data (usually 80%)\n  \n- **Testing set:**\n  - Held in reserve to determine efficacy of one or two chosen models\n  - Critical to look at it once, otherwise it becomes part of the modeling process\n  - Remainder of the data (usually 20%)\n  \n\n\n[Sidenote:]{style='color:blue;'} Splitting data is very important for the predictive questions, because we want to know how well our model performs on new data. It is not relevant for explanatory questions where we want to interpret coefficients and infer about the underlying population.\n\n\n\n## Performing the split {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 80% of the data into the training set \nemail_split <- initial_split(email, prop = 0.80)\n\n# Create data frames for the two sets:\ntrain_data <- training(email_split)\ntest_data  <- testing(email_split)\n```\n:::\n\n\n**A note on the seed:** The seed is an arbitrary number that is used to initialize a pseudorandom number generator. The same seed will always result in the same sequence of pseudorandom numbers. This is useful for reproducibility.\n\n## Peek at the split {.smaller}\n\n:::: {.columns}\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(train_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,136\nColumns: 21\n$ spam         <fct> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0,…\n$ sent_email   <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ time         <dttm> 2012-01-25 23:46:55, 2012-01-03 06:28:28, 2012-02-04 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 10, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, yes, no, no, no, no, no, no, …\n$ inherit      <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, …\n$ num_char     <dbl> 23.308, 1.162, 4.732, 42.238, 1.228, 25.599, 16.764, 10.7…\n$ line_breaks  <int> 477, 2, 127, 712, 30, 674, 367, 226, 98, 671, 46, 192, 67…\n$ format       <fct> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 12, 0, 2, 2, 2, 31, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 11, 1, …\n$ number       <fct> small, none, big, big, small, small, small, small, small,…\n```\n\n\n:::\n:::\n\n:::\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 785\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, …\n$ sent_email   <fct> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ time         <dttm> 2012-01-01 18:55:06, 2012-01-01 20:38:32, 2012-01-02 06:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ dollar       <dbl> 0, 0, 5, 0, 0, 0, 0, 5, 4, 0, 0, 0, 21, 0, 0, 2, 9, 0, 0,…\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, …\n$ num_char     <dbl> 4.837, 15.075, 18.037, 45.842, 11.438, 1.482, 14.431, 0.9…\n$ line_breaks  <int> 193, 354, 345, 881, 125, 24, 296, 13, 192, 14, 32, 30, 55…\n$ format       <fct> 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ exclaim_subj <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 1, 10, 20, 5, 2, 0, 0, 0, 6, 0, 0, 1, 3, 0, 4, 0, 1, 0, 1…\n$ number       <fct> big, small, small, big, small, none, small, small, small,…\n```\n\n\n:::\n:::\n\n:::\n::::\n\n\n# Workflow of Predictive Modeling\n\n## Fit a model to the training dataset \n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = train_data, family = \"binomial\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n:::\n\n\nWe get a warning and should explore the reasons for 0 or 1 probability. \n\n::: aside\n- A deeper looking into the predicted probabilities (not shown here) shows that 4 cases are predicted to be spam with 100% probability, as well as 864 cases are predicted to be not spam with 100% probability. \n- Note: The `dplyr` function `near` was used to assess if predicted probabilities were one. \n- This is usually undesirable. Hence the warning. \n:::\n\n## Look at categorical predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor_predictors <- train_data |>\n  select(where(is.factor), -spam) |>\n  names()\n\np_to_multiple <- ggplot(train_data, aes(x = to_multiple, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_from <- ggplot(train_data, aes(x = from, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_sent_email <- ggplot(train_data, aes(x = sent_email, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_winner <- ggplot(train_data, aes(x = winner, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_format <- ggplot(train_data, aes(x = format, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_re_subj <- ggplot(train_data, aes(x = re_subj, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_urgent_subj <- ggplot(train_data, aes(x = urgent_subj, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\np_number <- ggplot(train_data, aes(x = number, fill = spam)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"#E48957\", \"#CA235F\"))\n\nlibrary(patchwork)\np_to_multiple + p_from + p_sent_email + p_winner + p_format + p_re_subj + p_urgent_subj + p_number +\n  plot_layout(ncol = 4, guides = \"collect\") & \n  theme(axis.title.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nCloser look at `from` and `sent_email`. \n\n## Counting cases {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n`from`: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email). \n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data |> count(spam, from)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  spam  from      n\n  <fct> <fct> <int>\n1 0     1      2837\n2 1     0         3\n3 1     1       296\n```\n\n\n:::\n:::\n\n*No non-spam mails without `from`.*\n:::\n\n::: {.column width='50%'}\n`sent_mail`: Indicator for whether the sender had been sent an email from the receiver in the last 30 days.\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data |> count(spam, sent_email)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  spam  sent_email     n\n  <fct> <fct>      <int>\n1 0     0           1972\n2 0     1            865\n3 1     0            299\n```\n\n\n:::\n:::\n\n*No spam mails with `sent_email`.*\n:::\n\n::::\n\n- There is *incomplete separation* in the data for those variables. \n- That mean we have a *sure* prediction probabilities (0 or 1). (That is the warning. Also, these variables have the highest coefficients.)\n- This is not what we assume about reality. Maybe our sample is too small to see it. \n- Therefore we exclude these variables.\n\n## Look at numerical variables {.smaller}\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\ntrain_data |>\n group_by(spam) |>\n select(where(is.numeric)) |> \n pivot_longer(-spam) |> \n group_by(name, spam) |> \n summarize(mean = mean(value), sd = sd(value)) |> \n print(n = 22)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAdding missing grouping variables: `spam`\n`summarise()` has grouped output by 'name'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 × 4\n# Groups:   name [11]\n   name         spam       mean       sd\n   <chr>        <fct>     <dbl>    <dbl>\n 1 attach       0       0.124     0.775 \n 2 attach       1       0.227     0.620 \n 3 cc           0       0.393     2.62  \n 4 cc           1       0.388     3.25  \n 5 dollar       0       1.56      5.33  \n 6 dollar       1       0.779     3.01  \n 7 exclaim_mess 0       6.68     50.2   \n 8 exclaim_mess 1       8.75     88.4   \n 9 exclaim_subj 0       0.0783    0.269 \n10 exclaim_subj 1       0.0769    0.267 \n11 image        0       0.0536    0.503 \n12 image        1       0.00334   0.0578\n13 inherit      0       0.0352    0.216 \n14 inherit      1       0.0702    0.554 \n15 line_breaks  0     247.      326.    \n16 line_breaks  1     108.      321.    \n17 num_char     0      11.4      14.9   \n18 num_char     1       5.63     15.7   \n19 password     0       0.112     0.938 \n20 password     1       0.0201    0.182 \n21 viagra       0       0         0     \n22 viagra       1       0.0268    0.463 \n```\n\n\n:::\n:::\n\n\n`viagra` has no mentions in non-spam emails.\n\n- We should exclude this variable for the same reason. \n\n\n## Fit a model to the training dataset {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = \"binomial\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\nemail_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:  stats::glm(formula = spam ~ . - from - sent_email - viagra, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  to_multiple1            cc          time         image  \n  -9.867e+01    -2.505e+00     1.944e-02     7.396e-08    -2.854e+00  \n      attach        dollar     winneryes       inherit      password  \n   5.070e-01    -6.440e-02     2.170e+00     4.499e-01    -7.065e-01  \n    num_char   line_breaks       format1      re_subj1  exclaim_subj  \n   5.870e-02    -5.420e-03    -9.017e-01    -2.995e+00     1.002e-01  \nurgent_subj1  exclaim_mess   numbersmall     numberbig  \n   3.572e+00     1.009e-02    -8.518e-01    -1.329e-01  \n\nDegrees of Freedom: 3135 Total (i.e. Null);  3117 Residual\nNull Deviance:\t    1974 \nResidual Deviance: 1447 \tAIC: 1485\n```\n\n\n:::\n:::\n\n\nWe still get a warning, but without very high coefficients. \n\n::: aside\nA deeper analysis shows that now only two cases are predicted not spam with 100% probability. \n:::\n\n\n\n## Predict with the testing dataset {.smaller}\n\nPredicting the raw values (log-odds)\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(email_fit, test_data, type = \"raw\") |> head() # head prints the first values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n```\n\n\n:::\n:::\n\n\n:::: {.columns}\n\n::: {.column width='50%'}\nPredicting probabilities\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(email_fit, test_data, type = \"prob\") |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\nPredicting spam (default)\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(email_fit, test_data) # Would be type = \"class\"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ℹ 775 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n\n## Relate back to the model concept {.smaller}\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nemail_pred <- \n predict(email_fit, test_data, type = \"prob\") |>\n select(spam_prob = .pred_1) |> \n mutate(spam_logodds = \n         predict(email_fit, test_data, type = \"raw\"), \n        spam_odds = exp(spam_logodds)) |> \n bind_cols(predict(email_fit, test_data)) |> \n # Append real data\n bind_cols(test_data |> select(spam)) \nemail_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1   0.00709        -4.94   0.00714 0           0    \n 2   0.00181        -6.31   0.00181 0           0    \n 3   0.0191         -3.94   0.0195  0           0    \n 4   0.00124        -6.69   0.00124 0           0    \n 5   0.0121         -4.40   0.0123  0           0    \n 6   0.170          -1.59   0.204   0           0    \n 7   0.0410         -3.15   0.0427  0           0    \n 8   0.139          -1.83   0.161   0           0    \n 9   0.0617         -2.72   0.0657  0           0    \n10   0.0983         -2.22   0.109   0           0    \n# ℹ 775 more rows\n```\n\n\n:::\n:::\n\n\n- The raw predictions are the log-odds.\n- From which we can compute the odds. \n- From which the probability is computed. Here it is done by `predict`. \n- The `.pred_class` prediction is when the probability > 0.5. \n  - What does it mean for the odds and the log-odds?\n  \n. . . \n\nAnswers: odds > 1, log-odds > 0\n\n## Another look {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred |> arrange(desc(spam_prob)) |> print(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1     0.903       2.23       9.29  1           1    \n 2     0.833       1.60       4.98  1           0    \n 3     0.825       1.55       4.71  1           1    \n 4     0.733       1.01       2.75  1           1    \n 5     0.683       0.766      2.15  1           1    \n 6     0.626       0.517      1.68  1           1    \n 7     0.614       0.464      1.59  1           0    \n 8     0.597       0.392      1.48  1           1    \n 9     0.538       0.153      1.17  1           1    \n10     0.537       0.148      1.16  1           0    \n11     0.510       0.0404     1.04  1           0    \n12     0.491      -0.0345     0.966 0           0    \n13     0.490      -0.0407     0.960 0           0    \n14     0.489      -0.0453     0.956 0           1    \n15     0.483      -0.0698     0.933 0           1    \n16     0.473      -0.107      0.899 0           0    \n17     0.463      -0.150      0.861 0           0    \n18     0.457      -0.174      0.840 0           0    \n19     0.447      -0.212      0.809 0           0    \n20     0.447      -0.214      0.808 0           1    \n# ℹ 765 more rows\n```\n\n\n:::\n:::\n\n\nWe see false positives and false negatives.\n\n## Evaluate the performance {.smaller}\n\n**Receiver operating characteristic (ROC) curve**^[Originally developed for operators of military radar receivers, hence the odd name.] which plots true positive rate (sensitivity) vs. false positive rate (1 - specificity)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred |> roc_curve(\n    truth = spam, spam_prob,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n## Evaluate the performance  {.smaller}\n\nFind the area under the curve (AUC). \n\nIn calculus language:    \n$\\int_0^1 \\text{TPR}(\\text{FPR}) d\\text{FPR}$    \n\nRemember: \n\n- TPR = True Positive Rate = sensitivity \n- FPR = False Positive Rate = 1 - specificity\n- specificity is the true negative rate. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    spam_prob,\n    event_level = \"second\" \n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857\n```\n\n\n:::\n:::\n\n\n# Feature engineering\n\n## Feature engineering {.smaller}\n\n- We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\n- Variables that go into the model and how they are represented are critical to the success of the model\n- **Feature engineering** is getting creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance) \n\n## Modeling workflow, revisited {.smaller}\n\n- Create a **recipe** for feature engineering steps to be applied to the training data\n  - The `tidymodels` way (similar to a pipeline in python's scikit-learn). \n\n- Fit the model to the training data after these steps have been applied\n\n- Using the model estimates from the training data, predict outcomes for the test data\n\n- Evaluate the performance of the model on the test data\n\n# Recipes\n\n## Initiate a recipe {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsummary(email_rec) |> print(n = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 4\n   variable     type      role      source  \n   <chr>        <list>    <chr>     <chr>   \n 1 to_multiple  <chr [3]> predictor original\n 2 from         <chr [3]> predictor original\n 3 cc           <chr [2]> predictor original\n 4 sent_email   <chr [3]> predictor original\n 5 time         <chr [1]> predictor original\n 6 image        <chr [2]> predictor original\n 7 attach       <chr [2]> predictor original\n 8 dollar       <chr [2]> predictor original\n 9 winner       <chr [3]> predictor original\n10 inherit      <chr [2]> predictor original\n11 viagra       <chr [2]> predictor original\n12 password     <chr [2]> predictor original\n13 num_char     <chr [2]> predictor original\n14 line_breaks  <chr [2]> predictor original\n15 format       <chr [3]> predictor original\n16 re_subj      <chr [3]> predictor original\n17 exclaim_subj <chr [2]> predictor original\n18 urgent_subj  <chr [3]> predictor original\n19 exclaim_mess <chr [2]> predictor original\n20 number       <chr [3]> predictor original\n21 spam         <chr [3]> outcome   original\n```\n\n\n:::\n:::\n\n\nThe object `email_rec` only includes *meta-data* (columns names and types)! \n\n\n\n## Remove certain variables {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec |>\n  step_rm(from, sent_email, viagra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 20\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: from, sent_email, viagra\n```\n\n\n:::\n:::\n\n\n\n## Feature engineer date {.smaller}\n\n- The date-time may not be such an interesting predictor. \n  - It could only bring in a general trend over time\n- Often decomposing the date to the month or the day of the week (dow) is more interesting.\n  - `step_date` can easily extract these\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 20\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: from, sent_email, viagra\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Date features from: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: time\n```\n\n\n:::\n:::\n\n\n## Create dummy variables {.smaller}\n\n- Use helper functions like `all_nominal` or `all_outcomes` from `tidymodels` for column selection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 20\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: from, sent_email, viagra\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Date features from: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal() -all_outcomes()\n```\n\n\n:::\n:::\n\n\n## Remove zero variance variables {.smaller}\n\nVariables that contain only a single value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 20\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: from, sent_email, viagra\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Date features from: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal() -all_outcomes()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Zero variance filter on: all_predictors()\n```\n\n\n:::\n:::\n\n\n## Full recipe {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n) |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\nprint(email_rec)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 20\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: from, sent_email, viagra\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Date features from: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Variables removed: time\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal() -all_outcomes()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Zero variance filter on: all_predictors()\n```\n\n\n:::\n:::\n\n\nThe object `email_rec` only includes *meta-data* of the data frame it shall work on (a formula, columns names and types)!\n\n# Building workflows\n\n\n## Define model {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_mod <- logistic_reg() |> \n  set_engine(\"glm\")\n\nemail_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n\n\n:::\n:::\n\n\n## Define workflow {.smaller}\n\n**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_wflow <- workflow() |> \n  add_model(email_mod) |> \n  add_recipe(email_rec)\nemail_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n\n\n:::\n:::\n\n\n\n## Fit model to training data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit <- email_wflow |> \n  fit(data = train_data)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(email_fit) |> print(n = 27)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 × 5\n   term            estimate std.error statistic  p.value\n   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)     -0.651     0.254     -2.57   1.03e- 2\n 2 cc               0.0214    0.0229     0.936  3.49e- 1\n 3 image           -2.99      1.31      -2.28   2.28e- 2\n 4 attach           0.512     0.116      4.41   1.03e- 5\n 5 dollar          -0.0651    0.0307    -2.12   3.40e- 2\n 6 inherit          0.440     0.205      2.15   3.14e- 2\n 7 password        -0.723     0.302     -2.39   1.67e- 2\n 8 num_char         0.0585    0.0240     2.43   1.50e- 2\n 9 line_breaks     -0.00548   0.00139   -3.94   8.24e- 5\n10 exclaim_subj     0.0998    0.268      0.373  7.09e- 1\n11 exclaim_mess     0.0103    0.00198    5.20   2.02e- 7\n12 to_multiple_X1  -2.56      0.339     -7.56   4.11e-14\n13 winner_yes       2.24      0.430      5.21   1.90e- 7\n14 format_X1       -0.953     0.157     -6.06   1.38e- 9\n15 re_subj_X1      -3.00      0.444     -6.76   1.39e-11\n16 urgent_subj_X1   3.69      1.15       3.20   1.37e- 3\n17 number_small    -0.840     0.162     -5.20   1.98e- 7\n18 number_big      -0.0915    0.244     -0.375  7.07e- 1\n19 time_dow_Mon    -0.326     0.303     -1.08   2.82e- 1\n20 time_dow_Tue     0.0813    0.275      0.296  7.67e- 1\n21 time_dow_Wed    -0.260     0.275     -0.946  3.44e- 1\n22 time_dow_Thu    -0.220     0.279     -0.788  4.31e- 1\n23 time_dow_Fri    -0.0612    0.275     -0.223  8.24e- 1\n24 time_dow_Sat     0.0646    0.292      0.221  8.25e- 1\n25 time_month_Feb   0.760     0.178      4.26   2.03e- 5\n26 time_month_Mar   0.506     0.178      2.85   4.40e- 3\n27 time_month_Apr -12.0     394.        -0.0306 9.76e- 1\n```\n\n\n:::\n:::\n\n\n\n\n## Make predictions for test data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred <- predict(email_fit, test_data, type = \"prob\") |> \n  bind_cols(test_data) \n\nemail_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 785 × 23\n   .pred_0  .pred_1 spam  to_multiple from     cc sent_email time               \n     <dbl>    <dbl> <fct> <fct>       <fct> <int> <fct>      <dttm>             \n 1   0.993 0.00653  0     1           1         0 1          2012-01-01 18:55:06\n 2   0.998 0.00169  0     0           1         1 1          2012-01-01 20:38:32\n 3   0.987 0.0127   0     0           1         0 0          2012-01-02 06:42:16\n 4   0.999 0.000825 0     0           1         1 0          2012-01-02 16:12:51\n 5   0.991 0.00876  0     0           1         4 0          2012-01-02 17:45:36\n 6   0.878 0.122    0     0           1         0 0          2012-01-02 22:55:03\n 7   0.959 0.0414   0     0           1         0 0          2012-01-03 02:07:17\n 8   0.852 0.148    0     0           1         0 0          2012-01-03 06:41:35\n 9   0.938 0.0619   0     0           1         0 0          2012-01-03 17:02:35\n10   0.896 0.104    0     0           1         0 0          2012-01-03 12:14:51\n# ℹ 775 more rows\n# ℹ 15 more variables: image <dbl>, attach <dbl>, dollar <dbl>, winner <fct>,\n#   inherit <dbl>, viagra <dbl>, password <dbl>, num_char <dbl>,\n#   line_breaks <int>, format <fct>, re_subj <fct>, exclaim_subj <dbl>,\n#   urgent_subj <fct>, exclaim_mess <dbl>, number <fct>\n```\n\n\n:::\n:::\n\n\n\n## Evaluate the performance {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred |>\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\n\n## Evaluate the performance  {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n```\n\n\n:::\n:::\n\n\nThis is better than our former model (without the feature engineering workflow), which had AUC = 0.857. \n\n# From probabilities to decisions {.smaller}\n\n## Cutoff probability: 0.5 {.smaller}\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.5**. (That is the default.)\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.5\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|spam              | Email labelled spam| Email labelled not spam|\n|:-----------------|-------------------:|-----------------------:|\n|Email is spam     |                  14|                      54|\n|Email is not spam |                  10|                     707|\n\n\n:::\n:::\n\n\n**Sensitivity:** 14/(14+54) = 0.206   \n**Specificity:** 707/(707+10) = 0.986   \n\n## Cutoff probability: 0.25  {.smaller}\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.25**.\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.25\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|spam              | Email labelled spam| Email labelled not spam|\n|:-----------------|-------------------:|-----------------------:|\n|Email is spam     |                  32|                      36|\n|Email is not spam |                  61|                     656|\n\n\n:::\n:::\n\n\n**Sensitivity:** 32/(32+36) = 0.471   \n**Specificity:** 656/(656 + 61) = 0.915   \n\n\n## Cutoff probability: 0.75 {.smaller}\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.75**.\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.75\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|spam              | Email labelled spam| Email labelled not spam|\n|:-----------------|-------------------:|-----------------------:|\n|Email is spam     |                   3|                      65|\n|Email is not spam |                   1|                     716|\n\n\n:::\n:::\n\n\n**Sensitivity:** 3/(3+65) = 0.044   \n**Specificity:** 716/(716+1) = 0.999\n\n\n## Check our very first model {.smaller}\n\nWe make a new simple recipe and draw workflow and fitting re-using the same specified logisitc regression model `email_mod`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_email_rec <- recipe(\n spam ~ num_char,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsimple_email_pred <- \n workflow() |> \n add_model(email_mod) |> \n add_recipe(simple_email_rec) |> \n fit(data = train_data) |> \n predict(test_data, type = \"prob\") |> \n bind_cols(test_data |> select(spam,num_char,time)) \nsimple_email_pred \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 785 × 5\n   .pred_0 .pred_1 spam  num_char time               \n     <dbl>   <dbl> <fct>    <dbl> <dttm>             \n 1   0.889  0.111  0        4.84  2012-01-01 18:55:06\n 2   0.936  0.0644 0       15.1   2012-01-01 20:38:32\n 3   0.945  0.0547 0       18.0   2012-01-02 06:42:16\n 4   0.989  0.0113 0       45.8   2012-01-02 16:12:51\n 5   0.922  0.0784 0       11.4   2012-01-02 17:45:36\n 6   0.868  0.132  0        1.48  2012-01-02 22:55:03\n 7   0.933  0.0667 0       14.4   2012-01-03 02:07:17\n 8   0.864  0.136  0        0.978 2012-01-03 06:41:35\n 9   0.905  0.0953 0        7.79  2012-01-03 17:02:35\n10   0.864  0.136  0        0.978 2012-01-03 12:14:51\n# ℹ 775 more rows\n```\n\n\n:::\n:::\n\n\n## Evaluate the performance {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_email_pred |> roc_curve(\n    truth = spam, \n    .pred_1,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_email_pred |>\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\" \n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.753\n```\n\n\n:::\n:::\n\n\n**Conclusion:** It is not as good compared to AUC 0.86\n\n\n## Predict Penguins' Sex {.scrollable .smaller}\n\nFollowing all the steps introduced before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n# Put 80% of the data into the training set \npeng_split <- initial_split(penguins, prop = 0.80)\n# Create data frames for the two sets:\ntrain_data_peng <- training(peng_split)\ntest_data_peng  <- testing(peng_split)\n\n# Create a recipe for the model\npeng_rec <- recipe(\n sex ~ .,\n data = train_data_peng \n) |> \n step_naomit(all_predictors()) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\n\n# Create a logistic regression model \npeng_mod <- logistic_reg() |> \n set_engine(\"glm\")\n\n# Create a workflow\npeng_wflow <- workflow() |> \n add_model(peng_mod) |> \n add_recipe(peng_rec)\n\n# Fitting model to training data\npeng_fit <- peng_wflow |> \n fit(data = train_data_peng)\n\n# Make predictions for test data\npeng_pred <- peng_fit |> \n predict(test_data_peng, type = \"prob\") |> \n bind_cols(test_data_peng)\n```\n:::\n\n\n## Penguins: Evaluate Performance {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_pred |> roc_curve(\n    truth = sex,  .pred_male, event_level = \"second\"\n  ) |>  autoplot()\n```\n\n::: {.cell-output-display}\n![](W08_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_pred |> roc_auc(\n truth = sex, .pred_male, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.958\n```\n\n\n:::\n:::\n\n\n\n\n## Penguins: Cutoff probability 0.5 {.smaller}\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.5\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sex               | Penguin labelled male| Penguin labelled female|\n|:-----------------|---------------------:|-----------------------:|\n|Penguin is male   |                    31|                       4|\n|Penguin is female |                     4|                      28|\n\n\n:::\n:::\n\n\n**Sensitivity:** 31/(31+4) = 0.886   \n**Specificity:** 28/(28+4) = 0.875\n\n## Penguins: Cutoff probability 0.25 {.smaller}\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.25\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sex               | Penguin labelled male| Penguin labelled female|\n|:-----------------|---------------------:|-----------------------:|\n|Penguin is male   |                    34|                       1|\n|Penguin is female |                     7|                      25|\n\n\n:::\n:::\n\n\n**Sensitivity:** 34/(34+1) = 0.971   \n**Specificity:** 25/(25+7) = 0.781\n\n\n## Penguins: Cutoff probability 0.75 {.smaller}\n\n**Confusion matrix:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_prob <- 0.75\npeng_pred |>\n  mutate(\n    sex      = if_else(sex == \"male\", \"Penguin is male\", \"Penguin is female\"),\n    sex_pred = if_else(.pred_male > cutoff_prob, \"Penguin labelled male\", \"Penguin labelled female\")\n    ) |>\n  count(sex_pred, sex) |>\n  pivot_wider(names_from = sex_pred, values_from = n) |> \n  select(1,3,2) |> slice(c(2,1)) |> # reorder rows and cols to fit convention\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sex               | Penguin labelled male| Penguin labelled female|\n|:-----------------|---------------------:|-----------------------:|\n|Penguin is male   |                    29|                       6|\n|Penguin is female |                     2|                      30|\n\n\n:::\n:::\n\n\n**Sensitivity:** 29/(29+6) = 0.829    \n**Specificity:** 30/(30+2) = 0.938",
    "supporting": [
      "W08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}