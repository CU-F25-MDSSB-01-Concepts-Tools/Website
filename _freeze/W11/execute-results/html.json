{
  "hash": "9276c74ef59d7d5a0b7d412475df4715",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#11: Final Project, Collaborative Git, Bootstrapping, Cross validation, Bias-Variance Tradeoff\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    smaller: true\n    toc: true\n    toc-depth: 1\n    slide-number: true\n    chalkboard: \n      buttons: true\n    preview-links: true\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n# Final Project {background-color=khaki}\n\n\n## A project report in a nutshell  {background-color=khaki}\n\n- You pick a dataset, \n- do some interesting question-driven data analysis with it, \n- write up a well structured and nicely formatted report about the analysis, and \n- present it at the end of the semester. \n\n\n## Six types of questions {.smaller background-color=khaki}\n\n:::: {.columns}\n\n::: {.column width='60%'}\n1.  **Descriptive:** summarize a characteristic of  data\n2.  **Exploratory:** analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\n3.  **Inferential:** analyze patterns, trends, or relationships in representative data from a population\n4.  **Predictive:** make predictions for individuals or groups of individuals\n5.  **Causal:** whether changing one factor will change another factor, on average, in a population\n6.  **Mechanistic:** explore \"how\" as opposed to whether\n:::\n\n::: {.column width='40%'}\n![](img/DataAnalysisFlowChart_LeekPeng.jpeg)\n:::\n\n::::\n\n:::{.aside}\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. <https://doi.org/10.1126/science.aaa6146>.\n:::\n\n## What are good questions? {.smaller background-color=khaki}\n\nA good question\n\n- is not too broad (= complicated to answer)\n- is not too specific (= trivial to answer)\n- can be answered or at least approached with data\n- call for one of the different types of data analysis: \n    - descriptive\n    - exploratory\n    - inferential\n    - predictive\n    - (causal or mechanistic -- not the typical choices for a short term data analysis projects. They should not be central but may be touched.)\n\n\n## Tools and advice for questions {.smaller background-color=khaki}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n[**Descriptive**]{style='color:blue;'}\n\n::: {.fragment}\n- What is an observation? [Answer this in any data analysis!]{style='color:red;'}\n- What are the variables?\n- Summary statistics and basic visualizations\n- Category frequencies and numerical distributions\n:::\n:::\n\n::: {.column width='50%'}\n[**Exploratory**]{style='color:blue;'}\n\n::: {.fragment}\n- Questions specific to the topic\n- Compute/summarize relevant variables: percentages, ratios, differences, ...)\n- Scatter plots\n- Correlations\n- Cluster Analysis\n- **Crafted computations and visualizations for your particular questions**\n:::\n:::\n\n::::\n\n\n## Tools and advice for questions {.smaller background-color=khaki}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n[**Inferential**]{style='color:blue;'}\n\n::: {.fragment}\n- Describe the population the data represents\n- Can it be considered a random sample?\n- Think theoretically about variable selection and data transformations\n- Interpret coefficients of models\n- Be careful interpreting coefficients when variables are highly correlated\n- Are effects statistical significant?\n- Hypothesis test: Describe null distribution\n:::\n:::\n\n::: {.column width='50%'}\n[**Predictive**]{style='color:blue;'}\n\n:::{.fragment}\n- Regression or classification?\n- Do a train/test split to evaluate model performance (Optional, do crossvalidation) \n- Select sensible performance metrics for the problem (from confusion matrix or regression metrics)\n- Maybe compare different models and tune hyperparameters\n- Do feature selection and feature engineering to improve the performance!\n:::\n:::\n::::\n\n\n# Modeling Mindsets\n\n![](img/ModelingMindsets_annotated.png){height=300}\n\nSome methods:  \n**Linear Model:** [Inferential]{style='color:red;' .fragment} [and Predictive!]{style='color:red;' .fragment}  \n**Logistic Regression:** [Inferential]{style='color:red;' .fragment} [and Predictive!]{style='color:red;' .fragment}  \n**k-means Clustering:** [Exploratory]{style='color:red;' .fragment}\n\n[**Let's check your questions in the Course Organization Repo!**]{style='color:blue;' .fragment}\n\n# Collaborative Work with Git\n\nLearning goal: First experiences with collaborative data science work with `git`\n\n\n## Step 1: git clone *project-Y-USERNAMES* {.smaller}\n\n\n[Team formation is mostly complete.]{style='color:blue;'} \nRepositories `project-1`, `FinalProject_2`, ... are created. **You are to deliver your project reports in your repository.**\n\n- Find your project repository \n- Copy the URL   \n  ![](img/clone-repo-link.png){height=150}\n- Go to RStudio\n  - New Project > Form Version Control > Git > Paste your URL\n  - The project is created\n\n\n## Step 2: First Team member commits and pushes {.smaller}\n\n**One team member does the following:**\n\n- Open the file `report.qmd`\n- Write your name in the first line at `author` in the YAML\n- `git add` the file `report.qmd`\n- `git commit` (enter \"Added name\" as commit message)\n- `git push`\n  \n  \n## Step 3: Other team members pull {.smaller}\n\n- Do `git pull` in the RStudio interface. \n\n\n:::: {.columns}\n\n::: {.column width='60%'}\n**What does `git pull` do?**\n\n- `git pull` does two things: fetching new stuff and merging it with existing stuff\n    1. First it `git fetch`s the commit from the remote repository (GitHub) to the local machine\n    2. Then it `git merge`s the commit with the latest commit on your local machine. \n- When we are lucky this works with no problems. (Should be the case with new files.)\n:::\n\n::: {.column width='40%'}\n![](https://files.mastodon.social/media_attachments/files/109/303/119/338/776/592/original/662a5e803a5e6c21.png)\n\nSource: <https://mastodon.social/@allison_horst/109303149552034159>\n:::\n\n::::\n\n\n\n\n## Step 4: Merge independent changes {.smaller}\n\n[Git can merge changes in the same file when there are no conflicts.]{style='color:blue;'} Let's try. \n\n- **The second team member:**\n  - Add your name in the author section of the YAML, save the file, add the file in the Git pane and make a commit. \n  - Save, add the file in the Git pane, commit with message \"Next author name\", **push.**\n- **The third (or first) team member:**\n  - Change the title in the YAML to something meaningful (and also add your name if third team member), save the file, add the file in the Git pane and make a commit.\n  - Try to **push**. You should receive an error. Read it carefully, often it tells you what to do. Here: Do `git pull` first. You cannot push because remotely there is a newer commit (the one your colleague just made).\n  - **Pull.** This should result in message about a **successfull auto-merge**. Check that **both** are there: Your line and the line of your colleague.\n  *If you receive several hints instead, first read the next slide!*\n\n\n\n## ??? `git` configuration for divergent branches {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\nIf you pull for the first time in a local git repository, git may complain like this: \n\n![](img/gitconfigrebase_rstudio.png){height=200}\n\nRead that carefully. It advises to configure with `git config pull.rebase false` as the default version. \n:::\n\n::: {.column width='50%'}\n\n**How to do the configuration?**\n\n- Copy the line `git config pull.rebase false` and close the window.\n- Go to the Terminal pane (not the console, the one besides that). This is a terminal not for R but to speak with the computer in general. Paste the command and press enter. Now you are done and your next `git pull` should work.\n:::\n\n::::\n\n:::{.aside}\nWhat is a *branch* and what a *rebase*? These are features of git well worth to learn but not now. Learn at <http://happygitwithr.com>\n:::\n\n\n## Step 5: Push and pull the other way round {.smaller}\n\n- **The third/first member:**\n  - The successful merge creates a new commit, which you can directly push. \n  - **Push.**\n- **The second team member (and all others):** \n  - **Pull** the changes of your colleague. \n  \nPractice a bit more pulling and pushing commits and check the merging. \n\n\n## Step 6: Create a merge conflict {.smaller}\n\n- First and second team members: \n  - Write a different sentence after \"Executive Summary.\" in YAML `abstract:`.\n  - Each `git add`  and `git commit` on local machines. \n- First member: `git push`\n- Second member: \n  - `git pull`. That should result in a conflict. *If you receive several hints instead, first read the slide two slides before!*\n  - The conflict should show directly in the file with markings like this\n  \n`>>>>>>>>`   \none option of text,   \n`========` a separator,    \nthe other option, and     \n`<<<<<<<`. \n\n## Step 7: Solve the conflict {.smaller}\n\n- The second member\n  - You have to solve this conflict now!\n  - Solving is by editing the text\n  - Decide for an option or make a new text\n  - Thereby, remove the `>>>>>`,`=====`,`<<<<<<`\n  - When you are done: `git add`, `git commit`, and `git push`. \n\nNow you know how to solve merge conflicts. Practice a bit in your team. \n\n**Working in VSCode:** The workflow is the same because it relies on git not on the editor of choice. \n    \n## Advice: Collaborative work with git {.smaller}\n\n- Whenever you start a work session: First **pull** to see if there is anything new. That way you reduce the need for merges. \n- Inform your colleagues when you pushed new commits. \n- Coordinate the work, e.g. discuss who works on what part and maybe when. However, git allows to also work without full coordination and in parallel. \n- When you finish your work session, end with pushing a *nice* commit. That means. The file should render. You made comments when there are loose ends and todo's. \n- You can also use the *issues* section of the GitHub repository for things to do. \n- When you work on different parts of the file, be aware that also a successful merge can create problems. Example: Your colleague changed the data import, while you worked on graphics. Maybe after the merge the imported data is not what you need for your chunk. Then coordinate.  \n- Commit and push often. This avoids that potential merge conflicts become large. \n\n\n\n\n\n# Bootstrapping\n\n## Purpose: Quantify uncertainty {.smaller}\n\n- **Uncertainty** is a central concern in **inferential data analysis**. \n- We want to **estimate** a **parameter** of a **population** from a **sample**.\n\n. . .\n\nCentral inferential assumption: \n\n- Our data is a **random sample** from the **population** we are interested in. \n    - No **selection biases**.\n\nExamples:\n\n- We want to know **average height** of **all people** but we only have a sample of people. \n- We want know the **mean estimate** of all possible participants of the ox meat weigh guessing competition from the **sample ballots** of participants we have.\n- We want to learn something about penguins from the data of the penguins in `palmerpenguins::penguins`\n\n## Practical example: Galton's Data {.smaller}\n\nCompetition: Guess the weight of the meat of an Ox? (Galton, 1907)\n\n*What is the mean of all possible participants $\\mu$?* \n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(readxl)\ngalton <- read_csv(\"data/galton.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 787 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Estimate, id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe (arithmetic) mean of the 787 estimates is $\\hat\\mu =$ 1196.71. \n\n*How sure can we be that $\\hat\\mu$ is close to $\\mu$ and how can we quantify?*^[We already assume that we have an unbiased sample of all possible participants!]\n\n## Practical example: Palmer Penguins {.smaller}\n\nA linear model for body mass of penguins: $y_i = \\beta_0 + \\beta_1 x_i + \\dots + \\beta_m x_m  + \\varepsilon_i$.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.1     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.3.0\n✔ modeldata    1.5.1     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'palmerpenguins'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:modeldata':\n\n    penguins\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n\n```{.r .cell-code}\npeng_workflow <- workflow() |> add_recipe(recipe(body_mass_g ~ ., data = penguins) |> step_rm(year)) |> \n  add_model(linear_reg() |> set_engine(\"lm\"))\npeng_fit <- peng_workflow |> fit(data = penguins)\npeng_fit |> tidy() |> mutate_if(is.numeric, round, 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -1500.     576.      -2.61  0.00961  \n2 speciesChinstrap    -260.      88.6     -2.94  0.00352  \n3 speciesGentoo        988.     137.       7.20  0        \n4 islandDream          -13.1     58.5     -0.224 0.823    \n5 islandTorgersen      -48.1     60.9     -0.789 0.431    \n6 bill_length_mm        18.2      7.14     2.55  0.0113   \n7 bill_depth_mm         67.6     19.8      3.41  0.000734 \n8 flipper_length_mm     16.2      2.94     5.52  0.0000001\n9 sexmale              387.      48.1      8.04  0        \n```\n\n\n:::\n:::\n\n\n. . . \n\nThe column `estimate` shows the coefficients $\\hat\\beta_0, \\hat\\beta_1, \\dots, \\hat\\beta_m$ of the linear model for the variables named in `term`\n\n*How sure can we be that the $\\hat\\beta$'s are close to the $\\beta$'s we are interested in?*^[We already assume that we have an unbiased sample of all penguins!]\n\n## Bootstrapping^[The name comes from the saying \"pull oneself up by one's bootstrap\". It refers to the somehow *magical* idea that we can use our small sample to *mimic* the whole population.] idea {.smaller}\n\n1. Take a **bootstrap sample**: a random sample taken with replacement from the original sample with the same size. \n2. Calculate the *statistics* for the bootstrap sample. \n3. Repeat steps 1. and 2. many times to create a **bootstrap distribution** - a distribution of bootstrap statistics\n4. Use the bootstrap distribution to quantify the uncertainty. \n\nWhat is the statistic in the Galton example? [The mean]{.fragment}   \nWhat are the statistics in the penguins example? [The coefficients]{.fragment}\n\n## Bootstrapping in practice {.smaller}\n\n- You can do the bootstrapping procedure in various ways (including programming in base R or python). \n- We will use convenient functions from `rsample` form the `tidymodels` packages.\n\n## Resamplings: Galton's Data {.smaller}\n\n:::: {.columns}\n\n::: {.column width='35%'}\nThe function `sample` selects `size` random elements from a vector `x` with or without `replace`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(224)\nsample(galton$Estimate, size = 3, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1177 1232 1242\n```\n\n\n:::\n\n```{.r .cell-code}\nsample(galton$Estimate, size = 3, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1270 1440 1232\n```\n\n\n:::\n:::\n\n\nFor tibbles we can use `slice_sample`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> slice_sample(n = 3, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  Estimate    id\n     <dbl> <dbl>\n1     1189   320\n2     1203   367\n3     1181   278\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='35%'}\n::: {.fragment}\nA **bootstrap sample** is with replace and of the same size as the original sample: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(galton)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 787\n```\n\n\n:::\n\n```{.r .cell-code}\ngalton |> slice_sample(n = 787, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 787 × 2\n   Estimate    id\n      <dbl> <dbl>\n 1     1347   774\n 2     1221   489\n 3     1112    85\n 4     1320   766\n 5     1266   697\n 6     1145   149\n 7     1271   711\n 8     1215   442\n 9     1238   596\n10     1163   198\n# ℹ 777 more rows\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::: {.column width='30%'}\n::: {.fragment}\nDo some mean bootstrapping manually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> slice_sample(n = 787, replace = TRUE) |> \n summarize(mean(Estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  `mean(Estimate)`\n             <dbl>\n1            1194.\n```\n\n\n:::\n\n```{.r .cell-code}\ngalton |> slice_sample(n = 787, replace = TRUE) |> \n summarize(mean(Estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  `mean(Estimate)`\n             <dbl>\n1            1197.\n```\n\n\n:::\n\n```{.r .cell-code}\ngalton |> slice_sample(n = 787, replace = TRUE) |> \n summarize(mean(Estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  `mean(Estimate)`\n             <dbl>\n1            1193.\n```\n\n\n:::\n:::\n\n:::\n:::\n\n::::\n\n## Bootstrap set: Galton's Data {.smaller}\n\nThe function `bootstraps` from `rsample` does this type of bootstrapping for us. \n\n:::: {.columns}\n\n::: {.column width='50%'}\n1,000 resamples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton <- galton |> bootstraps(times = 1000)\nboots_galton\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 1,000 × 2\n   splits            id           \n   <list>            <chr>        \n 1 <split [787/295]> Bootstrap0001\n 2 <split [787/293]> Bootstrap0002\n 3 <split [787/296]> Bootstrap0003\n 4 <split [787/303]> Bootstrap0004\n 5 <split [787/283]> Bootstrap0005\n 6 <split [787/295]> Bootstrap0006\n 7 <split [787/295]> Bootstrap0007\n 8 <split [787/285]> Bootstrap0008\n 9 <split [787/291]> Bootstrap0009\n10 <split [787/275]> Bootstrap0010\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n\nThe column `splits` contains a list of split objects. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton$splits[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Analysis/Assess/Total>\n<787/295/787>\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n::: {.fragment}\nFrom each split we can extract the bootstrapped resample with `analysis`. \n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton$splits[[1]] |> analysis()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 787 × 2\n   Estimate    id\n      <dbl> <dbl>\n 1     1160   192\n 2     1260   678\n 3     1147   153\n 4     1221   484\n 5     1204   373\n 6     1165   211\n 7     1112    85\n 8     1415   780\n 9     1081    43\n10     1168   220\n# ℹ 777 more rows\n```\n\n\n:::\n:::\n\n\n(The `assessment` set contains the observations that were not selected randomly in the resample. We do not use it in the following.)\n:::\n:::\n\n::::\n\n## Bootstrap distribution: Galton's Data {.smaller}\n\n:::: {.columns}\n\n::: {.column width='50%'}\nWe iterate with a `map` function^[Here we use `map_dbl` which iterates over the list of splits and reports the values as a vector of doubles (numbers), the default of `map` would be to report also a list.] over the splits and extract the analysis set and calculate the mean. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean <- boots_galton |> \n  mutate(\n   mean = map_dbl(splits, \n                  \\(x) analysis(x)$Estimate |> mean()))\nboots_galton_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 1,000 × 3\n   splits            id             mean\n   <list>            <chr>         <dbl>\n 1 <split [787/295]> Bootstrap0001 1194.\n 2 <split [787/293]> Bootstrap0002 1196.\n 3 <split [787/296]> Bootstrap0003 1199.\n 4 <split [787/303]> Bootstrap0004 1196.\n 5 <split [787/283]> Bootstrap0005 1194.\n 6 <split [787/295]> Bootstrap0006 1197.\n 7 <split [787/295]> Bootstrap0007 1195.\n 8 <split [787/285]> Bootstrap0008 1194.\n 9 <split [787/291]> Bootstrap0009 1197.\n10 <split [787/275]> Bootstrap0010 1193.\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n::: {.fragment}\nPlot the bootstrap distribution of the mean. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean |> \n ggplot(aes(x = mean)) + geom_histogram() +\n theme_minimal(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n:::\n\n::: {.fragment}\nCompare distribution of estimates\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) +\n theme_minimal(base_size = 24)\n```\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n::::\n\n## Quantify uncertainty? Standard error {.smaller}\n\nCompute the standard deviation of the bootstrap distribution. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(boots_galton_mean$mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.732243\n```\n\n\n:::\n:::\n\n\n. . . \n\nThe standard deviation of the bootstrap distribution is called the **standard error** of the statistic.\n\n[Do not confuse standard error (of a statistic) and standard deviation (of a variable).]{style='color:red;'}\n\nThe **standard error of the mean** can also be estimated directly from the original sample $x$ as $\\frac{\\text{sd}(x)}{\\sqrt{n}}$ where $n$ is the sample size. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(galton$Estimate) / sqrt(nrow(galton))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.623085\n```\n\n\n:::\n:::\n\n\n. . . \n\n[Insight:]{style='color:blue;'} The standard error decreases with sample size! However, it decreases only with the square root of the sample size.\n\n[Question:]{style='color:blue;'} You want to shrink the standard error by half. How much larger does the sample size need to be? [4 times]{.fragment}\n\n## Confidence interval {.smaller}\n\nAnother way to quantify uncertainty is to compute a **confidence interval** (CI) for a certain **confidence level**:   \nThe true value of the statistic is expected to lie with the CI with certain probability (the confidence level).\n\n. . .\n\n:::: {.columns}\n\n::: {.column width='30%'}\nCompute the 95% confidence interval of the bootstrap distribution with `quantile`-functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean |> \n  summarize(\n    lower = quantile(mean, 0.025),\n    upper = quantile(mean, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1 1191. 1202.\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='60%'}\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean |> \n ggplot(aes(x = mean)) + geom_histogram() +\n geom_vline(xintercept = quantile(boots_galton_mean$mean, 0.025), color = \"red\") +\n geom_vline(xintercept = quantile(boots_galton_mean$mean, 0.975), color = \"red\") +\n annotate(\"text\", x = 1197, y = 10, label = \"95% of estimates\", color = \"white\", size = unit(12, \"pt\")) +\n theme_minimal(base_size = 24)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n::::\n\n## Confidence Interval: Meaning {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean |> \n  summarize(\n    lower = quantile(mean, 0.025),\n    upper = quantile(mean, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1 1191. 1202.\n```\n\n\n:::\n:::\n\n\nWhat is the correct interpretation? \n\n1. 95% of the estimates in this sample lie between 1191 and 1202.\n2. We are 95% confident that the mean estimate of all potential participants is between 1191 and 1202.\n3. We are 95% confident that the mean estimate of this sample is between 1191 and 1202.\n\n. . . \n\nCorrect: 2.  \n1. The confidence is about a parameter (here the population mean) not about sample values!  \n3. We know the mean of the sample precisely!   \nThe confidence interval assesses where 95% of the values are when do new samples.   \n\n## CI: Precision vs. Confidence {.smaller}\n\n- Can't we increase the confidence level to 99% to get a more precise estimate with a more narrow confidence interval?\n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_galton_mean |> \n  summarize(\n    lower = quantile(mean, 0.005),\n    upper = quantile(mean, 0.995))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1 1190. 1204.\n```\n\n\n:::\n:::\n\n\n[No, it is the other way round:]{style='color:red;'} The smaller and more precise the confidence interval, the lower must the confidence level be.\n\n\n## Bootstrap linear model coefficients {.smaller}  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_workflow <- workflow() |> add_recipe(recipe(body_mass_g ~ ., data = penguins) |> step_rm(year)) |> \n  add_model(linear_reg() |> set_engine(\"lm\"))\npeng_fit <- peng_workflow |> fit(data = penguins)\npeng_fit |> tidy() |> mutate_if(is.numeric, round, 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -1500.     576.      -2.61  0.00961  \n2 speciesChinstrap    -260.      88.6     -2.94  0.00352  \n3 speciesGentoo        988.     137.       7.20  0        \n4 islandDream          -13.1     58.5     -0.224 0.823    \n5 islandTorgersen      -48.1     60.9     -0.789 0.431    \n6 bill_length_mm        18.2      7.14     2.55  0.0113   \n7 bill_depth_mm         67.6     19.8      3.41  0.000734 \n8 flipper_length_mm     16.2      2.94     5.52  0.0000001\n9 sexmale              387.      48.1      8.04  0        \n```\n\n\n:::\n:::\n\n\n. . .\n\nLet us now bootstrap whole models instead of just mean values!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A function to fit a model to a bootstrap sample and tidy the coefficients\nfit_split <- function(split) peng_workflow |> fit(data = analysis(split)) |> tidy() \n# Make 1000 bootstrap samples and fit a model to each\nboots_peng <- bootstraps(penguins, times = 1000) |> \n  mutate(coefs = map(splits, fit_split))\n```\n:::\n\n\n## Bootstrap data frame {.smaller .scrollable}\n\nNow we have a column `coefs` with a list of data frames, each containing the fitted coefficients.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_peng\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 1,000 × 3\n   splits            id            coefs           \n   <list>            <chr>         <list>          \n 1 <split [344/125]> Bootstrap0001 <tibble [9 × 5]>\n 2 <split [344/131]> Bootstrap0002 <tibble [9 × 5]>\n 3 <split [344/123]> Bootstrap0003 <tibble [9 × 5]>\n 4 <split [344/116]> Bootstrap0004 <tibble [9 × 5]>\n 5 <split [344/130]> Bootstrap0005 <tibble [9 × 5]>\n 6 <split [344/126]> Bootstrap0006 <tibble [9 × 5]>\n 7 <split [344/126]> Bootstrap0007 <tibble [9 × 5]>\n 8 <split [344/129]> Bootstrap0008 <tibble [9 × 5]>\n 9 <split [344/129]> Bootstrap0009 <tibble [9 × 5]>\n10 <split [344/129]> Bootstrap0010 <tibble [9 × 5]>\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n\nWe can `unnest` the list column to make a much longer but unnested data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboots_peng |> unnest(coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9,000 × 7\n   splits            id            term    estimate std.error statistic  p.value\n   <list>            <chr>         <chr>      <dbl>     <dbl>     <dbl>    <dbl>\n 1 <split [344/125]> Bootstrap0001 (Inter… -1972.      630.     -3.13   1.91e- 3\n 2 <split [344/125]> Bootstrap0001 specie…  -294.       94.9    -3.10   2.09e- 3\n 3 <split [344/125]> Bootstrap0001 specie…   958.      137.      6.97   1.84e-11\n 4 <split [344/125]> Bootstrap0001 island…    -1.14     57.7    -0.0197 9.84e- 1\n 5 <split [344/125]> Bootstrap0001 island…   -71.9      61.8    -1.16   2.45e- 1\n 6 <split [344/125]> Bootstrap0001 bill_l…    22.3       8.00    2.78   5.68e- 3\n 7 <split [344/125]> Bootstrap0001 bill_d…    80.1      20.6     3.88   1.26e- 4\n 8 <split [344/125]> Bootstrap0001 flippe…    16.8       3.00    5.61   4.40e- 8\n 9 <split [344/125]> Bootstrap0001 sexmale   353.       51.2     6.89   2.94e-11\n10 <split [344/131]> Bootstrap0002 (Inter… -1945.      585.     -3.33   9.82e- 4\n# ℹ 8,990 more rows\n```\n\n\n:::\n:::\n\n\n## Some coefficient distributions {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\ng1 <- boots_peng |> unnest(coefs) |> filter(term == \"islandDream\") |> \n  ggplot(aes(x = estimate)) + geom_histogram() + xlab(\"coefficients islandDream\") +\n  geom_vline(xintercept = 0, color = \"gray\") + theme_minimal(base_size = 24)\ng2 <- boots_peng |> unnest(coefs) |> filter(term == \"bill_length_mm\") |> \n  ggplot(aes(x = estimate)) + geom_histogram() + xlab(\"coefficients bill_length_mm\") +\n  geom_vline(xintercept = 0, color = \"gray\") + theme_minimal(base_size = 24)\ng3 <- boots_peng |> unnest(coefs) |> filter(term == \"flipper_length_mm\") |> \n  ggplot(aes(x = estimate)) + geom_histogram() + xlab(\"coefficients flipper_length_mm\") +\n  geom_vline(xintercept = 0, color = \"gray\") + theme_minimal(base_size = 24)\ng1 + g2 + g3\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-23-1.png){width=1728}\n:::\n:::\n\n\n- We could derive *bootstrapped p-values* from these distributions.\n\n\n## computed p-values from each fit {.smaller}\n\n- Each bootstrap fit also delivers computed p-values for each coefficient.^[These are computed based on theoretical assumptions with the help of t-tests and t-distributions.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng1 <- boots_peng |> unnest(coefs) |> filter(term == \"islandDream\") |> ggplot(aes(x = p.value)) + geom_histogram() + xlab(\"coefficients islandDream\") + theme_minimal(base_size = 24)\ng2 <- boots_peng |> unnest(coefs) |> filter(term == \"bill_length_mm\") |> ggplot(aes(x = p.value)) + geom_histogram() + xlab(\"coefficients bill_length_mm\") + theme_minimal(base_size = 24)\ng3 <- boots_peng |> unnest(coefs) |> filter(term == \"flipper_length_mm\") |> ggplot(aes(x = p.value)) + geom_histogram() + xlab(\"coefficients flipper_length_mm\") + theme_minimal(base_size = 24)\ng1 + g2 + g3\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-24-1.png){width=1728}\n:::\n:::\n\n\n- computed p-values in resamples vary widely (see `islandDream`) when *bootstrapped p-values* show insignificance.\n\n\n\n# Another resampling method\n\n\n## How to evaluate performance on training data only? {.smaller}\n\n- Model performance changes with the random selection of the training data. How can we then reliably compare models?\n- Anyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already \"knows\". \n- Also, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: **Cross validation**\n\n## Cross validation {.smaller}\n\nMore specifically, **$v$-fold cross validation**:\n\n- Shuffle your data and make a partition with $v$ parts\n  - Recall from set theory: A **partition** is a division of a set into mutually disjoint parts which union cover the whole set. Here applied to observations (rows) in a data frame.\n- Use 1 part for validation, and the remaining $v-1$ parts for training\n- Repeat $v$ times\n\n## Cross validation\n\n![](img/cross-validation.png)\n\n# Wisdom of the crowd, Bias and Diversity\n\n## Galton's data {.smaller}\n\n*What is the weight of the meat of this ox?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton <- read_csv(\"data/galton.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 787 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Estimate, id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W11_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n787 estimates, [true value]{style=\"color:green;\"} 1198, [mean]{style=\"color:red;\"} 1196.7\n\n::: aside\nWe focus on the arithmetic mean as aggregation function for the wisdom of the crowd here. \n:::\n\n\n## RMSE Galton's data {.smaller}\n\nDescribe the estimation game as a predictive model:\n\n- All *estimates* are made to predict the same value: the truth. \n  - In contrast to the regression model, the estimate come from people and not from a regression formula.\n- The *truth* is the same for all.\n  - In contrast to the regression model, the truth is one value and not a value for each prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_galton <- galton |> \n mutate(true_value = 1198) |>\n rmse(truth = true_value, Estimate)\nrmse_galton\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        73.6\n```\n\n\n:::\n:::\n\n\n<!-- ## Regression vs. crowd estimation {.smaller} -->\n\n<!-- The linear regression and the crowd estimation problems are similar but not identical! -->\n\n<!-- |Variable      | Linear Regression Model                       | Crowd estimation -->\n<!-- |--------------|-----------------------------------------------|-------------------------------------------- -->\n<!-- | $y_i$        | Data point of response variable               | True value, uniform for all estimators $y_i = y$ -->\n<!-- | $\\hat{y}_i$  | Predicted value $\\hat{y}_i=b_0+b_1+x_1+\\dots$ | Estimate of one estimator -->\n<!-- | $\\bar{y}$    | Mean of response variable                     | Mean of estimates  -->\n\n\n## MSE, Variance, and Bias of estimates {.smaller}\n\nIn a crowd estimation, $n$ estimators delivered the estimates $\\hat{y}_1,\\dots,\\hat{y}_n$. \nLet us look at the following measures\n\n- $\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^n \\hat{y}_i^2$ is the mean estimate, it is the aggregated estimate of the crowd\n\n- $\\text{MSE} = \\text{RMSE}^2 = \\frac{1}{n}\\sum_{i = 1}^n (\\text{truth} - \\hat{y}_i)^2$\n  \n- $\\text{Variance} = \\frac{1}{n}\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2$ \n\n- $\\text{Bias-squared} = (\\bar{y} - \\text{truth})^2$ which is the square difference between truth and mean estimate. \n\nThere is a mathematical relation (a math exercise to check):\n\n$$\\text{MSE} = \\text{Bias-squared} + \\text{Variance}$$\n\n## Testing for Galton's data {.smaller}\n\n$$\\text{MSE} = \\text{Bias-squared} + \\text{Variance}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE <- (rmse_galton$.estimate)^2 \nMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5409.795\n```\n\n\n:::\n\n```{.r .cell-code}\nVariance <- var(galton$Estimate)*(nrow(galton)-1)/nrow(galton)\n# Note, we had to correct for the divisor (n-1) in the classical statistical definition\n# to get the sample variance instead of the estimate for the population variance\nVariance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5408.132\n```\n\n\n:::\n\n```{.r .cell-code}\nBias_squared <- (mean(galton$Estimate) - 1198)^2\nBias_squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.663346\n```\n\n\n:::\n\n```{.r .cell-code}\nBias_squared + Variance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5409.795\n```\n\n\n:::\n:::\n\n\n::: aside\nSuch nice mathematical properties are probably one reason why these squared measures are so popular. \n:::\n\n\n## The diversity prediction theorem^[Notion from: Page, S. E. (2007). The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press.] {.smaller}\n\n- *MSE* is a measure for the average **individuals error**\n- *Bias-squared* is a measure for the **collective error**\n- *Variance* is a measure for the **diversity** of estimates around the mean estimate\n\nThe mathematical relation $$\\text{MSE} = \\text{Bias-squared} + \\text{Variance}$$ can be formulated as \n\n**Collective error = Individual error - Diversity**\n\nInterpretation: *The higher the diversity the lower the collective error!*\n\n\n## Why is this message a bit suggestive? {.smaller}\n\nThe mathematical relation $$\\text{MSE} = \\text{Bias-squared} + \\text{Variance}$$ can be formulated as \n\n**Collective error = Individual error - Diversity**\n\nInterpretation: *The higher the diversity the lower the collective error!*\n\n. . . \n\n- $\\text{MSE}$ and $\\text{Variance}$ are not independent! \n- Activities to increase diversity (Variance) typically also increase the average individual error (MSE).\n- For example, if we just add more random estimates with same mean but wild variance to our sample we increase both and do not gain any decrease of the collective error.\n\n\n## Accuracy for numerical estimate {.smaller}\n\n- For binary classifiers **accuracy** has a simple definition: Fraction of correct classifications. \n  - It can be further informed by other more specific measures taken from the confusion matrix (sensitivity, specificity)\n\nHow about numerical estimators?   \nFor example outcomes of estimation games, or linear regression models? \n\n. . .\n\n- Accuracy is for example measured by (R)MSE\n- $\\text{MSE} = \\text{Bias-squared} + \\text{Variance}$ shows us that we can make a  \n**bias-variance decomposition**\n- That means some part of the error is a systematic (the bias) and another part due to random variation (the variance).\n- The bias-variance tradeoff is also an important concept in statistical learning! \n\n\n## 2-d Accuracy: Trueness and Precision {.smaller}\n\nAccording to ISO 5725-1 Standard: *Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions.* there are two dimension of accuracy of numerical measurement. \n\n![](https://upload.wikimedia.org/wikipedia/commons/9/92/Accuracy_%28trueness_and_precision%29.svg) ![](img/accuracy_trueness_precision.png){height=\"300px\"}\n\n\n\n## What is a wise crowd? {.smaller}\n\nAssume the dots are estimates. **Which is a wise crowd?**\n\n![](img/accuracy_trueness_precision.png){height=\"300px\"}\n\n. . . \n\n- Of course, high trueness and high precision! But, ...\n- Focusing on the **crowd** being wise instead of its **individuals**: High trueness, low precision. \n\n\n\n# Bias-variance trade-off in a script\n\n`penguins_bias_variance.R`\n\n\n",
    "supporting": [
      "W11_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}