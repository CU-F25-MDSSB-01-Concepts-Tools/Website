{
  "hash": "7fee74a87347d8fe05e67227ba128ede",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#12: Collaborative Git, Data Science Projects, Probability\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    toc: true\n    toc-depth: 1\n    slide-number: true\n    chalkboard: \n      buttons: true\n    preview-links: true\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Matrices\n\n## What is a matrix? {.smaller} \n\n:::: {.columns}\n\n::: {.column width='50%'}\n- A **matrix** is a 2-dimensional array of numbers with rows and columns.\n- The numbers in it are called its **elements**.\n\nIsn't that a data frame?\n\n- No, a dataframe is more general. In a matrix, every element has the same basic data type! \n- In R, a matrix is a vector plus some specification of the dimensions.  \n- (A matrix can also be of characters or logicals, it can also have row- and column-names)\n:::\n\n::: {.column width='50%'}\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmatrix(data = 1:6, nrow = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n:::\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmatrix(data = 1:6, ncol = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n```\n\n\n:::\n:::\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmatrix(data = 1:6, nrow = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n```\n\n\n:::\n:::\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmatrix(data = 1:6, nrow = 3, byrow = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n\n## Matrices in Mathematics {.smaller background-color=\"aquamarine\"}\n\n- **Addition** and **subtraction** of matrices is done element-wise.\n- **Multiplication** of a matrix with a scalar is done element-wise.\n- **Multiplication** of two matrices is done row times column.\n- **Transposition** of a matrix is done by flipping the matrix over its diagonal.\n\n\n\n\n\n\n\n\n# Probability for Data Science {background-color=\"aquamarine\"}\n\n## Probability Topics for Data Science {.smaller background-color=\"aquamarine\"}\n\n:::{.incremental}\n  * The concept of probability and the relation to statistics and data\n      * Probability: [Given a probabilistic model, what data will we see?  \n        *What footprints does the animal leave?*]{.fragment}  \n      * Statistics: [Given data, what probabilistic model could produce it?  \n        *What animal could have left these footprints?*]{.fragment}\n  * Probabilistic simulations \n      * Resampling: [Bootstrapping (p-values, confidence intervals), cross-validation]{.fragment}\n  * The confusion matrix: Conditional probabilities and Bayes' theorem\n      * [Sensitivity, specificity, positive and negative predictive value]{.fragment}\n  * Random variables: A probabilistic view on variables in a data frame\n      * Modeling distributions of variables: The concepts of discrete and continuous probability distributions.\n  * The central limit theorem or why the normal distribution is so important.\n\n:::\n\n## What is probability {.smaller background-color=\"aquamarine\"}\n\n::: {.incremental}\n- One of the most successful mathematical models used in many domains. \n- We have a certain intuition of probability visible in sentences like: *\"That's not very probable.\"* or *\"That is likely.\"*\n- A simplified but formalized way to think about [**uncertain events**]{style='color:blue;'}.  \n- Two flavors: *Objective* (*Frequentist*) or *Subjective* (*Bayesian*) probability. \n    - **Objective interpretation:** Probability is *relative frequency* in the limit of indefinite sampling. Long run behavior of non-deterministic outcomes. \n    - **Subjective interpretation:** Probability is a belief about the likelihood of an event. \n- Related to [**data**]{style='color:blue;'}: \n    - *Frequentist philosophy*: The parameters of the population we sample from are fixed and the data is a random selection. \n    - *Bayesian philosophy:* The data we know is fixed but the parameters of the population are random and associated with probabilities. \n:::\n\n# Events as subsets of a sample space {background-color=\"aquamarine\"}\n\n## Sample space, atomic events, events {.smaller background-color=\"aquamarine\"}\n\nIn the following, we say $S$ is the **sample space** which is a set of **atomic events**. \n\nExample for sample spaces:\n\n:::{.incremental}\n  * [Coin toss]{style='color:blue;'}: Atomic events $H$ (HEADS) and $T$ (TAILS), sample space $S = \\{H,T\\}$.  \n  * Selection [person from a group]{style='color:blue;'} of $N$ individuals labeled $1,\\dots,N$, sample space $S = \\{1,\\dots,N\\}$. \n  * [Two successive coin tosses]{style='color:blue;'}: Atomic events $HH$, $HT$, $TH$, $TT$; sample space  $S = \\{HH,HT, TH, TT\\}$. **Important:** Atomic events are *not* just $H$ and $T$. \n  * [COVID-19 Rapid Test + PCR Test for confirmation]{style='color:blue;'}: Atomic events *true positive* $TP$ (positive test confirmed), *false positive* $FP$ (positive test not confirmed), *false negative* $FN$ (negative test not confirmed), *true negative* $TN$ (negative test confirmed), sample space $S = \\{TP, FP, FN, TN\\}$.\n:::\n\n. . . \n\nAn **event** $A$ is a *subset* of the sample space $A \\subset S$.   \n\n[Important: *Atomic events* are *events* but not all events are atomic.]{style='color:red;'}\n\n\n## Example events for one coin toss {.smaller background-color=\"aquamarine\"}\n\n- The set with one atomic event is a subset $\\{H\\} \\subset \\{H,T\\}$. \n- Also the sample space $S = \\{H,T\\} \\subset \\{H,T\\}$ is an event. It is called the *sure event*. \n- Also the empty set $\\{\\} = \\emptyset \\subset \\{H,T\\}$ is an event. It is called the *impossible event*. \n- Interpretation: \n    - Event $\\{H,T\\}$ = \"The coin comes up HEAD *or* TAIL.\" \n    - Event $\\{\\}$ = \"The coin comes up *neither* HEADS *nor* TAILS.\" \n\nTwo coin tosses: \n\n- Event $\\{HH, TH\\}$ = \"The first toss comes up HEAD or TAIL and the second is HEADS.\"   \n- Event $\\{HT, TH, HH\\}$ = \"We have HEAD once or twice and it does not matter what coins.\" \n- The event $\\{TT, HH\\}$ = \"Both coins show the same side.\"\n\n## Questions for three coin tosses  {.smaller background-color=\"aquamarine\"}\n  \n  * \"The coins show one HEAD\" = [$\\{HTT, THT, TTH\\}$]{.fragment}\n  * \"The first and the third coin are not HEAD?\" = [$\\{THT, TTT\\}$]{.fragment}\n  * How many *atomic events* exist for three coin tosses? [$2^3=8$]{.fragment}\n\n. . .\n\n**For selecting one random person:**  \n\nEvent $\\{2,5,6\\}$ = [The selected person is *either* 2, 5, or 6.   \n(Not all three people which is a different random variable!)]{.fragment}\n\n. . .\n\n**For COVID-19 testing:**\n\n- Event $\\{TP, FP\\}$ = [The test is positive.]{.fragment}\n- Event $\\{TN, FP\\}$ = [The person does not have COVID-19.]{.fragment}\n- Event $\\{TP, TN\\}$ = [The rapid test delivers the correct result.]{.fragment}\n\n\n# The set of all events and the probability function  {background-color=\"aquamarine\"}\n\n## The set of all events  {.smaller background-color=\"aquamarine\"}\n\n- The **set of all events** is the set of all subsets of a sample space $S$. \n- When the sample space has $n$ atomic events, the set of all events has $2^n$ elements.\n- The set of all events is very large also for fairly simple examples!\n\n**Examples**\n\n* For 3 coin tosses: How many events exist? [$2^3=8$ atomic events $\\to$ $2^8=256$ event]{.fragment} \n* How is it for four coin tosses? [$2^{(2^4)} = 65536$]{.fragment} \n* Select two out of five people (without replacement)^[Mathematicall called \"n choose k\" ${n \\choose k} =\\frac{n!}{(n-k)!k!}$. Here: ${5\\choose 2}$.]?   \n  [Ten atomic events: 12, 13, 14, 15, 23, 24, 25, 34, 35, 45. Events: $2^{10} = 1024$]{.fragment}\n\n. . . \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nchoose(5,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n[These are typical problems of *combinatorics*, the **theory of counting**.]{style='color:blue;'} \n\n## Probability function {.smaller background-color=\"aquamarine\"}\n\n**Definition:** A set of all events a function $\\text{Pr}: \\text{Set of all subsets of $S$} \\to \\mathbb{R}$ is a **probability function** when \n\n  (i) The probability of any event is between 0 and 1: $0\\leq \\text{Pr}(A) \\leq 1$. (So, actually a probability function is a function into the interval $[0,1]$.)\n  (ii) The probability of the event coinciding with the whole sample space (the sure event) is 1: $\\text{Pr}(S) = 1$. \n  (iii) For events $A_1, A_2, \\dots, A_n$ which are *pairwise disjoint* we can sum up their probabilities:  \n$$\\text{Pr}(A_1 \\cup A_2\\cup\\dots\\cup A_n) = \\text{Pr}(A_1) + \\text{Pr}(A_2) + \\dots + \\text{Pr}(A_n) $$\n\n[This captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events.]{style='color:blue;'}\n\n\n## Example Probability Function {.smaller background-color=\"aquamarine\"}\n\n**Example coin tosses:** We can define a probability function $\\text{Pr}$ by assigning the same probability to each atomic event.\n\n- $\\text{Pr}(\\{H\\}) = \\text{Pr}(\\{T\\}) = 1/2$\n- $\\text{Pr}(\\{HH\\}) = \\text{Pr}(\\{HT\\}) = \\text{Pr}(\\{TH\\}) = \\text{Pr}(\\{TT\\}) = 1/4$\n\n. . . \n\nSo, the probability one or zero HEADs is $\\text{Pr}(\\{HT, TH, TT\\}) = \\text{Pr}(\\{HT\\}) + \\text{Pr}(\\{TH\\}) + \\text{Pr}(\\{TT\\}) = \\frac{3}{4}$.\n\n. . . \n\n**Example selection of two out of five people:** We can define a probability function $\\text{Pr}$ by assigning the same probability to each atomic event.\n\n- $\\text{Pr}(\\{12\\}) = \\text{Pr}(\\{13\\}) = \\dots = \\text{Pr}(\\{45\\}) = 1/10$.\n\n. . . \n\nSo, the probability that 1 is among the selected $\\text{Pr}(\\{12, 13, 14, 15\\}) = \\frac{4}{10}$.\n\n\n## Some basic probability rules {.smaller  background-color=\"aquamarine\"}\n\n:::: {.columns}\n\n::: {.column width='80%'}\n  * We can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function. \n  \n  * $\\text{Pr}(\\emptyset) = 0$\n  * For any events $A,B \\subset S$ it holds\n    * $\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)$\n    * $\\text{Pr}(A \\cap B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cup B)$\n    * $\\text{Pr}(A^c) = 1 - \\text{Pr}(A)$\n:::\n\n::: {.column width='20%'}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Venn0111.svg/1280px-Venn0111.svg.png){height=\"100px\"}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Venn0001.svg/1280px-Venn0001.svg.png){height=\"100px\"}\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Venn1010.svg/1280px-Venn1010.svg.png){height=\"100px\"}\n:::\n\n::::\n  \n. . . \n\n**Recap from the motivation of logistic regression:** When the probability of an event is $A$ is $\\text{Pr}(A)=p$, then its **odds** (in favor of the event) are $\\frac{p}{1-p}$. The logistic regression model \"raw\" predictions are **log-odds** $\\log\\frac{p}{1-p}$. \n\n\n## Conditional probability {.smaller background-color=\"aquamarine\"}  \n\n\n:::: {.columns}\n\n::: {.column width='75%'}\n**Definition:** The **conditional probability** of an event $A$ *given* an event $B$ (write \"$A | B$\") is defined as\n\n$$\\text{Pr}(A|B) = \\frac{\\text{Pr}(A \\cap B)}{\\text{Pr}(B)}$$\n:::\n\n::: {.column width='25%'}\n![](img/conditionalprobability.png){height=\"150px\"}\n:::\n::::\n\nWe want to know the probability of $A$ *given* that we know that $B$ has happened (or is happening for sure).\n\n**Two coin flips:** $A$ = \"first coin is HEAD\", $B$ = \"one or zero HEADS in total\". What is $\\text{Pr}(A|B)$?\n[$A$ = \\{HH, HT\\}, $B$ = \\{TT, HT, TH\\} $\\to$ $A \\cap B = \\{HT\\}$         \n$\\to$ $\\text{Pr}(A\\cap B) = \\frac{3}{4}$, $\\text{Pr}(A\\cap B) = \\frac{1}{4}$             \n$\\to$ $\\text{Pr}(A|B) = \\frac{1/4}{3/4} = \\frac{1}{3}$]{.fragment}\n\n\n\n## More examples of conditional probability {.smaller background-color=\"aquamarine\"}\n\n**COVID-19 Example:** What is the probability that a random person in the tested sample has COVID-19 (event $P$ \"positive\") *given* that she has a positive test result (event $PP$ \"predicted positive\")?\n\n$$\\text{Pr}(P|PP) = \\frac{\\text{Pr}(P \\cap PP)}{\\text{Pr}(PP)}$$\n\n**Definition p-value:** *Probability of observed or more extreme outcome given that the null hypothesis ($H_0$) is true.*\n\n$$\\text{p-value} = \\text{Pr}(\\text{observed or more extreme outcome for test-statistic} | H_0)$$\n\n\n# Probability in the Confusion Matrix  {background-color=\"aquamarine\"}\n\n## Confusion Matrix {.smaller background-color=\"aquamarine\"}\n\n[**Confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix)\nof statistical classification, large version: \n \n![](img/confusion_large.png)\n\n\n## 4 probabilities in confusion matrix {.smaller background-color=\"aquamarine\"}\n\n:::: {.columns}\n\n::: {.column width='35%'}\n![](img/confusion_small.png)\n:::\n\n::: {.column width='65%'}\n[**Sensitivity and Specificity**]{style='color:blue;'}\n\n[$\\to \\atop \\ $]{style=\"border-width:1px; border-style:solid;\"} **Sensitivity** is the *true positive rate*: TP / (TP + FN)  \n[$\\ \\atop \\to$]{style=\"border-width:1px; border-style:solid;\"} **Specificity** is the *true negative rate*: TN / (TN + FP)\n\n[**Positive/negative predictive value**]{style='color:blue;'}\n\n[$\\scriptsize\\downarrow \\ $]{style=\"border-width:1px; border-style:solid;padding-left:2px;padding-right:2px;\"} **Positive predictive value**: TP / (TP + FP)  \n[$\\scriptsize\\ \\downarrow$]{style=\"border-width:1px; border-style:solid;padding-left:2px;padding-right:2px;\"} **Negative predictive value**: TN / (TN + FN)\n:::\n\n::::\n\nHere TP, TN, FP, FN are the numbers of true positives, true negatives, false positives, and false negatives.\n\nAs the set of atomic events $\\{TP\\}, \\{FP\\}, \\{FN\\}, \\{TN\\}$ we can define the probabilities of the events like $\\text{Pr}(\\{TP\\}) = \\frac{TP}{N}$ with $N = TP + FP + FN + TN$.\n\n\n\n\n## ... as conditional probabilities {.smaller background-color=\"aquamarine\"}\n\nSensitivity and specificity are conditional probabilities:\n\n- **Sensitivity** is the probability of a positive test result *given* that the person has the condition: $\\text{Pr}(PP|P) = \\frac{TP}{TP + FN}$\n\n- **Specificity** is the probability of a negative test result *given* that the person does not have the condition: $\\text{Pr}(PN|N) = \\frac{TN}{TN + FP}$\n\n- **Positive predictive value** is the probability of the condition *given* that the test result is positive: $\\text{Pr}(P|PP) = \\frac{TP}{TP + FP}$\n\n- **Negative predictive value** is the probability of the condition *given* that the test result is negative: $\\text{Pr}(N|PN) = \\frac{TN}{TN + FN}$\n\n[**Note:** The positive predictive value $\\text{Pr}(P|PP)$ is sensitivity $\\text{Pr}(PP|P)$ with \"flipped\" conditionality.]{style='color:red;'}\n\n## Bayes' Theorem {.smaller background-color=\"aquamarine\"}\n\n[**Bayes' Theorem**](https://en.wikipedia.org/wiki/Bayes%27_theorem) is a fundamental theorem in probability theory that relates the conditional probabilities $\\text{Pr}(A|B)$ and $\\text{Pr}(B|A)$ to the marginal probabilities $\\text{Pr}(A)$ and $\\text{Pr}(B)$:\n\n$$\\text{Pr}(A|B) = \\frac{\\text{Pr}(B|A) \\cdot \\text{Pr}(A)}{\\text{Pr}(B)}$$\n\n. . .\n\n**Example:** What is the probability that a random person in the tested sample has COVID-19 ($P$ = positive) *given* that she has a positive test result ($PP$ = predicted positive)?\n\n$$\\text{Pr}(P|PP) = \\frac{\\text{Pr}(PP|P) \\cdot \\text{Pr}(P)}{\\text{Pr}(PP)}$$\nSo, we can compute the positive predictive value $\\text{Pr}(P|PP)$ from the sensitivity $\\text{Pr}(PP|P)$ and the rate (or probability) of positive conditions $\\text{Pr}(P)$ and the rate (or probability) of positive tests.\n\n\n\n## Prevalence {.smaller background-color=\"aquamarine\"}\n\n- The rate (or probability) of the positive conditions in the population is also called **Prevalence**:\n\n$$\\text{Pr}(P) = \\frac{P}{N} = \\frac{TP + FN}{N}$$ \n\n- Sensitivity and Specificity are properties of the test (or classifier) and are independent of the prevalence of the condition in the population of interest.\n\n- The Positive/Negative Predictive Values are not!\n\n\n## How the positive predictive value (PPV) depends on prevalence {.smaller background-color=\"aquamarine\"}\n\nWe assume a test with sensitivity 0.9 and specificity 0.99. $N = 1000$ people were tested. \n\n:::: {.columns}\n\n::: {.column width='33%'}\n\n&nbsp; | PP  | PN  |\n-------|-----|-----|\n**P**  | TP  | FN  |\n**N**  | FP  | TN  |\n\n\nSensitivity = TP / P  \nSpecificity = TN / N  \nPPV = TP / PP  \nPrevalence = P / N  \n:::\n\n::: {.column width='33%'}\n:::{.fragment}\n**Prevalence = 0.1** \n\n$P$ = 100\n$N$ = 900\n\n&nbsp; | PP  | PN  |\n-------|-----|-----|\n**P**  | 90  | 10  |\n**N**  | 9   | 891 |\n\nPPV = 90 / (90 + 9) = 0.909\n\nFrom the positive tests 90.9% have COVID-19.\n:::\n:::\n\n::: {.column width='33%'}\n:::{.fragment}\n**Prevalence = 0.01** \n\n$P$ = 10\n$N$ = 990\n\n&nbsp; | PP  | PN  |\n-------|-----|-----|\n**P**  | 9  | 1        |\n**N**  | 9.9  | 980.1  |\n\nPPV = 9 / (9 + 9.9) = 0.476\n\nFrom the positive tests only 47.6% have COVID-19!\n:::\n:::\n\n\n\n::::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}