{
  "hash": "ff9710a3eadd0932e7723f699e374106",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"W#05: Descriptive Statistics, Exploratory Data Analysis\"\nauthor: Jan Lorenz\nformat: \n  revealjs: \n    toc: true\n    toc-depth: 1\n    slide-number: true\n    chalkboard: \n      buttons: true\n    preview-links: true\n    logo: img/ConstructorUniversity.png\n    footer: \"MDSSB-DSCO-02: Data Science Concepts\"\nbibliography: \"/home/janlo/Documents/literature/litlorenz_zot.bib\"\neditor_options: \n  chunk_output_type: console\n---\n\n## Preliminaries {.smaller}\n\nIn this lectures we will use these packages and datasets. You need to do this code in the Console to download data and play with some of the code in this lecture.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'palmerpenguins'\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n```\n\n\n:::\n\n```{.r .cell-code}\nif (!file.exists(\"data/galton.csv\")) {\n  download.file(url = \"https://raw.githubusercontent.com/CU-F24-MDSSB-01-Concepts-Tools/Website/refs/heads/main/data/galton.csv\", \n                destfile = \"data/galton.csv\")\n} \nif (!file.exists(\"data/Viertelfest.csv\")) {\n  download.file(url = \"https://raw.githubusercontent.com/CU-F24-MDSSB-01-Concepts-Tools/Website/refs/heads/main/data/Viertelfest.csv\", \n                destfile = \"data/Viertelfest.csv\")\n} \ngalton <- read_csv(\"data/galton.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 787 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Estimate, id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nviertel <- read_csv(\"data/Viertelfest.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1226 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date Time\ndbl (2): Losnummer, Schätzung\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n::: aside\nTip: Run the script in an R-project and have a folder `data/` in it such that the local path works!\n:::\n\n\n# Descriptive Statistics \n\n\n## Descriptive vs. Inferential Statistics {background-color=\"khaki\"}\n\n- The process of using and analyzing [**summary statistics**]{style='color:red;'}\n  - Solely concerned with properties of the **observed data**.\n\n- Distinct from [**inferential statistics**]{style='color:red;'}: \n  - Inference of properties of an underlying distribution given sampled observations from a larger population. \n  \n  \n**Summary Statistics** are used to summarize a set of observations to communicate the largest amount of information as simple as possible. \n\n\n## Summary statistics {background-color=\"khaki\"}\n\n*Univariate* (for one variable)\n\n- Measures of **location**, or *central tendency*\n- Measures of statistical **dispersion** \n- Measure of the **shape** of the distribution like skewness or kurtosis\n\n*Bivariate* (for two variables)\n\n- Measures of statistical dependence or **correlation**\n\n# Measures of central tendency \n\n## Measures of central tendency {.smaller background-color=\"aquamarine\"}\n\nGoal: For a sequence of numerical observations $x_1,\\dots,x_n$ we want to measure\n\n- the \"typical\" value.\n- a value summarizing the **location** of values on the numerical axis.\n\nThree different ways:\n\n1. **Arithmetic mean** (also *mean*, *average*): Sum of the all observations divided by the number of observations $\\frac{1}{n}\\sum_{i=1}^n x_i$\n2. **Median**: Assume $x_1 \\leq x_2 \\leq\\dots\\leq x_n$. Then the median is middlemost values in the sequence $x_\\frac{n+1}{2}$ when $n$ odd. For $n$ even there are two middlemost values and the median is $\\frac{x_\\frac{n}{2} + x_\\frac{n+1}{2}}{2}$\n3. **Mode**: The value that appears most often in the sequence. \n\n\n## Measures of central tendency: Examples {.smaller background-color=\"aquamarine\"}\n\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nx <- c(1, 2, 4, 10, 300)\nmean(x) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 63.4\n```\n\n\n:::\n:::\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmedian(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n:::\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\ny <- c(-2, -2, 4, 7, 7, 7)\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.5\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nmedian(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.5\n```\n\n\n:::\n:::\n\n\nMedian of an even number of numbers: Mean of two most central numbers. \n:::\n\n::: {.column width='50%'}\nThere is no function for the *Mode* in R. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nMode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n```\n:::\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nMode(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nMode(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\nWarning: Mode is not unique and there is no fix like for the Median. \n:::\n\n::::\n\n\n\n\n## Philosophy of aggregation {.smaller background-color=\"khaki\"}\n\n::: {.incremental}\n- The **mean** represents *total value* per value.   \n[Example: *per capita income* in a town is the total income per individual]{style=\"color:blue;\"}\n- The **median** represents the value such that half of the values are lower *and* higher.    \n[In a *democracy* where each value is represented by one voter preferring it, the median is the value which is *unbeatable* by an *absolute majority*. Half of the people prefer higher the other half lower values. ([Median voter model](https://en.wikipedia.org/wiki/Median_voter_theorem))]{style='color:blue;'}\n- The **mode** represents the most common value.  \n[In a *democracy*, the mode represents the winner of a *plurality vote* where each value runs as a candidate and the winner is the one with the most votes.]{style='color:blue;'}\n:::\n\n## Mean, Median, Mode properties  {.smaller background-color=\"aquamarine\"}\n\n**Do they deliver one unambiguous answer for any sequence?**\n\n. . . \n\nMean and median, yes.   \nThe mode has no rules for a tie. \n\n. . . \n\n**Can they by generalized to variables with ordered or even unordered categories?**\n\n. . .\n\nMean: No.   \nMedian: For ordered categories (except when even number and the two middlemost are not the same)   \nMode: For any categorical variable.\n\n. . .\n\n**Is the measure always also in the data sequence?**\n\n. . . \n\nMean: No.   \nMedian: Yes, for sequences of odd length.   \nMode: Yes. \n\n\n## Generalized means^[Also called *power mean* or *$p$-mean*. ] {.smaller background-color=\"aquamarine\"}\n\nFor $x_1, \\dots, x_n > 0$ and $p\\in \\mathbb{R}_{\\neq 0}$ the generalized mean is\n\n$$M_p(x_1, \\dots, x_n) = (\\frac{1}{n}\\sum_{i=1}^n x_i^p)^\\frac{1}{p}$$\n\nFor $p = 0$ it is $M_0(x_1, \\dots, x_n) = (\\prod_{i=1}^n x_i)^\\frac{1}{n}$. \n\n$M_1$ is the arithmetic mean. $M_0$ is called the **geometric mean**. $M_{-1}$ the **harmonic mean**. \n\nNote: Generalized means are often only reasonable when all values are positive $x_i > 0$.\n\n::: aside\n$M_0$ can also be expressed as the exponential ($\\exp(x) = e^x$) of the mean of the the $\\log$'s of the $x_i$'s: $\\exp(\\log((\\prod_{i=1}^n x_i)^\\frac{1}{n})) = \\exp(\\frac{1}{n}\\sum_{i=1}^n\\log(x_i))$. \n:::\n\n## Box-Cox transformation function  {.smaller background-color=\"aquamarine\" .scrollable}\n\nFor $p \\in \\mathbb{R}$: $f(x) = \\begin{cases}\\frac{x^p - 1}{p} & \\text{for $p\\neq 0$} \\\\ \\log(x) & \\text{for $p= 0$}\\end{cases}$\n\n::: {.columns}\n\n::: {.column width='60%' .smaller}\nThe $p$-mean is\n\n$$M_p(x) = f^{-1}(\\frac{1}{n}\\sum_{i=1}^n f(x_i))$$\n\nwith $x = [x_1, \\dots, x_n]$. $f^{-1}$ is the **inverse** of $f$.\n\n\n- **Inverse** means $f^-1(f(x)) = x =f(f^-1(x))$.  \n- Box-Cox is a common transformation in data pre-processing to make the variable's (arithmetic) mean being a \"good\" measure of central tendency.\n:::\n\n::: {.column width='40%'}\n\n::: {.cell}\n\n```{.r .cell-code}\npfun <- function(x, p) (x^p-1)/p\nipfun <- function(x, p) (p*x + 1)^(1/p)\nggplot() + \n\tgeom_function(fun = pfun, args = list(p = 1), color=\"red\", size = 1.5) +\n\tgeom_function(fun = pfun, args = list(p = 2), color = \"red\", alpha=0.6) + \n\tgeom_function(fun = pfun, args = list(p = 3), color = \"red\", alpha=0.3) +  \n\tgeom_function(fun = pfun, args = list(p = 1/2), color = \"red3\") + \n\tgeom_function(fun = pfun, args = list(p = 1/3), color = \"red4\") + \n\tgeom_function(fun = pfun, args = list(p = -1), color = \"blue\", size = 1.5) +  \n\tgeom_function(fun = pfun, args = list(p = -1/2), color = \"blue3\") + \n\tgeom_function(fun = pfun, args = list(p = -1/3), color = \"blue4\") + \n\tgeom_function(fun = pfun, args = list(p = -2), color = \"blue\", alpha=0.6) + \n\tgeom_function(fun = pfun, args = list(p = -3), color = \"blue\", alpha=0.3) + \n\tgeom_function(fun = log, color = \"black\", size = 1.5) + coord_fixed() +\n\txlim(c(0.01,4)) + ylim(c(-2,2)) + \n\tlabs(x=\"x\", y = \"f(x)\", title = \"p = -1 (blue), 0 (black), +1 (red)\") + \n\ttheme(title = element_text(size = 2)) +\n\ttheme_minimal() \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 26 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 45 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 53 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 13 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_function()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-9-1.png){width=384}\n:::\n:::\n\n:::\n\n:::\n\n\n\n# Measures of central tendency and the Wisdom of the Crowd \n\n## Application: The Wisdom of the Crowd {.smaller background-color=\"khaki\"}\n\n::: {.columns}\n\n::: {.column width='80%'}\n- **Phenomenon:** When collective estimate of a diverse group of independent individuals is better than that of single experts. \n- The classical wisdom-of-the-crowds finding is about **point estimation** of a continuous quantity.\n- Popularized by James Surowiecki (2004).\n- The opening anecdote is about Francis Galton's^[Galton (1822-1911) was a half-cousin to Charles Darwin and one of the founding fathers of statistics. He also was a scientific racist, see <https://twitter.com/kareem_carr/status/1575506343401775104?s=20&t=8T5TzrayAWNShmOSzJgCJQ.>.] surprise in 1907 that the crowd at a county fair accurately guessed the weight of an ox's meat when their individual guesses were averaged.\n:::\n\n::: {.column width='20%'}\n\n![](https://upload.wikimedia.org/wikipedia/en/9/95/Wisecrowds.jpg) \n\n![](https://upload.wikimedia.org/wikipedia/commons/a/ae/Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg){width=150}  \n:::\n\n:::\n\n## Galton's data^[Kenneth Wallis dug out the data from Galton's notebook and put it here <https://warwick.ac.uk/fac/soc/economics/staff/kfwallis/publications/galton_data.xlsx>] {.smaller background-color=\"khaki\"}\n\n*What is the weight of the meat of this ox?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\") + \n geom_vline(xintercept = median(galton$Estimate), color = \"blue\") + \n geom_vline(xintercept = Mode(galton$Estimate), color = \"purple\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n787 estimates, [true value]{style=\"color:green;\"} 1198, [mean]{style=\"color:red;\"} 1196.7, [median]{style=\"color:blue;\"} 1208, [mode]{style=\"color:purple;\"} 1218\n\n\n## Viertelfest Bremen 2008^[Data collected as additional guessing game at the Lottery \"Haste mal 'nen Euro?\", data provided by Jan Lorenz  <https://docs.google.com/spreadsheets/d/1HiYhUrYrsbeybJ10mwsae_hQCawZlUQFOOZzcugXzgA/edit#gid=0>] {.smaller background-color=\"aquamarine\"}\n\n*How many lots will be sold by the end of the festival?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel |> ggplot(aes(`Schätzung`)) + geom_histogram() +\n geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + \n geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + \n geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n1226 estimates, the maximal value is 29530000!  We should filter ... \n\n\n## Viertelfest Bremen 2008 {.smaller background-color=\"khaki\"}\n\n*How many lots will be sold by the end of the festival?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel <- read_csv(\"data/Viertelfest.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1226 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date Time\ndbl (2): Losnummer, Schätzung\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) +  \n geom_vline(xintercept = 10788, color = \"green\") +\n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + \n geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + \n geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\") + \n geom_vline(xintercept = exp(mean(log(viertel$Schätzung))), color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n1226 estimates, [true value]{style=\"color:green;\"} 10788, [mean]{style=\"color:red;\"} 53163.9, [median]{style=\"color:blue;\"} 9843, [mode]{style=\"color:purple;\"} 10000,   \n[geometric mean]{style=\"color:orange;\"} 10510.1\n\n\n## $\\log_{10}$ transformation Viertelfest {.smaller background-color=\"khaki\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel |> mutate(log10Est = log10(Schätzung)) |> ggplot(aes(log10Est)) + geom_histogram(binwidth = 0.05) +\n geom_vline(xintercept = log10(10788), color = \"green\") + \n geom_vline(xintercept = log10(mean(viertel$Schätzung)), color = \"red\") + \n geom_vline(xintercept = log10(median(viertel$Schätzung)), color = \"blue\") + \n geom_vline(xintercept = log10(Mode(viertel$Schätzung)), color = \"purple\") + \n geom_vline(xintercept = mean(log10(viertel$Schätzung)), color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n1226 estimates, [true value]{style=\"color:green;\"} 10788, [mean]{style=\"color:red;\"} 53163.9, [median]{style=\"color:blue;\"} 9843, [mode]{style=\"color:purple;\"} 10000,   \n[geometric mean]{style=\"color:orange;\"} 10510.1\n\n\n## Wisdom of the crowd insights {.smaller background-color=\"khaki\"}\n\n::: {.incremental}\n- In Galton's sample the different measures do not make a big difference\n- In the Viertelfest data the arithmetic mean performs very bad!\n- The mean is *vulnerable to extreme values*.   \nQuoting Galton on the mean as a democratic aggregation function:   \n*\"The mean gives voting power to the cranks in proportion to their crankiness.\"*\n- The mode tends to be on *focal* values as round numbers (10,000). In Galton's data this is not so pronounced beause estimators used several weight units (which Galton converted to pounds). \n- **How to choose a measure to aggregate the wisdom?**\n  - By the nature of the estimate problem? Is the scale mostly clear? (Are we in the hundreds, thousands, ten thousands, ...)\n  - By the nature of the distribution?\n  - There is no real insurance against a systematic bias in the population.\n:::\n  \n\n# Measures of dispersion\n\n## Measures of dispersion^[Also called *variability*, *scatter*, or *spread*.] {.smaller background-color=\"aquamarine\"}\n\nGoal: We want to measure \n\n- How spread out values are around the central tendency. \n- How stretched or squeezed is the distribution?\n\n**Variance** is the mean of the squared deviation from the mean:  $\\text{Var}(x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2$ where $\\mu$ (mu) is the mean. \n\n. . .\n\n**Standard deviation** is the square root of the variance $\\text{SD}(x) = \\sqrt{\\text{Var}(x)}$. \n\nThe standard deviation is often denoted $\\sigma$ (sigma) and the variance $\\sigma^2$.\n\n. . .\n\n**Mean absolute deviation** (MAD) is the mean of the absolute deviation from the mean:  $\\text{MAD}(x) = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\mu|$.\n\n. . .\n\n[Warning:]{style='color:red;'} MAD can also be **Median absolute deviation from the median**.\n\n**Range** is the difference of the maximal and the minimal value $\\max(x) - \\min(x)$.\n\n\n## Examples of measures of dispersion {.smaller background-color=\"khaki\"}\n\n:::: {.columns}\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(galton$Estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5415.013\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(galton$Estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 73.58677\n```\n\n\n:::\n\n```{.r .cell-code}\nmad(galton$Estimate) # Warning: median absolute deviation is default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51.891\n```\n\n\n:::\n\n```{.r .cell-code}\nrange(galton$Estimate) # Oh, range gives us a vector of min and max. So, we diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  896 1516\n```\n\n\n:::\n\n```{.r .cell-code}\ndiff(range(galton$Estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 620\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='50%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(viertel$Schätzung)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 719774887849\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(viertel$Schätzung)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 848395.5\n```\n\n\n:::\n\n```{.r .cell-code}\nmad(viertel$Schätzung)  # Warning: median absolute deviation is default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8771.803\n```\n\n\n:::\n\n```{.r .cell-code}\nrange(viertel$Schätzung) # Oh, range gives us a vector of min and max. So, we diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]      120 29530000\n```\n\n\n:::\n\n```{.r .cell-code}\ndiff(range(viertel$Schätzung))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 29529880\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n:::{.aside}\nVariance (and standard deviation) in statistics is usually computed with $\\frac{1}{n-1}$ instead of $\\frac{1}{n}$ to provide an unbiased estimator of the potentially underlying population variance. We omit more detail here.  \n:::\n\n## Normalization of variables {.smaller background-color=\"aquamarine\"}\n\nIn Machine Learning, Statistics, and Descripitve Analysis we often want to bring different variables to **common scales**. We want to make the *dispersion* and the *location* comparable.  \n\nTo that end, some linear transformation are common:\n\n- Standardization\n- Min-max Feature Scaling\n\nWhen we normalize a variable we receive a dimensionless variable. It does not have a unit.   \nExample: We measure height in $m$ meters. When we standardize are scale by min-max the new variable has no unit. Mathematically it cancels out.\n\n \n\n## Standardization {.smaller background-color=\"aquamarine\"}\n\nVariables are *standardized* by subtracting their mean and then dividing by their standard deviations. \n\nA value from a standardized variable is called a **standard score** or **z-score**. \n\n$z_i = \\frac{x_i - \\mu}{\\sigma}$ \n\nwhere $\\mu$ is the mean and $\\sigma$ the standard deviation of the vector $x$.\n\n- This is a *shift-scale transformation*. We shift each value by the mean and scale by the standard deviation. \n- A standard score $z_i$ represents how many standard deviations $x_i$ is away from the mean of $x$.\n- The standard scores $z_i$'s have a mean of zero and a standard deviation of one (by construction). \n\n\n## Min-max Feature Scaling  {.smaller background-color=\"aquamarine\"}\n\nWhen we want to make the values of the scaled variable to range from zero to one. \n\nThe transformed variable values $y_i$'s are\n\n$$y_i = \\frac{x_i - \\min(x)}{\\max(x) - \\min(x)}.$$\n\nWe shift by the minimum $\\min(x)$ and scale by the range $\\max(x) - \\min(x)$.\n\n- What are mean and standard deviation of $y$? [We do not know, but less than one.]{.fragment}\n- Caution: The new values are heavily dependent of the actual values of $\\min$ and $\\max$! \n\n\n# Data Overview with `summary`\n\n## Data Sets 1a and 1b: Widsom of Crowd {.smaller}\n\n1a: Ox weigh guessing competition 1907 (collected by **Galton**)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n1b: **Viertelfest** \"guess the number of sold lots\"-competition 2009\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500)\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Data Set 2: Palmer Penguins {.scrollable .smaller}\n\n[Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/)\n\nChinstrap, Gentoo, and Adélie Penguins\n\n![](https://upload.wikimedia.org/wikipedia/commons/0/08/South_Shetland-2016-Deception_Island%E2%80%93Chinstrap_penguin_%28Pygoscelis_antarctica%29_04.jpg){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Hope_Bay-2016-Trinity_Peninsula%E2%80%93Ad%C3%A9lie_penguin_%28Pygoscelis_adeliae%29_04.jpg){height=200}\n![](http://r.qcbs.ca/workshop03/book-en/images/culmen_depth.png){height=200}  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n## `summary` from base R {.smaller}\n\n:::: {.columns}\n\n::: {.column width='65%'}\nShows summary statistics for the values in a vector\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(galton$Estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    896    1162    1208    1197    1236    1516 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(viertel$Schätzung)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     120     5000     9843    53164    20000 29530000 \n```\n\n\n:::\n:::\n\n\nOr for all columns in a data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='35%'}\n\n[Question]{style='color:red;'}\n\nWhat does   \n`1st Qu.` and   \n`3rd Qu.` mean?\n\n:::\n\n::::\n\n# Quantiles\n\n## Quantiles {.smaller  background-color=\"khaki\"}\n \nCut points specifying intervals which contain equal amounts of values of the distribution. \n\n**$q$-quantiles** divide numbers into $q$ intervals covering all values. \n\nThe quantiles are the cut points: For $q$ intervals there are $q-1$ cut points of interest. \n\n- The one 2-quantile is the median\n- The three 4-quantiles are called **quartiles**\n    - `1st Qu.` is the first quartile\n    - The second quartile is the median\n    - `3rd Qu.` is the third quartile\n- 100-quantiles are called **percentiles**\n\n::: {.aside}\nWe omit problems of estimating quantiles from a sample where the number of estimates does not fit to a desired partition of equal size here. \n:::\n\n\n## 1a Galton: Quartiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    0%    25%    50%    75%   100% \n 896.0 1162.5 1208.0 1236.0 1516.0 \n```\n\n\n:::\n:::\n\n\nInterpretation: What does the value at 25% mean?\n\n. . . \n\nThe 25% of all values are lower than the value. 75% are larger. \n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.25)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n## 1a Galton: 20-quantiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.05))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n 896.0 1078.3 1109.0 1126.9 1150.0 1162.5 1174.0 1181.0 1189.0 1199.0 1208.0 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%   100% \n1214.0 1219.0 1225.0 1231.0 1236.0 1243.8 1255.1 1270.0 1293.0 1516.0 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = quantile(galton$Estimate, prob = seq(0, 1, by = 0.05)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n## 1b Viertelfest: Quartiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.25))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      0%      25%      50%      75%     100% \n     120     5000     9843    20000 29530000 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.25))[1:4], color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## 1b Viertelfest: 20-quantiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.05))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         0%          5%         10%         15%         20%         25% \n     120.00     1213.25     2000.00     3115.00     4012.00     5000.00 \n        30%         35%         40%         45%         50%         55% \n    5853.50     7000.00     7821.00     8705.25     9843.00    10967.50 \n        60%         65%         70%         75%         80%         85% \n   12374.00    14444.00    16186.00    20000.00    27500.00    38000.00 \n        90%         95%        100% \n   63649.50    99773.50 29530000.00 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.05))[1:19], color = \"red\")\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n## 2 Palmer Penguins Flipper Length: Quartiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  0%  25%  50%  75% 100% \n 172  190  197  213  231 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE), color = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n## 2 Palmer Penguins Flipper Length: 20-quantiles {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n172.0 181.0 185.0 187.0 188.0 190.0 191.0 193.0 194.0 195.0 197.0 199.0 203.0 \n  65%   70%   75%   80%   85%   90%   95%  100% \n208.0 210.0 213.0 215.0 218.0 220.9 225.0 231.0 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.05), na.rm = TRUE), color = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n## Interquartile range (IQR) {.smaller background-color=\"khaki\"}\n\nThe difference between the 1st and the 3rd quartile. Alternative **dispersion measure**.   \nThe range in which the middle 50% of the values are located.\n\nExamples: \n\n\n:::: {.columns}\n\n::: {.column width='40%'}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Min, 3 Quartiles, Max\nIQR(galton$Estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 73.5\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(galton$Estimate) # for comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 73.58677\n```\n\n\n:::\n\n```{.r .cell-code}\nIQR(viertel$Schätzung)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15000\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(viertel$Schätzung) # for comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 848395.5\n```\n\n\n:::\n\n```{.r .cell-code}\nIQR(penguins$flipper_length_mm, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(penguins$flipper_length_mm, na.rm = TRUE) # for comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.06171\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width='60%'}\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- quantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + \n geom_vline(xintercept = q, color = \"red\") +\n annotate(\"segment\", x = q[2], xend = q[4], y = 0, yend = 0, color = \"blue\", size = 4) + \n theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- quantile(viertel$`Schätzung`, prob = seq(0, 1, by = 0.25))\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + \n geom_vline(xintercept = q[1:4], color = \"red\") +\n annotate(\"segment\", x = q[2], xend = q[4], y = 0, yend = 0, color = \"blue\", size = 4) + \n theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- quantile(penguins$flipper_length_mm, prob = seq(0, 1, by = 0.25), na.rm = TRUE)\npenguins |> ggplot(aes(flipper_length_mm)) + geom_histogram(binwidth = 1) + \n geom_vline(xintercept = q, color = \"red\") +\n annotate(\"segment\", x = q[2], xend = q[4], y = 0, yend = 0, color = \"blue\", size = 4) + \n theme_grey(base_size = 20)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n:::\n\n::::\n\n\n\n## Boxplots {.smaller}\n\nA condensed visualization of a distribution showing location, spread, skewness and outliers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n- The **box** shows the median in the middle and the other two quartiles as their borders.\n- **Whiskers**: From above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similarly, for the lower quartile. \n- Whiskers must end at an observed data point! (So lengths can differ.) \n- All other values outside of box and whiskers are shown as points and often called **outliers**. (There may be none.)\n\n## Boxplots vs. histograms {.smaller}\n\n- Histograms can show the shape of the distribution well, but not the summary statistics like the median.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton |> ggplot(aes(x = Estimate)) + geom_histogram(binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n\n## Boxplots vs. histograms {.smaller}\n\n- Boxplots [can not]{style='color:red;'} show the patterns of **bimodal or multimodal** distributions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_boxplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_histogram(binwidth = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n# More Summary Statistics\n\n## Minimizing proporties of Mean and Median { background-color=\"aquamarine\" .smaller}\n\n*Mean* minimizes the mean of squared deviations from it. No other value $a$ has a lower mean of square distances from the data points. $\\frac{1}{n}\\sum_{i=1}^n(x_i - a)^2$.\n\n. . .\n\n*Median* minimizes the sum of the absolute deviation. No other value $a$ has a lower mean of absolute distances from the data points. $\\frac{1}{n}\\sum_{i=1}^n|x_i - a|$.\n\n. . . \n\n[**The Concept of Minimizing**]{style='color:blue;'}  \nIs central for all statisitical fitting and learning procedures! These are among the simplest examples of this concept. \n\n## Two families of summary statistics {background-color=\"aquamarine\" .smaller}\n\n- Measures based on **sums** (related to *mathematical moments*)\n  - Mean\n  - Standard deviation\n- Measures based on the **ordered** sequence of these observations (*order statistics*)\n  - Median (and all quantiles)\n  - Interquartile range\n  \n## A hierarchy of moments {background-color=\"aquamarine\" .smaller}\n\n$k$th raw moment: $\\frac{1}{n}\\sum_i^n x_i^k$.  \n\n1. The *mean* is the *1st raw moment* (because no exponents appear in formula)\n2. The *variance* is the *2nd raw moment* the mean-shifted $x$\n3. The *3rd moment* appears in the definition of the **skewness** of $x$\n4. The *4th moment* appears in the definition of the **kurtosis** of $x$ \n\n## Skewness {background-color=\"aquamarine\" .smaller}\n\nThe skewness of a distribution is a measure of its asymmetry.\n\nEquation: $\\text{skewness} = \\frac{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^3}{\\left(\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\right)^{3/2}}$\n\n- **Positive skewness**: The right tail is longer or fatter than the left tail.\n- **Negative skewness**: The left tail is longer or fatter than the right tail.\n\n![](https://upload.wikimedia.org/wikipedia/commons/c/cc/Relationship_between_mean_and_median_under_different_skewness.png)\n\nThe relation of *mean* and *median* can give a hint on skewness!  \nThe Viertelfest data is heavily positively skew. (The Galton data is a little bit negatively skew, but it is barely visible.)\n\n\n\n## Kurtosis {background-color=\"aquamarine\" .smaller}\n\nThe kurtosis of a distribution is a measure of the \"tailedness\" of the distribution. It often goes along with also higher \"peakedness\". \n\nEquation: $\\text{kurtosis} = \\frac{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^4}{\\left(\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\right)^{2}}$\n\n- **Leptokurtic**: Fatter tails and a higher peak than the normal distribution.\n- **Platykurtic**: Thinner tails and a lower peak than the normal distribution.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Standard_symmetric_pdfs.svg/1920px-Standard_symmetric_pdfs.svg.png){height=200}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Standard_symmetric_pdfs_logscale.svg/1920px-Standard_symmetric_pdfs_logscale.svg.png){height=200} \nA logarithmic y-axis shows the fatter tails!\n\n\n# Bivariate Summary Statistics\n\n## Covariance  {.smaller background-color=\"aquamarine\"}\n\nGoal: We want to measure the joint variation in numerical observations of two variables $x_1,\\dots,x_n$ and $y_1, \\dots, y_n$. \n\n**Covariance** \n\n$\\text{cov}(x,y) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)$\n\nwhere $\\mu_x$ and $\\mu_y$ are the arithmetic means of $x$ and $y$. \n\n. . .\n\nNote: $\\text{cov}(x,x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(x_i - \\mu_x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)^2 = \\text{Var}(x)$\n\n\n## Correlation {.smaller background-color=\"aquamarine\"}\n\nGoal: We want to measure the linear correlation in numerical observations of two variables $x_1,\\dots,x_n$ and $y_1, \\dots, y_n$. \n\n**Pearson correlation coefficient**\n$r_{xy} = \\frac{\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n(x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y_i - \\mu_y)^2}}$ \n\n(Note: Do you are missing $\\frac{1}{n} terms compared to covariance? They all cancel out!)\n\n. . . \n\nRelation to covariance: $r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}$\n\nwhere $\\sigma_x$ and $\\sigma_y$ are the standard deviations of $x$ and $y$.\n\nRelation to standard scores:  \nWhen $x$ and $y$ are standard scores (each with mean zero and standard deviation one), then $\\text{cov}(x,y) = r_{xy}$. \n\n\n:::{.aside}\nThere are other correlation coefficients which we omit here. \n:::\n\n\n## Interpretation of correlation {.smaller}\n\nCorrelation between two vectors $x$ and $y$ is \"normalized\". \n\n- The maximal possible values is $r_{xy} = 1$ \n  - $x$ and $y$ are *fully correlated*\n- The minimal values is $r_{xy} = -1$\n  - $x$ and $y$ are *anticorrelated*\n- $r_{xy} \\approx 0$ mean \n  - the variables are *uncorrelated*\n\n- $r_{xy} = r_{yx}$\n\n## Correlation matrix {.smaller}\n\nUsing `corrr` from the packages `tidymodels`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corrr)\npenguins |> select(-species, -island, -sex) |> \n correlate()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  term        bill_length_mm bill_depth_mm flipper_length_mm body_mass_g    year\n  <chr>                <dbl>         <dbl>             <dbl>       <dbl>   <dbl>\n1 bill_lengt…        NA            -0.235              0.656      0.595   0.0545\n2 bill_depth…        -0.235        NA                 -0.584     -0.472  -0.0604\n3 flipper_le…         0.656        -0.584             NA          0.871   0.170 \n4 body_mass_g         0.595        -0.472              0.871     NA       0.0422\n5 year                0.0545       -0.0604             0.170      0.0422 NA     \n```\n\n\n:::\n:::\n\n\n\n## Correlation table {.smaller}\n\nUsing `correlation` from the packages `correlation`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(correlation)\nresults <- palmerpenguins::penguins |> \n select(-species, -island, -sex) |> \n correlation()\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Correlation Matrix (pearson-method)\n\nParameter1        |        Parameter2 |     r |         95% CI | t(340) |         p\n-----------------------------------------------------------------------------------\nbill_length_mm    |     bill_depth_mm | -0.24 | [-0.33, -0.13] |  -4.46 | < .001***\nbill_length_mm    | flipper_length_mm |  0.66 | [ 0.59,  0.71] |  16.03 | < .001***\nbill_length_mm    |       body_mass_g |  0.60 | [ 0.52,  0.66] |  13.65 | < .001***\nbill_length_mm    |              year |  0.05 | [-0.05,  0.16] |   1.01 | 0.797    \nbill_depth_mm     | flipper_length_mm | -0.58 | [-0.65, -0.51] | -13.26 | < .001***\nbill_depth_mm     |       body_mass_g | -0.47 | [-0.55, -0.39] |  -9.87 | < .001***\nbill_depth_mm     |              year | -0.06 | [-0.17,  0.05] |  -1.11 | 0.797    \nflipper_length_mm |       body_mass_g |  0.87 | [ 0.84,  0.89] |  32.72 | < .001***\nflipper_length_mm |              year |  0.17 | [ 0.06,  0.27] |   3.17 | 0.007**  \nbody_mass_g       |              year |  0.04 | [-0.06,  0.15] |   0.78 | 0.797    \n\np-value adjustment method: Holm (1979)\nObservations: 342\n```\n\n\n:::\n:::\n\n\n::: aside\nWhat do the stars mean? Statistical significance automatically added by the . We treat that later. \n:::\n\n## Correlation visualization {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>%\n  summary(redundant = TRUE) %>%\n  plot()\n```\n\n::: {.cell-output-display}\n![](W05_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n# Exploratory Data Analysis {background-color=\"khaki\"}\n\n## Exploratory Data Analysis {background-color=\"khaki\" .smaller}\n\n![](img/data-science-explore.png)\n\nEDA is the systematic exploration of data using\n\n- visualization\n- transformation\n- computation of characteristic values\n- modeling\n\n\n:::{.aside}\nComputation of characteristic values: Functions like mean, median, mode, standard deviation, or interquartile range  \nModeling: Operations like linear regression or dimensionality reduction. We haven't talked about it, but will do soon.   \n:::\n\n\n## Systematic but no standard routine {background-color=\"khaki\"}\n\n> “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n> “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey \n\n## Systematic but no standard routine {background-color=\"khaki\"}\n\n- Goal of EDA: Develop understanding of your data.\n- EDA's iterative cycle\n    1. Generate questions about your data.\n    2. Search for answers by visualizing, transforming, and modelling your data.\n    3. Use what you learn to refine your questions and/or generate new questions.\n- EDA is fundamentally a creative process.\n\n## Questions {background-color=\"khaki\"}\n\n- The way to ask quality questions:\n  - Generate many questions!\n  - You cannot come up with most interesting questions when you start. \n- There is no rule which questions to ask. These are useful\n    1. What type of **variation** occurs within my variables?  \n    (Barplots, Histograms,...)\n    2. What type of **covariation** occurs between my variables?   \n    (Scatterplots, Timelines,...)\n\n## EDA embedded in a *statistical* data science project {background-color=\"khaki\"}\n\n1.  Stating and refining the question\n2.  **Exploring the data**\n3.  Building formal statistical models\n4.  Interpreting the results\n5.  Communicating the results\n\n:::{.aside}\nRoger D. Peng and Elizabeth Matsui.\n\"The Art of Data Science.\" A Guide for Anyone Who Works with Data.\nSkybrude Consulting, LLC (2015).\n:::\n\n\n# Data science projects {background-color=\"khaki\"}\n\nOutline of question-driven data work\n\n## Six types of questions {background-color=\"khaki\" .smaller}\n\n1.  **Descriptive:** summarize a characteristic of a set of data\n2.  **Exploratory:** analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\n3.  **Inferential:** analyze patterns, trends, or relationships in representative data from a population\n4.  **Predictive:** make predictions for individuals or groups of individuals\n5.  **Causal:** whether changing one factor will change another factor, on average, in a population\n6.  **Mechanistic:** explore \"how\" one factor (probably/most likely/potentially) changes another\n\n. . . \n\n*We only did 1 and 2, so far.*\n\n:::{.aside}\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. <https://doi.org/10.1126/science.aaa6146>.\n:::\n\n\n## Descriptive Projects {background-color=\"khaki\"}\n\n![](img/DescripitiveResearch_Dubin1969.png)\n\n::: aside\nDubin (1969). Theory Building - A Practical Guide to the Construction and Testing of Theoretical Models\n:::\n\n\n## Data Analysis Flowchart {background-color=\"khaki\"}\n\n![](img/DataAnalysisFlowChart_LeekPeng.jpeg){fig-align=\"center\" height=\"550\"}\n\n## Example: COVID-19 and Vitamin D  {background-color=\"khaki\" .smaller}\n\n::: {.incremental}\n1. **Descriptive:** frequency of hospitalisations due to COVID-19 in a set of data collected from a group of individuals\n2. **Exploratory:** examine relationships between a range of dietary factors and COVID-19 hospitalisations\n3.  **Inferential:** examine whether any relationship between taking Vitamin D supplements and COVID-19 hospitalisations found in the sample hold for the population at large\n4. **Predictive:** what types of people will take Vitamin D supplements during the next year\n5. **Causal:** whether people with COVID-19 who were randomly assigned to take Vitamin D supplements or those who were not are hospitalised \n6.  **Mechanistic:** how increased vitamin D intake leads to a reduction in the number of viral illnesses\n:::\n\n## Questions to questions  {background-color=\"khaki\"}\n\n-   Do you have appropriate data to answer your question?\n-   Do you have information on confounding variables?\n-   Was the data you're working with collected in a way that introduces bias?\n\n:::{.aside}\n**Example**  \nI want to estimate the average number of children in households in Bremen.\nI conduct a survey at an elementary school and ask pupils how many children, including themselves, live in their house.\nThen, I take the average of the responses.\n\n- Is this a biased or an unbiased estimate of the number of children in households in Bremen?\n- If biased, will the value be an overestimate or underestimate?\n:::\n\n\n## Context Information is important! {background-color=\"khaki\" .smaller}\n\n- Not all information is in the data!\n- Potential confounding variables you infer from general knowledge\n- Information about data collection you may receive from an accompanying report\n- Information about computed variables you may need to look up in accompanying documentation\n- Information about certain variables you may find in an accompanying **codebook**. For example the exact wording of questions in survey data. ",
    "supporting": [
      "W05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}